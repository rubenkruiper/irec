{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00aa64c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import pickle\n",
    "import glob, os\n",
    "import requests, urllib\n",
    "import json, random\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from tqdm import tqdm\n",
    "from typing import List, Any, List, Dict\n",
    "from textblob import TextBlob\n",
    "from itertools import combinations, combinations_with_replacement\n",
    "from collections import Counter\n",
    "from nltk.corpus import wordnet as wn\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "from rdflib import URIRef, BNode, Literal, Namespace, Graph\n",
    "from rdflib.namespace import XSD, RDF, RDFS, SKOS, NamespaceManager\n",
    "\n",
    "import utils as util"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11663171",
   "metadata": {},
   "source": [
    "We would like to express the following features/relations:\n",
    "* Dictionary definition terms, which are always concepts\n",
    "  * We'll use the source as namespace, and corresponding concept identifier if it exists\n",
    "  * SKOS is used to establish a mapping (e.g., skos:exactMatch) and add the definition (skos:definition)\n",
    "* Special properties that we want to capture between words, which may help identify concepts:\n",
    "  * Word is part of MWE\n",
    "  * Morphologically similar words; stemming & Levenshtein distance\n",
    "  * Semantically similar words; distributed similarity (NNs)\n",
    "  * Acronyms\n",
    "  * Related, this is a generic relation, e.g., a `ampere` is related to `electric current`\n",
    "  * Domain-specificity; foreground or background term following our filtering procedure\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979b0616",
   "metadata": {},
   "source": [
    "### Prepare namespaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace4602c",
   "metadata": {},
   "source": [
    "* Note: that UNICLASS is not a namespace (yet) only has identifiers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b60e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = Namespace(\"http://example.org/top_concept_for_visulisation/#\")\n",
    "WIKI = Namespace(\"http://www.wikidata.org/entity/#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098e92aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "IREC_ontology_URL = \"http://example.org/irec-schema/#\"\n",
    "IREC_instances_URL = \"http://example.org/irec-spans/#\"\n",
    "IREC_concepts_URL = \"http://example.org/irec-concepts/#\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e6b31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our custom namespace for the schema to store spans\n",
    "IREC = Namespace(IREC_ontology_URL)\n",
    "\n",
    "# create a custom namespace to store spans and concepts\n",
    "SPANS = Namespace(IREC_instances_URL)\n",
    "CONCEPTS = Namespace(IREC_concepts_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcef6e56",
   "metadata": {},
   "source": [
    "### graph creation utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e8ce74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UID_assigner:\n",
    "    def __init__(self):\n",
    "        self.UIDs = {}\n",
    "        self.UID = 0\n",
    "        \n",
    "    def assign_UID(self, text, namespace: Namespace):\n",
    "        \"\"\"\n",
    "        Determines which type of UID to assign, based on the namespace.\n",
    "        \"\"\"\n",
    "        if namespace == SPANS:\n",
    "            return self.span_UID(text)\n",
    "        elif namespace == CONCEPTS:\n",
    "            return self.concept_UID(text)\n",
    "        else:\n",
    "            print(\"Currently no function implemented for assigning UIDs for this namespace\")\n",
    "    \n",
    "    def span_UID(self, text):\n",
    "        \"\"\"\n",
    "        NOTE: each text span is a unique identifier in and of itself. We'll simply convert the text span to \n",
    "        a URL friendly representation.\n",
    "        \"\"\"\n",
    "        n_space = SPANS.placeholder.defrag().__reduce__()[1][0]\n",
    "        if n_space not in self.UIDs:\n",
    "            self.UIDs[n_space] = {}\n",
    "        \n",
    "        urltext = urllib.parse.quote(text)\n",
    "        if text in self.UIDs[n_space]:\n",
    "            return self.UIDs[n_space][text], False\n",
    "        else:\n",
    "            self.UIDs[n_space][text] = urltext\n",
    "            return self.UIDs[n_space][text], True\n",
    "        \n",
    "    def concept_UID(self, text):\n",
    "        \"\"\"\n",
    "        For now I'll create my own dumb interger-based UIDs for nodes as a simple shortcut, split per namespace\n",
    "        \"\"\"\n",
    "        n_space = CONCEPTS.placeholder.defrag().__reduce__()[1][0]\n",
    "        \n",
    "        if n_space not in self.UIDs:\n",
    "            self.UIDs[n_space] = {}\n",
    "        \n",
    "        if text in self.UIDs[n_space]:\n",
    "            return self.UIDs[n_space][text], False\n",
    "        else:\n",
    "            self.UID += 1\n",
    "            self.UIDs[n_space][text] = str(self.UID)\n",
    "            return self.UIDs[n_space][text], True\n",
    "    \n",
    "    def count_nodes_in_namespace(self, namespace: Namespace = SPANS):\n",
    "        n_space = namespace.placeholder.defrag().__reduce__()[1][0]\n",
    "        print(f\"Number of nodes in '{n_space}': {len(self.UIDs[n_space])}\")\n",
    "        \n",
    "    def print_node_by_id(self, graph, node_id, namespace: Namespace = SPANS):\n",
    "        for s, p, o in graph.triples((namespace[str(node_id)],  None, None)):\n",
    "            print(f\"{s.split('#')[-1]} ; {p.split('#')[-1]} ; {o.split('#')[-1]}\")\n",
    "        \n",
    "    def print_node_by_text(self, graph, node_text, namespace: Namespace = SPANS):\n",
    "        n_space = namespace.placeholder.defrag().__reduce__()[1][0]\n",
    "        node_id = self.UIDs[n_space][node_text]\n",
    "        # find all triples with subject\n",
    "        for s, p, o in graph.triples((namespace[node_id],  None, None)):\n",
    "            print(f\"{s.split('#')[-1]} ; {p.split('#')[-1]} ; {o.split('#')[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772b6692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These wrappers only exist to help me quickly and consistently add nodes to the graph\n",
    "\n",
    "\n",
    "def add_top_concept(graph, node_uid, namespace: Namespace=CONCEPTS):\n",
    "    \"\"\" In some cases we'd like the concept to be linked to the ROOT of the graph, for visualisation. \"\"\"\n",
    "    graph.add((namespace[node_uid], SKOS.hasTopConcept, ROOT[top_concept_uid]))\n",
    "    return graph\n",
    "\n",
    "# IREC functions and REFERENCE\n",
    "IREC.Span # A span is a sequence of characters that occurs verbatim in a text, either contiguous or discontiguos as extracted by SPaR.txt (Kruiper et al., 2021).   \n",
    "IREC.constitutes  # Indicates that a span constitutes another span, e.g., the Multi-Word Expression (MWE) Span `hot water storage system` the Span `storage`.\n",
    "IREC.isMorphologicallySimilarTo # Indicates that a Span is morphologically similar to another Span, e.g., they may have the same stem or a small Levenshtein distance.\n",
    "IREC.isSemanticallySimilarTo # Indicates that a Span is semantically similar to another Span, following a cosine similarity between their  embeddings.\n",
    "IREC.related # General way to indicate some relation between two spans, e.g., `ampere` is related to `electric current`\n",
    "IREC.hasAcronym # A Span can have an acronym, e.g., `British Standards Institute` has the acronym `BSI`.\n",
    "IREC.isAcronymOf # A Span can have an acronym, e.g., `BSI` is the acronym for `British Standards Institute`.\n",
    "IREC.hasAntonym # Property that relates a Span to another Span, each being each other's antonyms.\n",
    "\n",
    "def irec_span(graph, node_uid, text, namespace: Namespace=SPANS):\n",
    "    \"\"\" Add a span node in the SPANS namespace, of type IREC.Span and the span text set as its RDF.label \"\"\"\n",
    "    # is preflabel a property? I would assume so\n",
    "    graph.add((namespace[node_uid], RDF.type, IREC.Span))\n",
    "    graph.add((namespace[node_uid], RDFS.label,  Literal(text, lang='en')))\n",
    "    return graph\n",
    "\n",
    "def irec_constitutes(graph, subject_node_uid, object_node_uid,\n",
    "                     subject_namespace: Namespace=SPANS, object_namespace: Namespace=SPANS):\n",
    "    \"\"\" Indicates that somewhere in the label of the first SPAN node, you can find the second span's label \"\"\"\n",
    "    graph.add((subject_namespace[subject_node_uid], IREC.constitutes, object_namespace[object_node_uid]))\n",
    "    return graph\n",
    "\n",
    "def irec_morp_sim(graph, subject_node_uid, object_node_uid,\n",
    "                  subject_namespace: Namespace=SPANS, object_namespace: Namespace=SPANS):\n",
    "    \"\"\" Indicates that the labels of two SPAN nodes are morphologically similar \"\"\"\n",
    "    graph.add((subject_namespace[subject_node_uid], IREC.isMorphologicallySimilarTo, object_namespace[object_node_uid]))\n",
    "    return graph\n",
    "\n",
    "def irec_sem_sim(graph, subject_node_uid, object_node_uid,\n",
    "                 subject_namespace: Namespace=SPANS, object_namespace: Namespace=SPANS):\n",
    "    \"\"\" Indicates that the labels of two SPAN nodes are semantically similar, following the distributed semantics hypothesis \"\"\"\n",
    "    graph.add((subject_namespace[subject_node_uid], IREC.isSemanticallySimilarTo, object_namespace[object_node_uid]))\n",
    "    return graph\n",
    "\n",
    "def irec_related(graph, subject_node_uid, object_node_uid,\n",
    "                 subject_namespace: Namespace=SPANS, object_namespace: Namespace=SPANS):\n",
    "    \"\"\" Indicates that the labels of two SPAN nodes are semantically similar, following the distributed semantics hypothesis \"\"\"\n",
    "    graph.add((subject_namespace[subject_node_uid], IREC.related, object_namespace[object_node_uid]))\n",
    "    return graph\n",
    "\n",
    "def irec_has_acronym(graph, subject_node_uid, object_node_uid,\n",
    "                     subject_namespace: Namespace=SPANS, object_namespace: Namespace=SPANS):\n",
    "    \"\"\" Indicates that the label of the subject node has an acronym, ergo the label of the object node  \"\"\"\n",
    "    graph.add((subject_namespace[subject_node_uid], IREC.hasAcronym, object_namespace[object_node_uid]))\n",
    "    return graph\n",
    "\n",
    "def irec_is_acronym_of(graph, subject_node_uid, object_node_uid,\n",
    "                       subject_namespace: Namespace=SPANS, object_namespace: Namespace=SPANS):\n",
    "    \"\"\" Indicates that the label of the subject node is an acronym of the label of the object node  \"\"\"\n",
    "    graph.add((subject_namespace[subject_node_uid], IREC.isAcronymOf, object_namespace[object_node_uid]))\n",
    "    return graph\n",
    "\n",
    "def irec_antonym(graph, subject_node_uid, object_node_uid,\n",
    "                 subject_namespace: Namespace=SPANS, object_namespace: Namespace=SPANS):\n",
    "    \"\"\" Indicates that the label of the subject node is an antonym of the label of the object node  \"\"\"\n",
    "    graph.add((subject_namespace[subject_node_uid], IREC.hasAntonym, object_namespace[object_node_uid]))\n",
    "    return graph\n",
    "\n",
    "\n",
    "# SKOS \n",
    "def skos_node(graph, node_uid, text, namespace: Namespace=CONCEPTS):\n",
    "    \"\"\" Add a concept to the graph in the CONCEPTS namespace, of type SKOS.Concept \"\"\"\n",
    "    graph.add((namespace[node_uid], RDF.type, SKOS.Concept))\n",
    "    graph = skos_prefLabel(graph, node_uid, text, namespace)\n",
    "    return graph\n",
    "\n",
    "def skos_prefLabel(graph, node_uid, text, namespace: Namespace=CONCEPTS):\n",
    "    \"\"\" Add the text label for a node \"\"\"\n",
    "    graph.add((namespace[node_uid], SKOS.prefLabel, Literal(text, lang='en')))\n",
    "    return graph\n",
    "\n",
    "def skos_altLabel(graph, node_uid, alt_label_uid, namespace: Namespace=CONCEPTS):\n",
    "    \"\"\" Add an alternative text label for a concept node \"\"\"\n",
    "    graph.add((namespace[node_uid], SKOS.altLabel, namespace[alt_label_uid]))\n",
    "    graph.add((namespace[alt_label_uid], SKOS.altLabel, namespace[node_uid]))\n",
    "    return graph\n",
    "\n",
    "def skos_related(subject_node_uid, object_node_uid,\n",
    "                subject_namespace: Namespace=SPANS, object_namespace: Namespace=CONCEPTS):\n",
    "    \"\"\" Denotes a relation between two nodes, would expect the nodes to be in different vocabularies \"\"\"\n",
    "    graph.add((subject_namespace[subject_node_uid], SKOS.related, object_namespace[object_node_uid]))\n",
    "    return graph\n",
    "    \n",
    "def skos_broader(graph, narrower_node_uid, broader_node_uid,\n",
    "                 narrower_namespace: Namespace=CONCEPTS, broader_namespace: Namespace=CONCEPTS):\n",
    "    \"\"\" Assuming narrower/broader is always reflexive, would expect the nodes to be in different vocabularies \"\"\"\n",
    "    graph.add((broader_namespace[narrower_node_uid], SKOS.narrower, narrower_namespace[broader_node_uid]))\n",
    "    graph.add((narrower_namespace[broader_node_uid], SKOS.broader, broader_namespace[narrower_node_uid]))\n",
    "    return graph\n",
    "    \n",
    "def skos_note(graph, node_uid, note_text, namespace: Namespace=CONCEPTS):\n",
    "    \"\"\" I don't think this is used right now; not sure if there is a use-case at any point \"\"\"\n",
    "    graph.add((namespace[node_uid], SKOS.note, Literal(note_text, lang='en')))\n",
    "    return graph\n",
    "\n",
    "def skos_definition(graph, node_uid, definition_text, namespace: Namespace=CONCEPTS):\n",
    "    \"\"\" The namespace indidcates the source of the definition? \"\"\"\n",
    "    graph.add((namespace[node_uid], SKOS.definition, Literal(definition_text, lang='en')))\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fdc557",
   "metadata": {},
   "source": [
    "### Prepare namespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a1e1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "irec_graph = Graph()\n",
    "\n",
    "irec_graph.bind(\"root\", ROOT)\n",
    "irec_graph.bind(\"wikipedia\", WIKI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56acbeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import our vocabulary\n",
    "irec_graph.parse(\"IREC.rdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb5024e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ua = UID_assigner()\n",
    "top_concept_uid = 'ROOT'\n",
    "\n",
    "irec_graph = skos_prefLabel(irec_graph, top_concept_uid, \"NUU_graph_root\", namespace = ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fceef5",
   "metadata": {},
   "source": [
    "### Add base antonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fe6de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to capture antonyms: dichotomy in meaning of words, \n",
    "# For this we'll use NLTK's version of WordNet, which mainly captures antonyms for adjectives and adverbs.\n",
    "wordnet_antonyms = {}\n",
    "for i in wn.all_synsets():\n",
    "    if i.pos() in ['a', 's']:    # If synset is adj or satelite-adj.\n",
    "        for j in i.lemmas():     # Iterating through lemmas for each synset.\n",
    "            if j.antonyms():     # If adj has antonym.\n",
    "                wordnet_antonyms[str(j.name()).strip()] = [x.name() for x in j.antonyms()]\n",
    "\n",
    "# Example of a useful antonym for us\n",
    "wordnet_antonyms['hot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3d0ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_antonyms['cold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd035fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for span in wordnet_antonyms.keys():\n",
    "    span_uid, new_uid_bool = ua.assign_UID(span, SPANS)\n",
    "    \n",
    "    if new_uid_bool: # equals if (SPANS[uid], None, None) not in graph: \n",
    "        # need to add the span to the graph\n",
    "        irec_graph = irec_span(irec_graph, span_uid, span)\n",
    "        \n",
    "    antonyms = wordnet_antonyms[span]\n",
    "    for antonym in antonyms:\n",
    "        antonym_uid, new_uid_bool = ua.assign_UID(antonym, SPANS)\n",
    "        \n",
    "        if new_uid_bool:\n",
    "            irec_graph = irec_span(irec_graph, antonym_uid, antonym)\n",
    "            \n",
    "        # add the antonym relation\n",
    "        irec_graph = irec_antonym(irec_graph, span_uid, antonym_uid)\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41f63e5",
   "metadata": {},
   "source": [
    "### Add domain terms extracted from the Approved documents as Spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8178ad1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_terms = pickle.load(open('data/domain_terms.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f4b0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c506907d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simply adding the extracted spans\n",
    "for span in domain_terms:\n",
    "    span_uid, new_uid_bool = ua.assign_UID(span, SPANS)\n",
    "    \n",
    "    if new_uid_bool:\n",
    "        irec_graph = irec_span(irec_graph, span_uid, span)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8c4c91",
   "metadata": {},
   "source": [
    "### Add Acronyms that were grabbed from the text\n",
    "\n",
    "These can help:\n",
    "* remove terms where the boundary detection is off\n",
    "* avoid suggesting similar acronyms, e.g., suggest that EPC and EPS are similar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434b8738",
   "metadata": {},
   "outputs": [],
   "source": [
    "acronyms = {'PAS': ['ecification', 'Specification'],  'GSIUR': ['Regulations 1998'],  'HSE': ['Regulations 2000',   'water systems',   'Safety Executive',   'Health and Safety Executive'],  'PE': ['Polyethylene', 'polyethylene'],  'DN': ['pipe'],  'DCLG': ['land', 'Local Government', 'England', 'ment'],  'PP': ['Polypropylene'],  'BCB': ['Control Body',   'the building control body',   'Building control body',   'building control body',   'Building Control Body'],  'SRHRV': ['ventilator',   'single room heat recovery ventilator',   'a single room heat recovery ventilator'],  'MVHR': ['blocks', 'heat recovery'],  'WC': ['sets'],  'TFA': ['the total floor area'],  'LRV': ['Light reflectance value'],  'BER': ['Building CO2 Emission Rate', 'CO2 Emission Rate'],  'TER': ['CO2 Emission Rate',   'the Target CO2 Emission Rate',   'Target CO2 Emission Rate'],  'DER': ['CO2 Emission Rate', 'the Dwelling CO2 Emission Rate'],  'EPC': ['energy performance certificate'],  'TFEE': ['Target Fabric Energy Efficiency',   'Fixed building services',   'Energy Efficiency'],  'DHF': ['the Door and Hardware Federation', 'Door and Hardware Federation'],  'REI': ['fire resistance', 'bility'],  'PHE': ['horizontal evacuation'],  'W': ['the final exit', 'final exit'],  'DWELLINGS': ['RESIDENTIAL'],  'OTHER': ['RESIDENTIAL'],  'TSO': ['Office', 'The Stationery Office'],  'FPA': ['the Fire Protection Association', 'Association'],  'A': ['absorption area'],  'AT': ['absorption area'],  'DECC': ['Climate Change'],  'NCM': ['the National Calculation Methodology'],  'ADCAS': ['Allied Services'],  'DFEE': ['Energy Efficiency'],  'LPA': ['the local planning authority', 'planning authority'],  'UKAS': ['the United Kingdom Accreditation Service'],  'BSI': ['the British Standards Institution'],  'EA': ['Accreditation'],  'BGS': ['British Geological Survey'],  'HBN': ['Notes'],  'GGF': ['Glazing Federation'],  'E': ['terms of integrity'],  'TRADA': ['the Timber Research and Development Association', 'Association'],  'ACOP': ['Code of Practice'],  'ATTMA': ['Association'],  'RVA': ['Association', 'the Residential Ventilation Association'],  'TEHVA': ['Association'],  'DSA': ['Association'],  'CIRIA': ['Association'],  'MCRMA': ['Association'],  'DSMA': ['Association'],  'OFTEC': ['Association'],  'WHO': ['Organisation'],  'GAI': ['Architectural Ironmongers'],  'MEV': ['mechanical extract', 'extract ventilation'],  'VST': ['Vicat softening temperature'],  'SCI': ['Guild Steel Construction Institute'],  'FBE': ['the Built Environment', 'ment'],  'DSER': ['Rating'],  'WER': ['Rating'],  'CIWM': ['ment', 'Wastes Management'],  'EOTA': ['ment'],  'GQRA': ['ment'],  'BRE': ['ment', 'the Building Research Establishment'],  'PPS': ['ment'],  'PSV': ['Passive stack ventilation'],  'EST': ['the Energy Saving Trust'],  'CIBSE': ['Ventilation hygiene toolkit', 'Building Services Engineers'],  'AGS': ['Geoenvironmental Specialists'],  'SPAB': ['Ancient Buildings'],  'UF': ['urea formaldehyde'],  'ODPM': ['the Deputy Prime Minister']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf33be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for acronym, spans in acronyms.items():\n",
    "    \n",
    "    acronym_uid, new_uid_bool = ua.assign_UID(acronym, SPANS)\n",
    "    if new_uid_bool:\n",
    "        # many of these will be in the graph already\n",
    "        irec_graph = irec_span(irec_graph, acronym_uid, acronym)\n",
    "    \n",
    "    for span in spans:\n",
    "        # these are all part of the graph already\n",
    "        span_uid, _ = ua.assign_UID(span, SPANS) \n",
    "        \n",
    "        # todo; \n",
    "        #  could do some filtering here of the clearly erroneous span-acronym combinations\n",
    "        #  or leave this until later, using the graph...\n",
    "    \n",
    "        if (SPANS[acronym_uid], IREC.isAcronymOf, SPANS[span_uid]) not in irec_graph:\n",
    "            irec_graph = irec_is_acronym_of(irec_graph, acronym_uid, span_uid)\n",
    "            irec_graph = irec_has_acronym(irec_graph, span_uid, acronym_uid)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953c8e57",
   "metadata": {},
   "source": [
    "### Add CONCEPTS: defined terms from the Approved Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af2368f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from csv file\n",
    "definitions = pd.read_excel(\"data/Approved Documents and derived terms.xlsx\", sheet_name=\"Definitions\", keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77530dc1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "definitions[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cbdde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_terms = pd.read_excel(\"data/Approved Documents and derived terms.xlsx\", sheet_name=\"Index terms\", keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4704ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo; below, perhaps check existince of relations to avoid duplication, e.g.;\n",
    "# if (SPANS[acronym_uid], IREC.isAcronymOf, SPANS[span_uid]) not in irec_graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d066368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create graph from definitions first\n",
    "for i, row in definitions.iloc[1:].iterrows():\n",
    "    term = row['Term'] if row['Term'].isupper() else row['Term'].lower()\n",
    "    alternative_labels = row['Alternative labels']\n",
    "    definition = row['Definition']\n",
    "    note = row['Note']\n",
    "    \n",
    "    # add the term \n",
    "    concept_uid, new_uid = ua.assign_UID(term, CONCEPTS)\n",
    "    if new_uid:\n",
    "        irec_graph = skos_node(irec_graph, concept_uid, term)\n",
    "    \n",
    "    if note: \n",
    "        irec_graph = skos_note(irec_graph, concept_uid, note) \n",
    "    \n",
    "    # always expecting a definition\n",
    "    irec_graph = skos_definition(irec_graph, concept_uid, definition) \n",
    "    \n",
    "    if alternative_labels:\n",
    "        # lowercase if not an abbreviation\n",
    "        alt_labels = [x.strip() if x.isupper() else x.lower().strip() for x in alternative_labels.split(\", \")]  \n",
    "        \n",
    "        for alt_label in alt_labels:\n",
    "            # treated like a span, that refers to the same concept <-- this is a deviation from previously\n",
    "            alt_label_uid, new_uid = ua.assign_UID(alt_label, SPANS)\n",
    "            if new_uid:\n",
    "                irec_graph = irec_span(irec_graph, alt_label_uid, alt_label)\n",
    "        \n",
    "            irec_graph = skos_altLabel(irec_graph, concept_uid, alt_label_uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ddb6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_terms[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1358bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add triples from index terms / glossaries\n",
    "# we will treat glossary terms like concepts, relations between their labels occur between the spans \n",
    "for i, row in index_terms.iloc[1:].iterrows():\n",
    "    term = row['Term'].strip() if row['Term'].isupper() else row['Term'].lower().strip()\n",
    "    alternative_labels = row['AltLabel(s)']\n",
    "    related_terms = row['Related terms']\n",
    "    broader_term = row['Broader term']\n",
    "    \n",
    "    # add the term \n",
    "    concept_uid, new_uid = ua.assign_UID(term, CONCEPTS)\n",
    "    if new_uid:\n",
    "        irec_graph = skos_node(irec_graph, concept_uid, term)\n",
    "        # hacky visualisation solution; connect all terms to the graph root -> need to find a better solution\n",
    "        # hacky visualisation solution; connect all terms to the graph root\n",
    "        # hacky visualisation solution; connect all terms to the graph root\n",
    "        irec_graph = add_top_concept(irec_graph, concept_uid)\n",
    "\n",
    "    if alternative_labels:\n",
    "        # lowercase if not an abbreviation\n",
    "        alt_labels = [x.strip() if x.isupper() else x.lower().strip() for x in alternative_labels.split(\", \")]  \n",
    "        \n",
    "        for alt_label in alt_labels:\n",
    "            # treated like a span, that refers to the same concept <-- this is a deviation from previously\n",
    "            alt_label_uid, new_uid = ua.assign_UID(alt_label, SPANS)\n",
    "            if new_uid:\n",
    "                irec_graph = irec_span(irec_graph, alt_label_uid, alt_label)\n",
    "                \n",
    "            irec_graph = skos_altLabel(irec_graph, concept_uid, alt_label_uid)\n",
    "\n",
    "    if related_terms:\n",
    "        rel_terms = [x.strip() if x.isupper() else x.lower().strip() for x in related_terms.split(\", \")]\n",
    "        for rel_term in rel_terms:\n",
    "            related_uid, new_uid = ua.assign_UID(rel_term, SPANS)\n",
    "            if new_uid:\n",
    "                irec_graph = irec_span(irec_graph, related_uid, rel_term)\n",
    "                \n",
    "            irec_graph = irec_related(irec_graph, concept_uid, related_uid)\n",
    "    \n",
    "    if broader_term:\n",
    "        # We expect 1 broader term at most currently, assuming we'd like a tree structure (DAG with 1 parent at most)\n",
    "        b_term = broader_term.strip().lower() if not broader_term.isupper() else broader_term.strip()\n",
    "        b_term_uid, new_uid = ua.assign_UID(b_term, SPANS)\n",
    "        if new_uid:\n",
    "            irec_graph = irec_span(irec_graph, b_term_uid, b_term)\n",
    "\n",
    "        # We do not expect that the broader term is necessarily a concept, although this is a feature we may rely on later\n",
    "        irec_graph = skos_broader(irec_graph, concept_uid, b_term_uid, CONCEPTS, SPANS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07d8b7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40952822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# irec_graph.serialize(destination=\"graph/approved_doc_terms_only.ttl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be08552f",
   "metadata": {},
   "source": [
    "### Print some insight in the graph so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14daa5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ua.count_nodes_in_namespace(SPANS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204cfd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "ua.count_nodes_in_namespace(CONCEPTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc02439",
   "metadata": {},
   "outputs": [],
   "source": [
    "ua.print_node_by_id(irec_graph, 291, CONCEPTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e73bd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ua.print_node_by_id(irec_graph, urllib.parse.quote('sanitary accommodation'), SPANS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e34c842",
   "metadata": {},
   "outputs": [],
   "source": [
    "ua.print_node_by_text(irec_graph, 'sanitary accommodation', SPANS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407358e6",
   "metadata": {},
   "source": [
    "### Grab wikipedia definitions for Concept nodes, and store locally for re-use\n",
    "* We have previously annotated the relevance of all WikiData classes returned for the defined terms and index terms in the Approved Documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5242d107",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_wikidata_classes_df = pd.read_csv(\"data/wiki_classes_annotated.csv\", index_col=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dab9e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_wikidata_classes_df[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd19d15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikiclass_dict = {}\n",
    "for row in annotated_wikidata_classes_df.iterrows():\n",
    "    uid_list_string, class_annotations_examples = row\n",
    "    uid_list = uid_list_string[2:-2].split(',')\n",
    "    for uid in uid_list:\n",
    "        wikiclass_dict[uid] = {\n",
    "            'Class': class_annotations_examples['WikiData class'],\n",
    "            'Annotation': class_annotations_examples['Annotation'],\n",
    "            'Example spans': class_annotations_examples['Example spans']\n",
    "        }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cfa832",
   "metadata": {},
   "source": [
    "* First, we try to grab all wiki definitions for all spans and concepts that are in the graph (so far)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e41964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the SPARQL endpoint for wikidata\n",
    "\n",
    "sparql_wrapper = SPARQLWrapper(\"https://query.wikidata.org/sparql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e9e280",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wiki_matches(graph_sparql_endpoint: SPARQLWrapper,\n",
    "                     jargon_term_and_uids: List):\n",
    "\n",
    "    all_wiki_definitions = {}\n",
    "    # we want to grab the term (subject), any definition (subjectDescription) and the class (subjectClass)\n",
    "    sparql_q = \"\"\"\n",
    "               SELECT ?subject ?subjectDescription ?classUID ?className WHERE {\n",
    "                  ?subject rdfs:label \"$QUERY\"@en.\n",
    "                  ?subject wdt:P31|wdt:P279 ?classUID.\n",
    "                  SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\".}\n",
    "                  ?classUID  rdfs:label ?className  FILTER(LANG(?className) = \"en\").\n",
    "                }\n",
    "               \"\"\"\n",
    "    \n",
    "    for term, uid in tqdm(jargon_term_and_uids):\n",
    "        # make the call to \n",
    "        temp_q = sparql_q.replace(\"$QUERY\", term)\n",
    "        graph_sparql_endpoint.setQuery(temp_q)\n",
    "        graph_sparql_endpoint.setReturnFormat(JSON)\n",
    "        try:\n",
    "            json_output = graph_sparql_endpoint.query().convert()\n",
    "        except:\n",
    "            # If no result, wait 2s; One client is allowed 30 error queries per minute\n",
    "            print(f\"Error for query, you should what's wrong with the term: {term}\")\n",
    "            time.sleep(2)\n",
    "            continue\n",
    "            \n",
    "        # sometimes multiple Wiki UIDs for a single term, we grab them all here\n",
    "        bindings = [v for v in json_output['results']['bindings']]\n",
    "            \n",
    "\n",
    "        for v in bindings:\n",
    "            class_uid = v['classUID']['value'] if 'classUID' in v else \"\"\n",
    "            class_label = v['className']['value'] if 'className' in v else \"\"\n",
    "            \n",
    "            if 'subjectDescription' in v:\n",
    "                if uid not in all_wiki_definitions:\n",
    "                    all_wiki_definitions[uid] = [{'prefLabel': term,\n",
    "                                                  'class_uid': class_uid,\n",
    "                                                  'class_label': class_label,\n",
    "                                                  'WikiUID': v['subject']['value'],\n",
    "                                                  'WikiDefinition': v['subjectDescription']['value']}]\n",
    "                else:\n",
    "                    all_wiki_definitions[uid].append({'prefLabel': term,\n",
    "                                                      'class_uid': class_uid,\n",
    "                                                      'class_label': class_label,\n",
    "                                                      'WikiUID': v['subject']['value'],\n",
    "                                                      'WikiDefinition': v['subjectDescription']['value']})\n",
    "            elif 'subject' in v:\n",
    "                # no description found, simply adding wiki UID if that exists\n",
    "                if uid not in all_wiki_definitions:\n",
    "                    all_wiki_definitions[uid] = [{'prefLabel': term,\n",
    "                                                  'class_uid': class_uid,\n",
    "                                                  'class_label': class_label,\n",
    "                                                  'WikiUID': v['subject']['value']}]\n",
    "                else:\n",
    "                    all_wiki_definitions[uid].append({'prefLabel': term,\n",
    "                                                      'class_uid': class_uid,\n",
    "                                                      'class_label': class_label,\n",
    "                                                      'WikiUID': v['subject']['value']})\n",
    "    return all_wiki_definitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9f1b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "concepts_and_uids = [(k, v) for k, v in ua.UIDs[CONCEPTS.placeholder.defrag().__reduce__()[1][0]].items()]\n",
    "spans_and_uids = [(k, v) for k, v in ua.UIDs[SPANS.placeholder.defrag().__reduce__()[1][0]].items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7235604e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First run for the Concepts\n",
    "if not os.path.exists(\"data/concept_wiki_dict.json\"):\n",
    "    wiki_dict = get_wiki_matches(sparql_wrapper, concepts_and_uids)#{'test': 1, 'conductor':2})\n",
    "    with open(\"data/concept_wiki_dict.json\", 'w') as f:\n",
    "        json.dump(wiki_dict, f, indent=2)\n",
    "else:\n",
    "    with open(\"data/concept_wiki_dict.json\", 'r') as f:\n",
    "        wiki_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b368865",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now run for the spans\n",
    "if not os.path.exists(\"data/span_wiki_dict.json\"):\n",
    "    wiki_dict = get_wiki_matches(sparql_wrapper, spans_and_uids)#{'test': 1, 'conductor':2})\n",
    "    with open(\"data/span_wiki_dict.json\", 'w') as f:\n",
    "        json.dump(wiki_dict, f, indent=2)\n",
    "else:\n",
    "    with open(\"data/span_wiki_dict.json\", 'r') as f:\n",
    "        wiki_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6832aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(wiki_dict))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67f1866",
   "metadata": {},
   "source": [
    "TODO\n",
    "* parse WikiData definitions and add spans!\n",
    "* link definitions to spans (if span occurs, link),\n",
    "* create concepts for the 500ish spans that occur in Uniclass\n",
    "* compute span properties \n",
    "  * constitutes; x occurs in y, thus y might be an extended phrase for x and perhaps a subclass, or x may be a material property, and so on\n",
    "  * morphological similarity, x may be an inflection of y\n",
    "  * semantic similarity, x and y might be alternative labels or have the same superclass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3033646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - create a new node for the wikipedia term, with prefLabel the term\n",
    "for uid, wiki_def_dict_list in expanded_dict.items():\n",
    "    \n",
    "    # we will only take into account the 1st definition for now!\n",
    "    # we will only take into account the 1st definition for now!\n",
    "    # we will only take into account the 1st definition for now!\n",
    "    idx = 0\n",
    "    d = wiki_def_dict_list[idx]\n",
    "    \n",
    "    term = d['prefLabel']\n",
    "    wiki_uid = d['WikiUID'].rsplit('/',1)[1]\n",
    "    if 'WikiDefinition' in d:\n",
    "        if wiki_definition == \"Wikimedia disambiguation page\":\n",
    "            # skip disambiguation pages in general\n",
    "            continue\n",
    "        \n",
    "        definition = d['WikiDefinition']\n",
    "        spar_objects = d['WikiDef_terms']\n",
    "        mygraph = add_wiki_exact_match(mygraph, term, uid, wiki_uid, definition, spar_objects)\n",
    "    else:\n",
    "        print(f\"Term with wiki exact match, but without definition: {term}\")\n",
    "        mygraph = add_wiki_exact_match(mygraph, term, uid, wiki_uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeba6a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def add_wiki_exact_match(graph, term, mygraph_uid, wiki_uid, wiki_definition=None, spar_objects=None):\n",
    "#     graph.add((EX[mygraph_uid], SKOS.exactMatch, WIKI[wiki_uid]))\n",
    "#     graph.add((WIKI[wiki_uid], SKOS.prefLabel, Literal(term, lang='en')))\n",
    "#     if wiki_definition:\n",
    "#         graph.add((WIKI[wiki_uid], SKOS.definition, Literal(wiki_definition, lang='en')))\n",
    "#     if spar_objects:\n",
    "#         for obj in spar_objects:\n",
    "#             # UIDs for these terms are assigned within our EXAMPLE namespace\n",
    "#             def_term_uid, new_uid_bool = ua.assign_UID(def_term)\n",
    "#             if new_uid_bool:\n",
    "#                 mygraph = add_prefLabel(mygraph, def_term_uid, def_term)\n",
    "            \n",
    "#             graph.add((WIKI[wiki_uid], SKOS.related, EX[def_term_uid]))\n",
    "#             graph.add((EX[def_term_uid], SKOS.related, WIKI[wiki_uid]))\n",
    "            \n",
    "#     return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03886a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfdeed0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1ecbff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec49fc8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03b5c56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "732800f8",
   "metadata": {},
   "source": [
    "### Step 1: create a graph from the approved documents terms file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e978d7",
   "metadata": {},
   "source": [
    "### Step 3: parse all definitions, in order to identify new nodes and links between nodes\n",
    "* we won't re-define the new nodes (again), because we assume that this would cause too much drift?\n",
    "* should check to make sure that that's the case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83372d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo; change to a local parser, see term extraction notebook\n",
    "\n",
    "def parse_definition(full_definition):\n",
    "    definition_sentences = [str(sent) for sent in TextBlob(full_definition).sentences]\n",
    "    identified_objects = []\n",
    "    for definition_sent in definition_sentences:\n",
    "        encoded_def = urllib.parse.quote(definition_sent)\n",
    "        spar_output = requests.get(f\"http://localhost:8000/predict_objects/{encoded_def}\").json()\n",
    "        try:\n",
    "            spar_objects = spar_output[\"prediction\"]['obj']\n",
    "            identified_objects += util.custom_cleaning_rules(spar_objects)\n",
    "        except:\n",
    "            continue\n",
    "    return identified_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98cba29",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in complete_dict.items():\n",
    "    # run spar.txt on definition\n",
    "    full_definition = v['definition']\n",
    "    # should break up into sentences\n",
    "    definition_sentences = [str(sent) for sent in TextBlob(full_definition).sentences]\n",
    "    complete_dict[k]['def_terms'] = []\n",
    "    \n",
    "    for definition_sent in definition_sentences:\n",
    "        encoded_def = urllib.parse.quote(definition_sent)\n",
    "        spar_output = requests.get(f\"http://localhost:8000/predict_objects/{encoded_def}\").json()\n",
    "        try:\n",
    "            spar_objects = spar_output[\"prediction\"]['obj']\n",
    "            cleaned_objs = util.custom_cleaning_rules(spar_objects)\n",
    "            for obj in cleaned_objs:\n",
    "                complete_dict[k]['def_terms'] += cleaned_objs\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5b3b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# approved_documents\n",
    "object_dict = {}\n",
    "for row in definitions.iterrows():\n",
    "    idx, (term, definition, alt_labels, note) = row\n",
    "    objects = parse_definition(definition)\n",
    "    \n",
    "    object_dict[term] = objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739d3e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#     term = term if term.isupper() else term\n",
    "    \n",
    "#     for obj in objects:\n",
    "#         obj = obj if obj.isupper() else obj\n",
    "#         # add the object to the graph \n",
    "#         obj_uid, new_uid = ua.assign_UID(obj)\n",
    "#         if new_uid:\n",
    "#             mygraph = add_prefLabel(mygraph, obj_uid, obj)\n",
    "        \n",
    "#         # add relation between term and mygraph\n",
    "            \n",
    "      # ====      \n",
    "#     def add_related(graph, jargon_uid, other_term_uid, jargon_namespace: Namespace=JARGON, related_namespace: Namespace=JARGON):\n",
    "#     graph.add((jargon_namespace[jargon_uid], SKOS.related, related_namespace[other_term_uid]))\n",
    "#     graph.add((related_namespace[other_term_uid], SKOS.related, jargon_namespace[jargon_uid]))\n",
    "#     return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22864cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c6d627",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91da428b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff71a8d7",
   "metadata": {},
   "source": [
    "### Step 4: Add relevant wikipedia nodes and to graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f79113f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO merge same defintion, us rdfs:a \n",
    "# TODO merge same defintion, us rdfs:a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0ec590",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5da37ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a600fd19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d33d1178",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9290c9d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
