{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3659dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, List, Any, Optional, Dict\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from textblob import TextBlob\n",
    "\n",
    "from utilities import cleaning_utils\n",
    "from utilities.customdocument import CustomDocument"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2bc5a7",
   "metadata": {},
   "source": [
    "### Grab acronyms from text (naive SPaR based approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6161bf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load our domain terms and foreground corpus\n",
    "file_names_foreground_corpus = [\"merged_approved.json\"]\n",
    "corpus_fp = Path.cwd().joinpath(\"data\", \"converted_documents\")\n",
    "foreground_corpus = [CustomDocument.load_document(corpus_fp.joinpath(file_name)) for file_name in file_names_foreground_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d99dbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_data_fp = Path(\"data/graph_data/\")\n",
    "domain_terms = pickle.load(open(graph_data_fp.joinpath('domain_terms.pkl'), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fb27865",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 5332/5332 [00:07<00:00, 683.76it/s]\n"
     ]
    }
   ],
   "source": [
    "# Grab the potential acronyms for spans\n",
    "span_acronym_dict = {}\n",
    "for span in tqdm(domain_terms):\n",
    "    span_acronym_dict[span] = []\n",
    "    for content in foreground_corpus[0].all_contents:\n",
    "        if span in content.text:\n",
    "            # now we start looking for \n",
    "            text_splits = content.text.split(span)\n",
    "            for subsequent_text in text_splits[1:]:\n",
    "                potential_abbrev = re.match(r\"^\\s+\\([A-Z]+\\)\", subsequent_text)\n",
    "                if potential_abbrev:\n",
    "                    potential_abbrev = potential_abbrev.group(0).split('(', 1)[1][:-1]\n",
    "                    span_acronym_dict[span].append(potential_abbrev)\n",
    "\n",
    "    span_acronym_dict[span] = list(set(span_acronym_dict[span]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e72fce1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll filter out some of the lower quality acronyms\n",
    "acronym_dict = {}\n",
    "for span, acronym_list in span_acronym_dict.items():\n",
    "    if acronym_list:\n",
    "        acronym_list = list(set(acronym_list))\n",
    "        for acronym in acronym_list:\n",
    "            if acronym not in acronym_dict:\n",
    "                acronym_dict[acronym] = [span]\n",
    "            else:\n",
    "                acronym_dict[acronym].append(span) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "267ad944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some examples of acronyms extracted automatically\n",
      "BRE: ['Building Research Establishment']\n",
      "LEV: ['Local Exhaust Ventilation', 'Ventilation']\n",
      "BREL: ['Part L']\n",
      "N: ['people']\n",
      "DHW: ['domestic hot water', 'hot water']\n",
      "REI: ['fire resistance', 'stability']\n",
      "SEER: ['ratio', 'seasonal energy efficiency ratio']\n",
      "VST: ['temperature']\n",
      "FLA: ['Football Licensing Authority']\n",
      "BRUKL: ['Part L']\n",
      "We'll disregard the low quality of acronyms for now\n"
     ]
    }
   ],
   "source": [
    "print(\"Some examples of acronyms extracted automatically\")\n",
    "[print(f\"{k}: {acronym_dict[k]}\")  for k in random.sample(list(acronym_dict.keys()), 10)]\n",
    "print(\"We'll disregard the low quality of acronyms for now\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ece11e",
   "metadata": {},
   "source": [
    "* As we can see above, there are still many domain-specific patterns that SPaR.txt should be taught to recognise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35c9b84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_acronym(acronym: str, span: str):\n",
    "    capitals = [c for c in acronym]\n",
    "    words = [str(w) for w in TextBlob(span).words]\n",
    "    \n",
    "    for capital, word, in zip(capitals, words):\n",
    "        if capital != word[0]:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff2219f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some examples of acronyms extracted automatically\n",
      "WRAS: ['Water Regulations Advisory Scheme']\n",
      "BRE: ['Building Research Establishment']\n",
      "BCB: ['Body', 'Building Control Body']\n",
      "FLA: ['Football Licensing Authority']\n",
      "UKAS: ['United Kingdom Accreditation Service']\n",
      "Total number of cleaned acronyms found:  11\n"
     ]
    }
   ],
   "source": [
    "# We'll filter out some of the lower quality acronyms\n",
    "acronym_dict = {}\n",
    "for span, acronym_list in span_acronym_dict.items():\n",
    "    if acronym_list:\n",
    "        acronym_list = list(set(acronym_list))\n",
    "        for acronym in acronym_list:\n",
    "            if is_acronym(acronym, span):\n",
    "                if acronym not in acronym_dict:\n",
    "                    acronym_dict[acronym] = [span]\n",
    "                else:\n",
    "                    acronym_dict[acronym].append(span)\n",
    "                \n",
    "print(\"Some examples of acronyms extracted automatically\")\n",
    "[print(f\"{k}: {acronym_dict[k]}\")  for k in random.sample(list(acronym_dict.keys()), 5)]            \n",
    "print(\"Total number of cleaned acronyms found: \", len(acronym_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fc55ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_data_fp = Path.cwd().joinpath(\"data\", \"graph_data\")\n",
    "with open(graph_data_fp.joinpath(\"acronyms_found_in_text.pkl\"), 'wb') as f:\n",
    "    pickle.dump(acronym_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0021479c",
   "metadata": {},
   "source": [
    "### Prepare Uniclass terms that occur in the Merged Approved Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86547907",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "We read Uniclass terms from a .ttl file that we have previoulsy prepared. Could switch to grabbing them from .csv as well.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a49c935",
   "metadata": {},
   "source": [
    "Source of Uniclass 2015 as .csv file: [https://buildig.com/uniclass-2015/](https://buildig.com/uniclass-2015/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55dd2529",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_ttl_lines(text):\n",
    "    groups = []\n",
    "    current_group = []\n",
    "    for idx, line in enumerate(text.split(\"\\n\")):\n",
    "        if line == '':\n",
    "            if current_group:\n",
    "                groups.append(current_group)\n",
    "            current_group = []\n",
    "        else:\n",
    "            current_group.append(line)\n",
    "            if idx+1 == len(text.split(\"\\n\")):\n",
    "                groups.append(current_group)\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07ecac12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_uids_and_labels_with_definition(groups):\n",
    "    uid_dict = {}\n",
    "    for g in groups:\n",
    "        if any([line.startswith('  skos:prefLabel') for line in g]):\n",
    "            # only use group if a prefLabel exists\n",
    "            pref_label = ''\n",
    "            alt_labels = []\n",
    "            definition = ''\n",
    "            for line in g:\n",
    "                if line.startswith('  skos:prefLabel'):\n",
    "                    pref_label = line.split('\"')[1]\n",
    "                elif line.startswith('  skos:altLabel'):\n",
    "                    labels = line.split('\"')[1::2]\n",
    "                    alt_labels += labels\n",
    "                elif line.startswith('  skos:definition'):\n",
    "                    definition = line.split('\"')[1]\n",
    "\n",
    "            if pref_label:\n",
    "                uid = g[0].split()[0].split(\":\")[1]\n",
    "                uid_dict[uid] = {'pref_label': pref_label, \n",
    "                                 'alt_labels': alt_labels,\n",
    "                                 'definition': definition\n",
    "                                }\n",
    "    return uid_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81b7861c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_nodes(vocab_name):\n",
    "    processed_file = Path(f\"{vocab_name}.json\")\n",
    "    \n",
    "    # check if file as processed before \n",
    "    NODES_LOADED = False\n",
    "    if processed_file.exists():\n",
    "        with open(processed_file) as f:\n",
    "            graph_dict = json.load(f)\n",
    "            \n",
    "        NODES_LOADED = True\n",
    "        print(f\"Loaded nodes and neighbours for: {vocab_name}\") \n",
    "    else: \n",
    "        print(f\"Will have to grab nodes for: {vocab_name}\")\n",
    "    \n",
    "        # compute the neighbours for each node\n",
    "        graph_dict = {}\n",
    "        \n",
    "        print(f\"Working on file: {vocab_name}\")\n",
    "        with open(vocab_name, 'r') as f:\n",
    "            text =  f.read()\n",
    "            \n",
    "        groups = group_ttl_lines(text)\n",
    "        print(\"Collecting nodes with definitions from dict\")\n",
    "        graph_dict = grab_uids_and_labels_with_definition(groups)\n",
    "        \n",
    "        # save the dictionary somewhere for reloading\n",
    "        with open(processed_file, 'w') as f:\n",
    "            json.dump(graph_dict, f)\n",
    "    return graph_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18ce995a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded nodes and neighbours for: /Users/rubenkruiper/dev/irec/data/term_extraction_input/uniclass_2015.ttl\n"
     ]
    }
   ],
   "source": [
    "uniclass_dict = grab_nodes(Path.cwd().joinpath(\"data\", \"term_extraction_input\", \"uniclass_2015.ttl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c685a32b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15020"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(uniclass_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f190b834",
   "metadata": {},
   "source": [
    "* Provide some insight in the number of uniclass terms found in the Merged Approved Documents\n",
    "  * We will lowercase the terms, otherwise it is very unlikely that classes are found verbatim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38610a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1455/1455 [00:41<00:00, 35.13it/s]\n"
     ]
    }
   ],
   "source": [
    "uniclass_terms_in_text = {}\n",
    "for doc in foreground_corpus:\n",
    "    for content in tqdm(doc.all_contents):\n",
    "        for uid, uiv in uniclass_dict.items():\n",
    "            uterm = uiv['pref_label']\n",
    "            if uterm.lower() in content.text.lower():  # lowercase everything to increase likelihood\n",
    "                uniclass_terms_in_text[uid] = uiv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ad8fe5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Uniclass terms found in the Merged Approved Documents: 598 (3.98%)\n",
      "Examples of Uniclass terms found in the Merged Approved Documents:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Standards',\n",
       " 'Thin',\n",
       " 'Stairs',\n",
       " 'Pedestrian routes',\n",
       " 'Water heaters',\n",
       " 'Landings',\n",
       " 'Garages',\n",
       " 'Beds',\n",
       " 'Systems engineer',\n",
       " 'Escalators']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentage = (len(uniclass_terms_in_text)/len(uniclass_dict)) * 100\n",
    "print(\"Number of Uniclass terms found in the Merged Approved Documents: {} ({:.2f}%)\".format(len(uniclass_terms_in_text), percentage))\n",
    "print(\"Examples of Uniclass terms found in the Merged Approved Documents:\")\n",
    "random.sample([x['pref_label'] for x in uniclass_terms_in_text.values()], 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13e8265",
   "metadata": {},
   "source": [
    "* store the Uniclass terms that we found in the foreground corpus, we want to add them to our graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1102b9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_data_fp = Path.cwd().joinpath(\"data\", \"graph_data\")\n",
    "with open(graph_data_fp.joinpath(\"uniclass_terms_in_text.pkl\"), 'wb') as f:\n",
    "    pickle.dump(uniclass_terms_in_text, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c050c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniclass_mwes = []\n",
    "uniclass_single = [{}]\n",
    "for uid, uiv in uniclass_dict.items():\n",
    "    uterm = uiv['pref_label']\n",
    "    if len(uterm.split(' ')) > 1:  # lowercase everything to increase likelihood\n",
    "        uniclass_mwes.append(uterm)\n",
    "    else:\n",
    "        uniclass_single.append(uterm)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0cf59f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of MWEs: 14100 (93.87%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Gas pressure switches',\n",
       " 'Ductwork installing',\n",
       " 'Ceremonial worship activities',\n",
       " 'Storm water gravity drainage systems',\n",
       " 'Gas waste collection spaces',\n",
       " 'Bollards and impact protectors',\n",
       " 'Water skiing courses',\n",
       " 'Glass-to-glass clips',\n",
       " 'Solid waste disposal products',\n",
       " 'Render stops']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some insight in number of MWEs in Uniclass\n",
    "print(\"Number of MWEs: {} ({:.2f}%)\".format(len(uniclass_mwes), (len(uniclass_mwes)/len(uniclass_mwes+uniclass_single))*100))\n",
    "random.sample(uniclass_mwes, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1170df60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
