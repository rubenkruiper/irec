{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "788a8bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Any, Optional, Dict\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import glob\n",
    "import random\n",
    "import urllib\n",
    "import requests\n",
    "import subprocess\n",
    "import concurrent.futures\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "from textblob import TextBlob\n",
    "from threading import current_thread\n",
    "from collections import Counter\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import utils\n",
    "import embedding\n",
    "import IDF_computation\n",
    "from customdocument import CustomDocument"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d081bf43",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> \n",
    "We will run our term extraction on the Merged Approved Documents, the .pdf file can be found in the `data/` directory. For filtering of out-of-domain terms we will also process a set of EU regulations for medical device design, the .html files for these can be found in the same directory.\n",
    "</div>\n",
    "\n",
    "1. Preprocessing will consist only of removing headers/footers from PDF files. \n",
    "\n",
    "2. Candidate terms are identified using SPaR.txt (Kruiper et al., 2021), sentence splitting is done with the PunkSentTokenizer (Strunk, 2006).\n",
    "\n",
    "3. Filtering of term candidates consists of:\n",
    "  * a set of regular expressions found in utils.py\n",
    "  * clustering of terms found in (1) the Approved Documents, and (2) a set of EU regulations for medical device design; any clusters containing terms from (2) will be designated as terms that are irrelevant to the AEC domain.\n",
    "  \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996a98d0",
   "metadata": {},
   "source": [
    "### 1 Preprocessing: get text from PDF and HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f74d6fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approved Documents: data/input/The Merged Approved Documents.pdf\n",
      "Reference corpus: ['data/input/EUR-Lex - 31993L0042 - EN.html', 'data/input/CELEX 32017R0746 EN TXT.html', 'data/input/CELEX 32017R0745 EN TXT.html', 'data/input/EUR-Lex - 31998L0079 - EN.html', 'data/input/EUR-Lex - 31990L0385 - EN.html']\n"
     ]
    }
   ],
   "source": [
    "merged_approved_pdf_file = glob.glob(\"data/input/*.pdf\")[0]\n",
    "eu_html_files = glob.glob(\"data/input/*.html\")\n",
    "print(f\"Approved Documents: {merged_approved_pdf_file}\")\n",
    "print(f\"Reference corpus: {eu_html_files}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271bdc02",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "First we grab the text from the Merged Approved Documents pdf file. Our implementation is based on the pdf conversion pipeline in Haystack.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e0d20e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf(file_path: Path, layout: bool = True, encoding: Optional[str] = \"Latin1\") -> List[str]:\n",
    "        \"\"\"\n",
    "        Extract pages from the pdf file at file_path; based on Haystack.\n",
    "\n",
    "        :param file_path: path of the pdf file\n",
    "        :param layout: whether to retain the original physical layout for a page. If disabled, PDF pages are read in\n",
    "                       the content stream order.\n",
    "        \"\"\"\n",
    "        if layout:\n",
    "            command = [\"pdftotext\", \"-enc\", encoding, \"-layout\", str(file_path), \"-\"]\n",
    "        else:\n",
    "            command = [\"pdftotext\", \"-enc\", encoding, str(file_path), \"-\"]\n",
    "        output = subprocess.run(command, stdout=subprocess.PIPE, shell=False)  # type: ignore\n",
    "        document = output.stdout.decode(errors=\"ignore\")\n",
    "        pages = document.split(\"\\f\")\n",
    "        pages = pages[:-1]  # the last page in the split is always empty.\n",
    "        return pages\n",
    "\n",
    "def convert_pdf_to_mydoc(source_file_path: Path, \n",
    "                         output_file_path: Path, \n",
    "                         meta: Optional[Dict[str, str]] = None,\n",
    "                         clean_header_footer: Optional[bool] = True,\n",
    "                         encoding: Optional[str] = \"Latin1\") -> CustomDocument:\n",
    "        \"\"\"\n",
    "        Extract pages from the pdf file at file_path; based on Haystack.\n",
    "\n",
    "        :param output_file_path:    Path to the .json file to store the converted file.\n",
    "        :param source_file_path:    Path to the .pdf file you want to convert\n",
    "        :param meta: Optional dictionary with metadata that shall be attached to all resulting documents.\n",
    "                     Can be any custom keys and values.\n",
    "        :param encoding: Encoding that will be passed as -enc parameter to pdftotext. \"Latin 1\" is the default encoding\n",
    "                         of pdftotext. While this works well on many PDFs, it might be needed to switch to \"UTF-8\" or\n",
    "                         others if your doc contains special characters (e.g. German Umlauts, Cyrillic characters ...).\n",
    "                         Note: With \"UTF-8\" we experienced cases, where a simple \"fi\" gets wrongly parsed as\n",
    "                         \"xef\\xac\\x81c\" (see test cases). That's why we keep \"Latin 1\" as default here.\n",
    "                         (See list of available encodings by running `pdftotext -listenc` in the terminal)\n",
    "        \"\"\"\n",
    "        pages = read_pdf(source_file_path, layout=True, encoding=encoding)\n",
    "\n",
    "        if not pages:\n",
    "            # empty input file\n",
    "            return None\n",
    "        \n",
    "        pages = [\"\\n\".join(p.splitlines()) for p in pages]\n",
    "\n",
    "        # splitting text happens during preprocessing, so no split_size passed here;\n",
    "        # split_size will be set to -1 during conversion.\n",
    "        document = CustomDocument(output_file_path, source_file_path, split_size=-1)\n",
    "\n",
    "        if clean_header_footer:\n",
    "            pages = utils.find_and_remove_header_footer(\n",
    "                pages, n_chars=300, n_first_pages_to_ignore=2, n_last_pages_to_ignore=1\n",
    "            )\n",
    "        \n",
    "        print(\"Converted PDF file to pages of text, combining to a single CustomDocument to keep track of page nrs.\")\n",
    "        for page_idx, page in tqdm(enumerate(pages)):\n",
    "            # no splitting here yet, so simply using page_nr as a place holder and split_id is left blank\n",
    "            page_nr = str(page_idx + 1)\n",
    "            document.add_content(text=page, \n",
    "                                 page_nr=page_nr, \n",
    "                                 doc_title=source_file_path.rsplit('/',1)[1])   # we're using the pdf file name for simplicity\n",
    "\n",
    "        return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71184fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted PDF file to pages of text, combining to a single CustomDocument to keep track of page nrs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1274it [00:00, 232783.42it/s]\n"
     ]
    }
   ],
   "source": [
    "merged_approved_document = convert_pdf_to_mydoc(merged_approved_pdf_file, \"data/converted_documents/merged_approved.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1473ee8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum character length for a single block of text: 10716\n"
     ]
    }
   ],
   "source": [
    "print(f\"Maximum character length for a single block of text: {max([len(c.text) for c in merged_approved_document.all_contents])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203400ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84117041",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Second, we grab the text from the EU regulation HTML files. Because the text in HTML files isn't split into pages, the blocks of text are much longer.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5bb53f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_HTML_text_simple(file):\n",
    "    \"\"\"\n",
    "    All text in the EU htmls seems to be captured neatly in <p> tags, we don't care about structure currently.\n",
    "    We do remove all unicode characters, see `utils.remove_unicode_chars()`.\n",
    "    \"\"\" \n",
    "    with open(file, 'r') as f:\n",
    "        text = f.read()\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    return [utils.remove_unicode_chars(x.text) for x in soup.body.find_all('p')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6630e6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_html_to_mydoc(source_file_path: Path, \n",
    "                          output_file_path: Path) -> CustomDocument:\n",
    "    \n",
    "    document = CustomDocument(output_file_path, source_file_path, split_size=-1)\n",
    "    document_paragraphs = []\n",
    "    list_of_paragraphs = grab_HTML_text_simple(html_file)\n",
    "    for paragraph in list_of_paragraphs:\n",
    "        if paragraph.strip() != '':\n",
    "            document_paragraphs.append(paragraph)\n",
    "    \n",
    "    for paragraph_idx, paragraph in tqdm(enumerate(document_paragraphs)):\n",
    "            # no splitting here yet, so simply using page_nr as a place holder and split_id is left blank\n",
    "            paragraph_nr = str(paragraph_idx + 1)\n",
    "            document.add_content(text=paragraph, \n",
    "                                 page_nr=paragraph_nr, \n",
    "                                 doc_title=source_file_path) # we're using the html file name for simplicity\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91b68b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                               | 0/5 [00:00<?, ?it/s]\n",
      "826it [00:00, 296262.62it/s]\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "4344it [00:00, 39607.27it/s]\u001b[A\n",
      " 40%|██████████████████████████████████████████████████████████████████▊                                                                                                    | 2/5 [00:00<00:01,  2.14it/s]\n",
      "4799it [00:00, 34982.48it/s]\n",
      " 60%|████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                  | 3/5 [00:01<00:01,  1.48it/s]\n",
      "623it [00:00, 257038.30it/s]\n",
      "\n",
      "511it [00:00, 247350.18it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.53it/s]\n"
     ]
    }
   ],
   "source": [
    "eu_regulation_documents = []\n",
    "for html_file in tqdm(eu_html_files):\n",
    "    outfile = f\"data/converted_documents/{html_file.rsplit('/',1)[1]}.json\"\n",
    "    eu_regulation_documents.append(convert_html_to_mydoc(html_file, outfile))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "033634ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum character length for a single paragraph: 143428\n"
     ]
    }
   ],
   "source": [
    "print(f\"Maximum character length for a single paragraph: {max([len(c.text) for d in eu_regulation_documents for c in d.all_contents])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77347abc",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Third, if the output document doesn't exist (yet), we save the ConvertedDocument.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24237e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "foreground_corpus = [merged_approved_document]\n",
    "background_corpus = eu_regulation_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a00ed81",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in foreground_corpus + background_corpus:\n",
    "    if not os.path.exists(d.output_fp):\n",
    "        d.write_document()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f6e56a",
   "metadata": {},
   "source": [
    "### 2 Term extraction: identify object spans with SPaR.txt\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "For each of the contents in a document, run SPaR.txt for object identification. To this end, we split the text into sentences and pass a sentence to a running instance fo a SPaR.txt predictor.\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edf22a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download SPaR.txt if required\n",
    "if not os.path.exists(\"SPaR.txt/README.md\"):\n",
    "    !git clone https://github.com/rubenkruiper/SPaR.txt.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b19aa4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imp\n",
    "with open('SPaR.txt/spar_predictor.py', 'rb') as fp:\n",
    "    spar_predictor = imp.load_module(\n",
    "        'spar_predictor', fp, 'SPaR.txt.spar_predictor.py',\n",
    "        ('.py', 'rb', imp.PY_SOURCE)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9daef1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# trains a model if needed, otherwise loads from archive; \n",
    "# - best F1 on dev/validation in the paper is 80,96 trained on a GPU, CPU will be a bit lower ~77.x I think\n",
    "sp = spar_predictor.SparPredictor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0206ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'obj': ['An example sentence', 'ACC terminology', 'the British Standards']}\n",
      "Parsing took 0.11214494705200195\n"
     ]
    }
   ],
   "source": [
    "example = \"An example sentence to show how ACC terminology will be extracted from the British Standards.\"\n",
    "start_time = time.time()\n",
    "# prepare instance and run model on single instance\n",
    "docid = ''                  # ToDo - add doc_id during pre_processing?\n",
    "token_list = sp.predictor._dataset_reader.tokenizer.tokenize(example)\n",
    "instance = sp.predictor._dataset_reader.text_to_instance(docid,\n",
    "                                                      example,\n",
    "                                                      token_list,\n",
    "                                                      sp.predictor._dataset_reader._token_indexer)\n",
    "result = sp.predictor.predict_instance(instance)\n",
    "printable_result = sp.parse_output(result, ['obj'])\n",
    "print(printable_result)\n",
    "print(\"Parsing took {}\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5f7c27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparInstance:\n",
    "    def __init__(self):\n",
    "        self.sp = spar_predictor.SparPredictor()\n",
    "    \n",
    "    def call(self, input_str:str=''):\n",
    "        if input_str:\n",
    "            # prepare instance and run model on single instance\n",
    "            docid = ''  # ToDo - add doc_id during pre_processing?\n",
    "            token_list = self.sp.predictor._dataset_reader.tokenizer.tokenize(input_str)\n",
    "\n",
    "            # truncating the input to SPaR.txt to maximum 512 tokens\n",
    "            token_length = len(token_list)\n",
    "            if token_length > 512:\n",
    "                token_list = token_list[:511] + [token_list[-1]]\n",
    "                token_length = 512\n",
    "\n",
    "            instance = self.sp.predictor._dataset_reader.text_to_instance(docid, input_str, token_list,\n",
    "                                                              self.sp.predictor._dataset_reader._token_indexer)\n",
    "            result = self.sp.predictor.predict_instance(instance)\n",
    "            printable_result = self.sp.parse_output(result, ['obj'])\n",
    "            return {\n",
    "                    \"prediction\": printable_result,\n",
    "                    \"num_input_tokens\": token_length,\n",
    "            }\n",
    "            \n",
    "        # If the input is None, or too long, return an empty list of objects\n",
    "        return {\n",
    "                \"prediction\": {'obj': []},\n",
    "                \"num_input_tokens\": 0\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fb733c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TermExtractor:\n",
    "    \n",
    "    def __init__(self, split_length=300, max_num_cpu_threads=4):\n",
    "        \"\"\"\n",
    "        Initialise SPaR.txt predictors `max_num_cpu_threads` \n",
    "        \"\"\"\n",
    "        self.split_length = split_length   # in number of tokens\n",
    "        self.max_num_cpu_threads = max_num_cpu_threads\n",
    "        self.PREDICTORS = []\n",
    "        for i in range(max_num_cpu_threads + 1):\n",
    "            self.PREDICTORS.append(SparInstance())\n",
    "    \n",
    "    \n",
    "    def process_sentence(self, sentence: str = ''):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        predictor_to_use = int(current_thread().name.rsplit('_', 1)[1])\n",
    "        spartxt = self.PREDICTORS[predictor_to_use]\n",
    "\n",
    "        # SPaR doesn't handle ALL uppercase sentences well, which the OCR system sometimes outputs    \n",
    "        sentence = sentence.lower() if sentence.isupper() else sentence\n",
    "        prediction_dict =  spartxt.call(sentence)\n",
    "        if not prediction_dict:\n",
    "            return []\n",
    "\n",
    "        pred_labels = prediction_dict[\"prediction\"]\n",
    "        return pred_labels['obj']\n",
    "        \n",
    "\n",
    "    def split_into_sentences_and_run_spar(self, input_document):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        print(f\"Working on: {input_document.source_fp}\")\n",
    "        content_as_list_of_dicts = input_document.to_list_of_dicts()\n",
    "        total_number_of_sentences_found = 0\n",
    "        content_idx = 0\n",
    "        for content_dict in tqdm(content_as_list_of_dicts):\n",
    "\n",
    "            text = ' '.join([x for x in content_dict[\"content\"].split(' ') if x != ''])\n",
    "            # some really long paragraphs in the EU regulations are summations that should be split at ';'\n",
    "            if len(text) > 3000:\n",
    "                text = text.replace(\";\", \".\\n\")\n",
    "\n",
    "            # We'll split into sentences even if this has been done before, it doesn't take long\n",
    "            sentences = []\n",
    "            for part in text.split('\\n'):\n",
    "                # split into sentences using PunktSentTokenizer (TextBlob implements NLTK's version under the hood) \n",
    "                sentences += [str(s) for s in TextBlob(part).sentences if len(str(s)) > 10]\n",
    "\n",
    "            content_dict[\"meta\"][\"sentences\"] = '###'.join(sentences)\n",
    "                \n",
    "            total_number_of_sentences_found += len(sentences)\n",
    "\n",
    "            # process sentences in the content and add SPaR.txt object tags to the content dict.        \n",
    "            if not content_dict[\"meta\"][\"SPaR_labels\"]:\n",
    "                with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_num_cpu_threads) as executor:\n",
    "                    futures = [executor.submit(self.process_sentence, sentences[idx]) for idx in range(len(sentences))]\n",
    "\n",
    "                content_spar_objects = [f.result() for f in futures]\n",
    "                content_dict[\"meta\"][\"SPaR_labels\"] = ', '.join([tag for tags in content_spar_objects for tag in tags])\n",
    "                \n",
    "            # immediately update the list of content_dicts and every X iterations we save the file \n",
    "            content_as_list_of_dicts[content_idx] = content_dict\n",
    "            if content_idx // 5 == 0:\n",
    "                converted_document.replace_contents(content_as_list_of_dicts)\n",
    "                converted_document.write_document()\n",
    "            \n",
    "            content_idx += 1\n",
    "\n",
    "        print(f\"Number of sentences found: {total_number_of_sentences_found}\")\n",
    "        converted_document.replace_contents(content_as_list_of_dicts)\n",
    "        converted_document.write_document()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a7dc2f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "te = TermExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b3d086b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on: data/input/The Merged Approved Documents.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1274/1274 [00:03<00:00, 321.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences found: 55273\n",
      "Working on: data/input/EUR-Lex - 31993L0042 - EN.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 826/826 [00:00<00:00, 5123.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences found: 926\n",
      "Working on: data/input/CELEX 32017R0746 EN TXT.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4344/4344 [00:02<00:00, 1786.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences found: 3404\n",
      "Working on: data/input/CELEX 32017R0745 EN TXT.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4799/4799 [00:01<00:00, 2651.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences found: 3837\n",
      "Working on: data/input/EUR-Lex - 31998L0079 - EN.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 623/623 [00:00<00:00, 2068.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences found: 1255\n",
      "Working on: data/input/EUR-Lex - 31990L0385 - EN.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 511/511 [00:00<00:00, 4580.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences found: 771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run SPaR.txt on all documents and write to file\n",
    "for converted_document in foreground_corpus + background_corpus:\n",
    "    # re-load the document from file, to make sure we don't overwrite existing SPaR.txt labels\n",
    "    converted_document = converted_document.load_document(converted_document.output_fp)\n",
    "    te.split_into_sentences_and_run_spar(converted_document)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4835185f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Number of sentences found in: <ul>\n",
    "    <li>Merged Approved documents: 63071</li>\n",
    "    <li>Background corpus (1366+5753+6467+1496+1076): 16158</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b15878",
   "metadata": {},
   "source": [
    "### 3 Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b511762d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "First, load all terms from the processed files.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4fb5373f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of foreground terms: 140495\n",
      "Total number of UNIQUE foreground terms: 45609\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('the', 2582),\n",
       " ('a', 1301),\n",
       " ('buildings', 1115),\n",
       " ('the building', 888),\n",
       " ('a .', 825),\n",
       " ('b', 677),\n",
       " ('guidance', 544),\n",
       " ('a building', 468),\n",
       " ('dwellings', 407),\n",
       " ('building', 404)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foreground_terms_lists = [c.NER_labels for d in foreground_corpus for c in d.load_document(d.output_fp).all_contents]\n",
    "foreground_terms = [t for t_list in foreground_terms_lists for t in t_list]\n",
    "foreground_terms_c = Counter(foreground_terms)\n",
    "print(f\"Total number of foreground terms: {len(foreground_terms)}\")\n",
    "print(f\"Total number of UNIQUE foreground terms: {len(foreground_terms_c)}\")\n",
    "foreground_terms_c.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8006f6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of background terms: 73345\n",
      "Total number of UNIQUE background terms: 10522\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('the', 2437),\n",
       " ('devices', 1488),\n",
       " ('the manufacturer', 1120),\n",
       " ('the device', 1118),\n",
       " ('-', 792),\n",
       " ('Regulation', 624),\n",
       " ('the notified body', 554),\n",
       " ('information', 522),\n",
       " ('Member States', 457),\n",
       " ('the Commission', 423)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "background_terms_lists = [c.NER_labels for d in background_corpus for c in d.load_document(d.output_fp).all_contents]\n",
    "background_terms = [t for t_list in background_terms_lists for t in t_list]\n",
    "background_terms_c = Counter(background_terms)\n",
    "print(f\"Total number of background terms: {len(background_terms)}\")\n",
    "print(f\"Total number of UNIQUE background terms: {len(background_terms_c)}\")\n",
    "background_terms_c.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464451e8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Second, clean the terms with the regular expressions we've defined in utils.py\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2503708d",
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_filter = utils.RegexFilter()\n",
    "def run_filters(input_counter):\n",
    "    cleaned_counter = Counter()\n",
    "    for k, v in input_counter.items():\n",
    "        # terms should occur twice at least\n",
    "        if v < 2:\n",
    "            continue\n",
    "\n",
    "        # todo; clean up these util functions and how to call them|\n",
    "        _, k = regex_filter.run_filter(k)\n",
    "        if k:\n",
    "            cleaned_k = utils.custom_cleaning_rules(k)\n",
    "            if cleaned_k:\n",
    "                cleaned_counter[cleaned_k[0]] = v\n",
    "    return cleaned_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2e42c7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13526\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('the building', 888),\n",
       " ('a building', 468),\n",
       " ('the Building Regulations', 359),\n",
       " ('requirements', 330),\n",
       " ('part', 280),\n",
       " ('document', 273),\n",
       " ('work', 272),\n",
       " ('the requirements', 264),\n",
       " ('building work', 229),\n",
       " ('the guidance', 229)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_foreground_terms_c = run_filters(foreground_terms_c)\n",
    "print(len(cleaned_foreground_terms_c))\n",
    "cleaned_foreground_terms_c.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "899b1cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6026\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('devices', 1488),\n",
       " ('the manufacturer', 1120),\n",
       " ('the device', 1118),\n",
       " ('the notified body', 554),\n",
       " ('Member States', 457),\n",
       " ('the Commission', 423),\n",
       " ('accordance', 407),\n",
       " ('the market', 406),\n",
       " ('a device', 368),\n",
       " ('the requirements', 323)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_background_terms_c = run_filters(background_terms_c)\n",
    "print(len(cleaned_background_terms_c))\n",
    "cleaned_background_terms_c.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406d6ca8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Third, we cluster the embeddings for both foreground and background terms with KMeans.\n",
    "</div>\n",
    "\n",
    "\n",
    "* Note: embeddings will be IDF weighted (IDF weights over both foreground and background corpora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "10278f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_terms_c = cleaned_foreground_terms_c + cleaned_background_terms_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a1000a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('devices', 1520),\n",
       " ('the manufacturer', 1158),\n",
       " ('the device', 1124),\n",
       " ('the building', 888),\n",
       " ('the requirements', 587),\n",
       " ('the notified body', 554),\n",
       " ('accordance', 546),\n",
       " ('requirements', 507),\n",
       " ('a building', 468),\n",
       " ('Member States', 459)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_terms_c.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d8d0d4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute IDF weights\n",
    "sentence_lists = [c.sentences for d in foreground_corpus + background_corpus for c in d.load_document(d.output_fp).all_contents]\n",
    "all_sentences = [s for sent_list in sentence_lists for s in sent_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7bf4a15f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68923"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "411b8d88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '', '', '', '', '', '', '', '', '']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5b1959bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'IdfComputer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/lp/l_mzhpjs6bg95plfkl_n_vsc0000gn/T/ipykernel_14906/3872530839.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mIDF_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIdfComputer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIDF_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcluster_bert_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconversion_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconversion_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mIDF_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIDF_c\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_IDF_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'IdfComputer' is not defined"
     ]
    }
   ],
   "source": [
    "IDF_c = IdfComputer(IDF_path, bert_model=cluster_bert_model, conversion_type=conversion_type)\n",
    "IDF_path = IDF_c.compute_IDF_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f20d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(cluster_bert_model)\n",
    "embedder = embedding.Embedder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a669a4a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce59ea93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
