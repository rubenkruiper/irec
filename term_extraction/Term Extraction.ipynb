{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "788a8bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KM cuda not found, defaulting to sklearn CPU version of kmeans\n"
     ]
    }
   ],
   "source": [
    "from typing import Union, List, Any, Optional, Dict\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "import urllib\n",
    "import requests\n",
    "import subprocess\n",
    "import concurrent.futures\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from textblob import TextBlob\n",
    "from threading import current_thread\n",
    "from collections import Counter\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "try:\n",
    "    from libKMCUDA import kmeans_cuda\n",
    "except:\n",
    "    print(\"KM cuda not found, defaulting to sklearn CPU version of kmeans\")\n",
    "    kmeans_cuda = None\n",
    "\n",
    "from utils import cleaning_utils\n",
    "from utils import cluster_utils\n",
    "from utils import IDF_computation\n",
    "from utils import embedding_utils as embedding\n",
    "from utils.customdocument import CustomDocument"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d081bf43",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> \n",
    "We will run our term extraction on the Merged Approved Documents, the .pdf file can be found in the `data/` directory. For filtering of out-of-domain terms we will also process a set of EU regulations for medical device design, the .html files for these can be found in the same directory.\n",
    "</div>\n",
    "\n",
    "1. Preprocessing will consist only of removing headers/footers from PDF files. \n",
    "\n",
    "2. Candidate terms are identified using SPaR.txt (Kruiper et al., 2021), sentence splitting is done with the PunkSentTokenizer (Strunk, 2006).\n",
    "\n",
    "3. Filtering of term candidates consists of:\n",
    "  * a set of regular expressions found in utils.py\n",
    "  * clustering of terms found in (1) the Approved Documents, and (2) a set of EU regulations for medical device design; any clusters containing terms from (2) will be designated as terms that are irrelevant to the AEC domain.\n",
    "  \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996a98d0",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 1 Preprocessing: get text from PDF and HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f74d6fb",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approved Documents: data/input/The Merged Approved Documents.pdf\n",
      "Reference corpus: ['data/input/EUR-Lex - 31993L0042 - EN.html', 'data/input/CELEX 32017R0746 EN TXT.html', 'data/input/CELEX 32017R0745 EN TXT.html', 'data/input/EUR-Lex - 31998L0079 - EN.html', 'data/input/EUR-Lex - 31990L0385 - EN.html']\n"
     ]
    }
   ],
   "source": [
    "merged_approved_pdf_file = glob.glob(\"data/input/*.pdf\")[0]\n",
    "eu_html_files = glob.glob(\"data/input/*.html\")\n",
    "print(f\"Approved Documents: {merged_approved_pdf_file}\")\n",
    "print(f\"Reference corpus: {eu_html_files}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271bdc02",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "First we grab the text from the Merged Approved Documents pdf file. Our implementation on based on the pdf conversion pipeline in Haystack.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e0d20e7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def read_pdf(file_path: Path, layout: bool = True, encoding: Optional[str] = \"Latin1\") -> List[str]:\n",
    "        \"\"\"\n",
    "        Extract pages from the pdf file at file_path; based on Haystack.\n",
    "\n",
    "        :param file_path: path of the pdf file\n",
    "        :param layout: whether to retain the original physical layout for a page. If disabled, PDF pages are read in\n",
    "                       the content stream order.\n",
    "        \"\"\"\n",
    "        if layout:\n",
    "            command = [\"pdftotext\", \"-enc\", encoding, \"-layout\", str(file_path), \"-\"]\n",
    "        else:\n",
    "            command = [\"pdftotext\", \"-enc\", encoding, str(file_path), \"-\"]\n",
    "        output = subprocess.run(command, stdout=subprocess.PIPE, shell=False)  # type: ignore\n",
    "        document = output.stdout.decode(errors=\"ignore\")\n",
    "        pages = document.split(\"\\f\")\n",
    "        pages = pages[:-1]  # the last page in the split is always empty.\n",
    "        return pages\n",
    "\n",
    "def convert_pdf_to_mydoc(source_file_path: Path, \n",
    "                         output_file_path: Path, \n",
    "                         meta: Optional[Dict[str, str]] = None,\n",
    "                         remove_header_and_footer: Optional[bool] = True,\n",
    "                         clean_whitespace: Optional[bool] = True,\n",
    "                         clean_empty_lines: Optional[bool] = True,\n",
    "                         encoding: Optional[str] = \"Latin1\") -> CustomDocument:\n",
    "        \"\"\"\n",
    "        Extract pages from the pdf file at file_path; based on Haystack.\n",
    "\n",
    "        :param output_file_path:    Path to the .json file to store the converted file.\n",
    "        :param source_file_path:    Path to the .pdf file you want to convert\n",
    "        :param meta: Optional dictionary with metadata that shall be attached to all resulting documents.\n",
    "                     Can be any custom keys and values.\n",
    "        :param encoding: Encoding that will be passed as -enc parameter to pdftotext. \"Latin 1\" is the default encoding\n",
    "                         of pdftotext. While this works well on many PDFs, it might be needed to switch to \"UTF-8\" or\n",
    "                         others if your doc contains special characters (e.g. German Umlauts, Cyrillic characters ...).\n",
    "                         Note: With \"UTF-8\" we experienced cases, where a simple \"fi\" gets wrongly parsed as\n",
    "                         \"xef\\xac\\x81c\" (see test cases). That's why we keep \"Latin 1\" as default here.\n",
    "                         (See list of available encodings by running `pdftotext -listenc` in the terminal)\n",
    "        \"\"\"\n",
    "        pages = read_pdf(source_file_path, layout=True, encoding=encoding)\n",
    "\n",
    "        if not pages:\n",
    "            # empty input file\n",
    "            return None\n",
    "        \n",
    "        pages = [\"\\n\".join(p.splitlines()) for p in pages]\n",
    "\n",
    "        # splitting text happens during preprocessing, so no split_size passed here;\n",
    "        # split_size will be set to -1 during conversion.\n",
    "        document = CustomDocument(output_file_path, source_file_path, split_size=-1)\n",
    "        \n",
    "        print(\"Converted PDF file to pages of text, combining to a single CustomDocument to keep track of page nrs.\")\n",
    "        for page_idx, page in tqdm(enumerate(pages)):\n",
    "            \n",
    "            # some simple cleaning -- roughly based on haystack.\n",
    "            lines = page.splitlines()\n",
    "            if remove_header_and_footer:\n",
    "                # simplest way for removing header and footer \n",
    "                lines = lines[1:-2]\n",
    "\n",
    "            if clean_whitespace:\n",
    "                cleaned_lines = []\n",
    "                for line in lines:\n",
    "                    line = line.strip()\n",
    "                    cleaned_lines.append(line)\n",
    "                text = \" \".join(cleaned_lines)\n",
    "\n",
    "            if clean_empty_lines:\n",
    "                text = re.sub(r\"\\n\\n+\", \"\\n\\n\", text)\n",
    "                text = re.sub(r\"[\\s]+\", \" \", text)\n",
    "            \n",
    "            # no splitting here yet, so simply using page_nr as a place holder and split_id is left blank\n",
    "            page_nr = str(page_idx + 1)\n",
    "            document.add_content(text=text, \n",
    "                                 page_nr=page_nr, \n",
    "                                 doc_title=source_file_path.rsplit('/',1)[1])   # we're using the pdf file name for simplicity\n",
    "\n",
    "        return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71184fcf",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted PDF file to pages of text, combining to a single CustomDocument to keep track of page nrs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1274it [00:00, 5218.17it/s]\n"
     ]
    }
   ],
   "source": [
    "merged_approved_document = convert_pdf_to_mydoc(merged_approved_pdf_file, \"data/converted_documents/merged_approved.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1473ee8b",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum character length for a single block of text: 5537\n"
     ]
    }
   ],
   "source": [
    "print(f\"Maximum character length for a single block of text: {max([len(c.text) for c in merged_approved_document.all_contents])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203400ce",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84117041",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Second, we grab the text from the EU regulation HTML files. Because the text in HTML files isn't split into pages, the blocks of text are much longer.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5bb53f9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def grab_HTML_text_simple(file):\n",
    "    \"\"\"\n",
    "    All text in the EU htmls seems to be captured neatly in <p> tags, we don't care about structure currently.\n",
    "    We do remove all unicode characters, see `utils.remove_unicode_chars()`.\n",
    "    \"\"\" \n",
    "    with open(file, 'r') as f:\n",
    "        text = f.read()\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    return [cleaning_utils.remove_unicode_chars(x.text) for x in soup.body.find_all('p')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6630e6ba",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def convert_html_to_mydoc(source_file_path: Path, \n",
    "                          output_file_path: Path) -> CustomDocument:\n",
    "    \n",
    "    document = CustomDocument(output_file_path, source_file_path, split_size=-1)\n",
    "    document_paragraphs = []\n",
    "    list_of_paragraphs = grab_HTML_text_simple(html_file)\n",
    "    for paragraph in list_of_paragraphs:\n",
    "        if paragraph.strip() != '':\n",
    "            document_paragraphs.append(paragraph)\n",
    "    \n",
    "    for paragraph_idx, paragraph in tqdm(enumerate(document_paragraphs)):\n",
    "            # no splitting here yet, so simply using page_nr as a place holder and split_id is left blank\n",
    "            paragraph_nr = str(paragraph_idx + 1)\n",
    "            document.add_content(text=paragraph, \n",
    "                                 page_nr=paragraph_nr, \n",
    "                                 doc_title=source_file_path) # we're using the html file name for simplicity\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91b68b58",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                 | 0/5 [00:00<?, ?it/s]\n",
      "826it [00:00, 299050.07it/s]\n",
      "\n",
      "4344it [00:00, 279483.01it/s]\n",
      " 40%|███████████████████████████████████████████████████████████████████▌                                                                                                     | 2/5 [00:01<00:01,  1.86it/s]\n",
      "4799it [00:00, 229253.59it/s]\n",
      " 60%|█████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                   | 3/5 [00:02<00:01,  1.13it/s]\n",
      "623it [00:00, 162452.68it/s]\n",
      "\n",
      "511it [00:00, 165863.59it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  1.96it/s]\n"
     ]
    }
   ],
   "source": [
    "eu_regulation_documents = []\n",
    "for html_file in tqdm(eu_html_files):\n",
    "    outfile = f\"data/converted_documents/{html_file.rsplit('/',1)[1]}.json\"\n",
    "    eu_regulation_documents.append(convert_html_to_mydoc(html_file, outfile))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "033634ca",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum character length for a single paragraph: 143428\n"
     ]
    }
   ],
   "source": [
    "print(f\"Maximum character length for a single paragraph: {max([len(c.text) for d in eu_regulation_documents for c in d.all_contents])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77347abc",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Third, if the output document doesn't exist (yet), we save the ConvertedDocument.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24237e3e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "foreground_corpus = [merged_approved_document]\n",
    "background_corpus = eu_regulation_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a00ed81",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for d in foreground_corpus + background_corpus:\n",
    "    if not os.path.exists(d.output_fp):\n",
    "        d.write_document()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f6e56a",
   "metadata": {},
   "source": [
    "### 2 Term extraction: identify object spans with SPaR.txt\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "For each of the contents in a document, run SPaR.txt for object identification. To this end, we split the text into sentences and pass a sentence to a running instance fo a SPaR.txt predictor.\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edf22a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download SPaR.txt if required\n",
    "if not os.path.exists(\"SPaR.txt/README.md\"):\n",
    "    !git clone https://github.com/rubenkruiper/SPaR.txt.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b19aa4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imp\n",
    "with open('SPaR.txt/spar_predictor.py', 'rb') as fp:\n",
    "    spar_predictor = imp.load_module(\n",
    "        'spar_predictor', fp, 'SPaR.txt.spar_predictor.py',\n",
    "        ('.py', 'rb', imp.PY_SOURCE)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9daef1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# trains a model if needed, otherwise loads from archive; \n",
    "# - best F1 on dev/validation in the paper is 80,96 trained on a GPU, CPU will be a bit lower ~77.x I think\n",
    "sp = spar_predictor.SparPredictor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0206ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'obj': ['An example sentence', 'ACC terminology', 'the British Standards']}\n",
      "Parsing took 0.23555612564086914\n"
     ]
    }
   ],
   "source": [
    "example = \"An example sentence to show how ACC terminology will be extracted from the British Standards.\"\n",
    "start_time = time.time()\n",
    "# prepare instance and run model on single instance\n",
    "docid = ''                  # ToDo - add doc_id during pre_processing?\n",
    "token_list = sp.predictor._dataset_reader.tokenizer.tokenize(example)\n",
    "instance = sp.predictor._dataset_reader.text_to_instance(docid,\n",
    "                                                      example,\n",
    "                                                      token_list,\n",
    "                                                      sp.predictor._dataset_reader._token_indexer)\n",
    "result = sp.predictor.predict_instance(instance)\n",
    "printable_result = sp.parse_output(result, ['obj'])\n",
    "print(printable_result)\n",
    "print(\"Parsing took {}\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5f7c27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These should automatically run on your Nvidia GPU if available\n",
    "class SparInstance:\n",
    "    def __init__(self):\n",
    "        self.sp = spar_predictor.SparPredictor()\n",
    "    \n",
    "    def call(self, input_str:str=''):\n",
    "        if input_str:\n",
    "            # prepare instance and run model on single instance\n",
    "            docid = ''  # ToDo - add doc_id during pre_processing?\n",
    "            token_list = self.sp.predictor._dataset_reader.tokenizer.tokenize(input_str)\n",
    "\n",
    "            # truncating the input to SPaR.txt to maximum 512 tokens\n",
    "            token_length = len(token_list)\n",
    "            if token_length > 512:\n",
    "                token_list = token_list[:511] + [token_list[-1]]\n",
    "                token_length = 512\n",
    "\n",
    "            instance = self.sp.predictor._dataset_reader.text_to_instance(docid, input_str, token_list,\n",
    "                                                              self.sp.predictor._dataset_reader._token_indexer)\n",
    "            result = self.sp.predictor.predict_instance(instance)\n",
    "            printable_result = self.sp.parse_output(result, ['obj'])\n",
    "            return {\n",
    "                    \"prediction\": printable_result,\n",
    "                    \"num_input_tokens\": token_length,\n",
    "            }\n",
    "            \n",
    "        # If the input is None, or too long, return an empty list of objects\n",
    "        return {\n",
    "                \"prediction\": {'obj': []},\n",
    "                \"num_input_tokens\": 0\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb733c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TermExtractor:\n",
    "    \n",
    "    def __init__(self, split_length=300, max_num_cpu_threads=4):\n",
    "        \"\"\"\n",
    "        Initialise SPaR.txt predictors `max_num_cpu_threads` \n",
    "        \"\"\"\n",
    "        self.split_length = split_length   # in number of tokens\n",
    "        self.max_num_cpu_threads = max_num_cpu_threads\n",
    "        self.PREDICTORS = []\n",
    "        for i in range(max_num_cpu_threads + 1):\n",
    "            self.PREDICTORS.append(SparInstance())\n",
    "    \n",
    "    \n",
    "    def process_sentence(self, sentence: str = ''):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        predictor_to_use = int(current_thread().name.rsplit('_', 1)[1])\n",
    "        spartxt = self.PREDICTORS[predictor_to_use]\n",
    "\n",
    "        # SPaR doesn't handle ALL uppercase sentences well, which the OCR system sometimes outputs    \n",
    "        sentence = sentence.lower() if sentence.isupper() else sentence\n",
    "        prediction_dict =  spartxt.call(sentence)\n",
    "        if not prediction_dict:\n",
    "            return []\n",
    "\n",
    "        pred_labels = prediction_dict[\"prediction\"]\n",
    "        return pred_labels['obj']\n",
    "        \n",
    "\n",
    "    def split_into_sentences_and_run_spar(self, input_document):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        print(f\"Working on: {input_document.source_fp}\")\n",
    "        content_as_list_of_dicts = input_document.to_list_of_dicts()\n",
    "        total_number_of_sentences_found = 0\n",
    "        content_idx = 0\n",
    "        for content_dict in tqdm(content_as_list_of_dicts):\n",
    "\n",
    "            text = ' '.join([x for x in content_dict[\"content\"].split(' ') if x != ''])\n",
    "            # some really long paragraphs in the EU regulations are summations that should be split at ';'\n",
    "            if len(text) > 3000:\n",
    "                text = text.replace(\";\", \".\\n\")\n",
    "\n",
    "            # We'll split into sentences even if this has been done before, it doesn't take long\n",
    "            sentences = []\n",
    "            for part in text.split('\\n'):\n",
    "                # split into sentences using PunktSentTokenizer (TextBlob implements NLTK's version under the hood) \n",
    "                sentences += [str(s) for s in TextBlob(part).sentences if len(str(s)) > 10]\n",
    "\n",
    "            content_dict[\"meta\"][\"sentences\"] = '###'.join(sentences)\n",
    "                \n",
    "            total_number_of_sentences_found += len(sentences)\n",
    "\n",
    "            # process sentences in the content and add SPaR.txt object tags to the content dict.        \n",
    "            if not content_dict[\"meta\"][\"SPaR_labels\"]:\n",
    "                with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_num_cpu_threads) as executor:\n",
    "                    futures = [executor.submit(self.process_sentence, sentences[idx]) for idx in range(len(sentences))]\n",
    "\n",
    "                content_spar_objects = [f.result() for f in futures]\n",
    "                content_dict[\"meta\"][\"SPaR_labels\"] = ', '.join([tag for tags in content_spar_objects for tag in tags])\n",
    "                \n",
    "            # immediately update the list of content_dicts and every X iterations we save the file \n",
    "            content_as_list_of_dicts[content_idx] = content_dict\n",
    "            if content_idx // 5 == 0:\n",
    "                converted_document.replace_contents(content_as_list_of_dicts)\n",
    "                converted_document.write_document()\n",
    "            \n",
    "            content_idx += 1\n",
    "\n",
    "        print(f\"Number of sentences found: {total_number_of_sentences_found}\")\n",
    "        converted_document.replace_contents(content_as_list_of_dicts)\n",
    "        converted_document.write_document()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7dc2f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "te = TermExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3d086b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on: data/input/The Merged Approved Documents.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1274/1274 [00:02<00:00, 497.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences found: 17745\n",
      "Working on: data/input/EUR-Lex - 31993L0042 - EN.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 826/826 [00:00<00:00, 2741.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences found: 1679\n",
      "Working on: data/input/CELEX 32017R0746 EN TXT.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4344/4344 [00:02<00:00, 1896.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences found: 3405\n",
      "Working on: data/input/CELEX 32017R0745 EN TXT.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4799/4799 [00:02<00:00, 2225.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences found: 3838\n",
      "Working on: data/input/EUR-Lex - 31998L0079 - EN.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 623/623 [00:00<00:00, 1215.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences found: 1255\n",
      "Working on: data/input/EUR-Lex - 31990L0385 - EN.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 511/511 [00:00<00:00, 2863.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences found: 771\n"
     ]
    }
   ],
   "source": [
    "# Run SPaR.txt on all documents and write to file\n",
    "for converted_document in foreground_corpus + background_corpus:\n",
    "    # re-load the document from file, to make sure we don't overwrite existing SPaR.txt labels\n",
    "    converted_document = converted_document.load_document(converted_document.output_fp)\n",
    "    te.split_into_sentences_and_run_spar(converted_document)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4835185f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Number of sentences (longer than 10 characters) found in: <ul>\n",
    "    <li>Merged Approved documents: 17745</li>\n",
    "    <li>Background corpus (1679+3405+3838+1255+771): 10948</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b15878",
   "metadata": {},
   "source": [
    "### 3 Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b511762d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "First, load all terms from the processed files.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4fb5373f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of foreground terms: 114333\n",
      "Total number of UNIQUE foreground terms: 42657\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('the', 2130),\n",
       " ('a', 930),\n",
       " ('the building', 864),\n",
       " ('buildings', 839),\n",
       " ('guidance', 522),\n",
       " ('a building', 455),\n",
       " ('the Building Regulations', 434),\n",
       " ('document', 303),\n",
       " ('people', 301),\n",
       " ('the requirements', 297)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foreground_terms_lists = [c.NER_labels for d in foreground_corpus for c in d.load_document(d.output_fp).all_contents]\n",
    "foreground_terms = [t for t_list in foreground_terms_lists for t in t_list if t]\n",
    "foreground_terms_c = Counter(foreground_terms)\n",
    "print(f\"Total number of foreground terms: {len(foreground_terms)}\")\n",
    "print(f\"Total number of UNIQUE foreground terms: {len(foreground_terms_c)}\")\n",
    "foreground_terms_c.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8006f6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of background terms: 73124\n",
      "Total number of UNIQUE background terms: 10245\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('the', 2651),\n",
       " ('devices', 1641),\n",
       " ('the manufacturer', 1243),\n",
       " ('the device', 1187),\n",
       " ('Regulation', 624),\n",
       " ('the notified body', 595),\n",
       " ('information', 550),\n",
       " ('Member States', 489),\n",
       " ('a', 451),\n",
       " ('the market', 447)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "background_terms_lists = [c.NER_labels for d in background_corpus for c in d.load_document(d.output_fp).all_contents]\n",
    "background_terms = [t for t_list in background_terms_lists for t in t_list if t]\n",
    "background_terms_c = Counter(background_terms)\n",
    "print(f\"Total number of background terms: {len(background_terms)}\")\n",
    "print(f\"Total number of UNIQUE background terms: {len(background_terms_c)}\")\n",
    "background_terms_c.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e79727a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique spans identified by SPaR.txt:51296\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of unique spans identified by SPaR.txt:{len(foreground_terms_c+background_terms_c)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464451e8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Second, clean the terms with the regular expressions we've defined in utils.py\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2503708d",
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_filter = cleaning_utils.RegexFilter()\n",
    "def run_filters(input_counter):\n",
    "    cleaned_counter = Counter()\n",
    "    for k, v in input_counter.items():\n",
    "        # terms should occur twice at least\n",
    "        if v < 2:\n",
    "            continue\n",
    "\n",
    "        # todo; clean up these util functions and how to call them|\n",
    "        _, k = regex_filter.run_filter(k)\n",
    "        if k:\n",
    "            cleaned_k = cleaning_utils.custom_cleaning_rules(k)\n",
    "            if cleaned_k:\n",
    "                cleaned_counter[cleaned_k[0]] = v\n",
    "    return cleaned_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2e42c7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11187\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('guidance', 522),\n",
       " ('a building', 455),\n",
       " ('document', 303),\n",
       " ('the requirements', 297),\n",
       " ('requirements', 266),\n",
       " ('work', 242),\n",
       " ('the guidance', 240),\n",
       " ('the work', 232),\n",
       " ('Schedule 1', 226),\n",
       " ('the dwelling', 215)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_foreground_terms_c = run_filters(foreground_terms_c)\n",
    "print(len(cleaned_foreground_terms_c))\n",
    "cleaned_foreground_terms_c.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "899b1cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6189\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('devices', 1641),\n",
       " ('the device', 1187),\n",
       " ('the notified body', 595),\n",
       " ('Member States', 489),\n",
       " ('accordance', 429),\n",
       " ('a device', 405),\n",
       " ('conformity', 357),\n",
       " ('the requirements', 350),\n",
       " ('The notified body', 328),\n",
       " ('notified bodies', 324)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_background_terms_c = run_filters(background_terms_c)\n",
    "print(len(cleaned_background_terms_c))\n",
    "cleaned_background_terms_c.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406d6ca8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Third, we cluster the embeddings for both foreground and background terms with KMeans.\n",
    "</div>\n",
    "\n",
    "\n",
    "* Note: embeddings will be IDF weighted (IDF weights over both foreground and background corpora)\n",
    "  * Could add more sentences to the computation of IDF weights, e.g., definitions from vocabularies/WikiData."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0b177bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which tokenizer to use for IDF computation and Embedding;\n",
    "bert_model_name = 'bert-base-cased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "10278f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_terms_c = cleaned_foreground_terms_c + cleaned_background_terms_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8f403d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16441"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_terms_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a1000a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('devices', 1665),\n",
       " ('the device', 1195),\n",
       " ('the requirements', 647),\n",
       " ('accordance', 631),\n",
       " ('the notified body', 595),\n",
       " ('guidance', 539),\n",
       " ('Member States', 491),\n",
       " ('requirements', 457),\n",
       " ('a building', 455),\n",
       " ('a device', 412)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_terms_c.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ac5654c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of MWEs: 10833\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['minimum content',\n",
       " 'kWh /',\n",
       " 'regulation 24 ii',\n",
       " 'hot water storage vessel',\n",
       " 'a continuous training education programme',\n",
       " 'light or ionization',\n",
       " 'certification decisions',\n",
       " 'the reference values',\n",
       " 'whole dwelling supply ventilation rate',\n",
       " 'of specification a )',\n",
       " 'sloping floor tier',\n",
       " 'IT systems',\n",
       " 'the Building Regulations requirement',\n",
       " 'the layout walls',\n",
       " 'Transfer of Functions )',\n",
       " 'a solid fuel appliance',\n",
       " 's heating',\n",
       " 'SI 2010 / 2214 )',\n",
       " 'waste disposal policies stock - management',\n",
       " 'cavity insulation',\n",
       " 'cooler air',\n",
       " 'the TER / BER calculation',\n",
       " 'covered ways',\n",
       " 'the notified bodies',\n",
       " 'Regulation quality',\n",
       " 'the principle of',\n",
       " 'the change of designation',\n",
       " 'a habitable room',\n",
       " 'step / stagger',\n",
       " 'animal experimentation',\n",
       " 'door width',\n",
       " 'the vertical position',\n",
       " 'medical field',\n",
       " 'functional safety',\n",
       " 'Such measures',\n",
       " 'Published Document Recommendations',\n",
       " 'linings ) Requirement B3 : fire spread',\n",
       " 'Article 58',\n",
       " 'the floor basement',\n",
       " 'Free area Free area',\n",
       " 'the column members',\n",
       " 'a level access shower',\n",
       " 'the greywater savings',\n",
       " 'certification organisations',\n",
       " 'a loadbearing wall',\n",
       " 'movement joints',\n",
       " 'www hsebooks com',\n",
       " 'Articles 95',\n",
       " 'the mutual compatibility',\n",
       " 'The equivalent area',\n",
       " 'self - contained units',\n",
       " 'Council Directive 76 / 764 / EEC of 27 July 1976',\n",
       " 'small premises',\n",
       " 'Diagram 11 5',\n",
       " 'Background ventilators',\n",
       " 're - evaluation criteria',\n",
       " 'amendments 1',\n",
       " 'analytical specificity trueness',\n",
       " 'the Directive 2010 / 63 / EU',\n",
       " 'regulation 2',\n",
       " 'a ventilation opening',\n",
       " 'tree roots',\n",
       " 'loadbearing capacity',\n",
       " 'Standard Assessment Procedure',\n",
       " 'Glass and Glazing Federation GGF )',\n",
       " 'side - hinged',\n",
       " 'portal frames',\n",
       " 'above performance',\n",
       " 'Cubic capacity',\n",
       " 'single fault condition',\n",
       " 'the overall test performance i1',\n",
       " 'May 2024',\n",
       " 'para 3 29',\n",
       " 'the declaration of conformity',\n",
       " 'Article 104',\n",
       " 'aspects volume x dimension',\n",
       " 'Private entrances',\n",
       " 'a flanking laboratory',\n",
       " 'core UDI database data elements',\n",
       " 'crush hall',\n",
       " 'class A1 rated',\n",
       " 'Part E',\n",
       " 'Each device',\n",
       " 'paragraph 13',\n",
       " 'clinical performance',\n",
       " 'a misleading context',\n",
       " 'low maintenance',\n",
       " 'remedial works',\n",
       " 'side panels',\n",
       " 'temporary buildings',\n",
       " 'state of the art',\n",
       " 'objective evidence',\n",
       " 'the travel distance',\n",
       " 'graphic information',\n",
       " 'floor coverings',\n",
       " 'Preventive health protection measures',\n",
       " 'independence principles',\n",
       " 'ent M1',\n",
       " 'nursing homes',\n",
       " 'the flood performance']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some insight in number of MWEs\n",
    "mwes = []\n",
    "for t in all_terms_c.keys():\n",
    "    words = t.split(' ')\n",
    "    if words[0] in ['the', 'a', 'The', 'A', 'an', 'An', 'any', 'Any', 'this', 'This']:\n",
    "        words = words[1:]\n",
    "    if len(words) > 1:\n",
    "        mwes.append(t)\n",
    "\n",
    "print(f\"Number of MWEs: {len(mwes)}\")\n",
    "random.sample(mwes, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d8d0d4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute IDF weights\n",
    "sentence_lists = [c.sentences for d in foreground_corpus + background_corpus for c in d.load_document(d.output_fp).all_contents]\n",
    "all_sentences = [s for sent_list in sentence_lists for s in sent_list if s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7bf4a15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28693\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['The Merged Approved Documents',\n",
       " 'How to use the Merged Approved Documents This document combines the approved documents into a single PDF.',\n",
       " 'Each approved document is self-contained and has its own introduction.',\n",
       " 'Each introduction relates only to the corresponding approved document.',\n",
       " \"Each introduction also contains information on when the document's guidance came into effect (or will come into effect).\",\n",
       " 'It is important to check that the version of each approved document you are using remains current and is the correct version for your project.',\n",
       " 'Please refer to the Ministry of Housing, Communities and Local Government website to check, and confirm with your building control body if in doubt.',\n",
       " 'Key features The Merged Approved Documents enable the user to: undertake a word search across all of the approved documents cut and paste text and diagrams into other documents add notes to a saved copy use an index to access individual sections of the guidance Correction to Approved Document K The heading in section 1.18 of the online version of Approved Document K have been corrected to match the print version.',\n",
       " \"Forthcoming changes Please check the Ministry of Housing, Communities and Local Government's website to ensure that each approved document you are using is current for your project.\",\n",
       " 'This is particularly important in relation to Approved Document B as this has been subject to frequent update.']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(all_sentences))\n",
    "all_sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c09c984e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing IDF weights.\n",
      "Printing some IDF values, should be subword units!\n",
      "['cluster']\n",
      "['##of']\n",
      "['deposits']\n",
      "['ultimate']\n",
      "['horizontal']\n",
      "['proximity']\n",
      "['estates']\n",
      "['exploration']\n",
      "['##most']\n",
      "['rendered']\n"
     ]
    }
   ],
   "source": [
    "IDF_c = IDF_computation.IdfComputer(\"data/IDF_weights.json\", bert_model_name=bert_model_name)\n",
    "IDF_path = IDF_c.compute_or_load_IDF_weights(all_sentences, overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "411b8d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed each of the terms identified by SPaR.txt, applies IDF weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5b1959bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique terms: 16441\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['the Merged Approved Documents',\n",
       " 'document',\n",
       " 'documents',\n",
       " 'Each introduction',\n",
       " 'approved',\n",
       " 'information',\n",
       " 'guidance',\n",
       " 'the version',\n",
       " 'project',\n",
       " 'the Ministry']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_terms = [k for k in all_terms_c.keys()] # counter keys, so already unique\n",
    "print(f\"Number of unique terms: {len(all_terms)}\")\n",
    "all_terms[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "26f20d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "bert_model = BertModel.from_pretrained(bert_model_name, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a669a4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = embedding.Embedder(tokenizer, bert_model, \n",
    "                              IDF_dict=json.load(open(IDF_path)), \n",
    "                              embedding_fp=\"output/\",\n",
    "                              layers_to_use = [12],         # we'll use the output of the last layer\n",
    "                              layer_combination = \"avg\",    # how to combine layers if multiple are used\n",
    "                              idf_threshold = 1.5,          # minimum IDF value for a token to contribute\n",
    "                              idf_weight_factor = 1.0,      # modify how strong the influence of IDF weighting is\n",
    "                              not_found_idf_value = 0.5)    # IDF value for tokens that weren't seen during IDF computation (doesn't apply here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce59ea93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embeddings for 16441 spans, in groups of: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                           | 11/17 [05:04<03:00, 30.01s/it]"
     ]
    }
   ],
   "source": [
    "# Compute the embeddings, this is split into subsets so we don't overload your memory (adjust these values if needed)\n",
    "max_num_cpu_threads = 4\n",
    "subset_size = 1000\n",
    "\n",
    "# Checks which of the embeddings for the clustering cluster_data already exist, so they can be re-used\n",
    "term_subsets = cleaning_utils.split_list(all_terms, subset_size)\n",
    "embedding_files = glob.glob(embedder.embedding_fp + 'embeddings*.pkl')\n",
    "span_and_embedding_pairs = []\n",
    "if len(embedding_files) == len(term_subsets):\n",
    "    for e in embedding_files:\n",
    "        span_and_embedding_pairs += pickle.load(open(e, 'rb'))\n",
    "else:\n",
    "    print(f\"Preparing embeddings for {len(all_terms)} spans, in groups of: {subset_size}\")\n",
    "    subset_idx = 0            # iterator index outside of tqdm \n",
    "    for subset in tqdm(term_subsets):\n",
    "        subset_embeddings = []\n",
    "        subset_file_name = embedder.embedding_fp + \"embeddings_part_\" + '{}.pkl'.format(subset_idx)\n",
    "        subset_idx += 1\n",
    "        if os.path.exists(subset_file_name):\n",
    "            # already computed previously\n",
    "            continue\n",
    "        \n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=max_num_cpu_threads) as executor:\n",
    "            futures = [executor.submit(embedder.embed_a_span, subset[idx]) for idx in range(len(subset))]\n",
    "\n",
    "        subset_embeddings += [f.result() for f in futures if f.result()]\n",
    "\n",
    "        with open(subset_file_name, 'wb') as f:\n",
    "            pickle.dump(subset_embeddings, f)\n",
    "\n",
    "    # Once all embeddings are created; combine them in span_and_embedding_pairs\n",
    "    embedding_files = glob.glob(embedder.embedding_fp + \"embeddings_part_\" + '*.pkl')\n",
    "    for e in embedding_files:\n",
    "        span_and_embedding_pairs += pickle.load(open(e, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e8b64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a single file with all embeddings, in the meantime standardising the embeddings to improve the representation\n",
    "print(f\"Normalising and combining computed/existing {len(embedding_files)} embeddings from files into single file\")\n",
    "unique_spans, unique_embeddings = zip(*span_and_embedding_pairs)\n",
    "with open(embedder.embedding_fp + \"unique_spans.pkl\", 'wb') as f:\n",
    "    pickle.dump(unique_spans, f)\n",
    "\n",
    "with open(embedder.embedding_fp + \"unique_embeddings.pkl\", 'wb') as f:\n",
    "    # we average over the token embeddings in a term\n",
    "    unique_clustering_data = np.stack([np.mean(e, axis=0) if len(e.shape) > 1 else e for e in unique_embeddings])\n",
    "\n",
    "    # standardise the unique clustering data, as suggested by https://github.com/wtimkey/rogue-dimensions\n",
    "    embedder.emb_mean = unique_clustering_data.mean(axis=0)\n",
    "    embedder.emb_std = unique_clustering_data.std(axis=0)\n",
    "    pickle.dump(embedder.emb_mean, open(embedder.embedding_fp + \"standardisation_mean.pkl\", 'wb'))\n",
    "    pickle.dump(embedder.emb_std, open(embedder.embedding_fp + \"standardisation_std.pkl\", 'wb'))\n",
    "\n",
    "    standardised_clustering_data = (unique_clustering_data - embedder.emb_mean) / embedder.emb_std\n",
    "\n",
    "    pickle.dump(standardised_clustering_data, f)\n",
    "    \n",
    "# Store the standardised embeddings for reuse; could honeslty remove all the other embedding files but will keep them just in case\n",
    "pickle.dump(standardised_clustering_data, open(embedder.embedding_fp + \"standardised_embeddings.pkl\", 'wb'))\n",
    "spans_and_standardised_embeddings_dict = dict(zip(unique_spans, standardised_clustering_data))\n",
    "# pickle.dump(spans_and_standardised_embeddings_dict, open(embedder.embedding_fp + \"spans_and_standardised_embeddings_dict.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30daa510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to compute clusters on either CPU or GPU\n",
    "def compute_clusters_sklearn(standardised_clustering_data, cluster_model_fp, num_clusters=10):\n",
    "    \"\"\"\n",
    "    Note that this clustering function relies on the CPU. It won't be able to compute clusters for large \n",
    "    amounts of inputs, e.g., 100.000 spans. When using a large number of clusters (e.g. 5000) it is also\n",
    "    a lot slower than a GPU implementation for. Or it may simply not converge! \n",
    "    For large inputs/num_clusters you'll need to use compute_clusters_kmcuda, and have access to a GPU.\n",
    "    \"\"\"\n",
    "    print(f\"Computing {num_clusters} clusters from scratch, using sklearn on the CPU\")\n",
    "    start_time = time.time()\n",
    "    sklearn_kmeans = KMeans(n_clusters=num_clusters, init='k-means++', max_iter=3, n_init=1, random_state=14,\n",
    "                            tol=0.0001, verbose=0)\n",
    "    assignments = sklearn_kmeans.fit_predict(standardised_clustering_data)\n",
    "    centroids = sklearn_kmeans.cluster_centers_\n",
    "    print(\"Clustering took {}\".format(time.time() - start_time))\n",
    "    with open(cluster_model_fp, 'wb') as f:\n",
    "        pickle.dump((centroids, assignments), f)\n",
    "        \n",
    "# compute clusters on CPU for now\n",
    "def compute_clusters_kmcuda(standardised_clustering_data, cluster_model_fp, num_clusters=10000):\n",
    "    \"\"\"\n",
    "    Won't implement cosine KMeans here, as I want to predict with sklearn in this notebook. \n",
    "    Also not sure if the results are really that much better.\n",
    "    \"\"\"\n",
    "    centroids, assignments = kmeans_cuda(standardised_clustering_data, num_clusters, init=\"k-means++\",\n",
    "                                                 verbosity=1, seed=14) # , device=0)\n",
    "    with open(cluster_model_fp, 'wb') as f:\n",
    "        pickle.dump((centroids, assignments), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39d48c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not kmeans_cuda:\n",
    "    clustering_type = \"sklearn\"\n",
    "    # Computing clusters on the CPU\n",
    "    for num_clusters in range(2000,8001, 500):\n",
    "        cluster_file = f\"output/sklearn_{num_clusters}_clusters.pkl\"\n",
    "        if not os.path.exists(cluster_file):\n",
    "            compute_clusters_sklearn(standardised_clustering_data, cluster_file, num_clusters)\n",
    "        else:\n",
    "            print(f\"sklearn cluster file exists for {num_clusters} clusters\")\n",
    "else:    \n",
    "    \"\"\"\n",
    "    Currently not implemented, would have to provide instructions for kmcuda installation as well probably\n",
    "    \"\"\"\n",
    "    clustering_type = \"kmcuda\" \n",
    "    for num_clusters in range(4000,10001, 500):\n",
    "        cluster_file = f\"output/kmcuda_{num_clusters}_clusters.pkl\"\n",
    "        if not os.path.exists(cluster_file):\n",
    "            compute_clusters_kmcuda(standardised_clustering_data, cluster_file, num_clusters) \n",
    "        else:\n",
    "            print(f\"kmcuda cluster file exists for {num_clusters} clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc97b112",
   "metadata": {},
   "source": [
    "* Select the 'best' cluster model using Elbow and Silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6810b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_type = \"sklearn\"\n",
    "clustering_files = glob.glob(f'output/{clustering_type}_*.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d87f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes a bit of time; 6-10 minutes per cluster size for 2000-6000 clusters \n",
    "# but resulting scores are saved in a csv and reused\n",
    "clustering_data_fp = \"output/\"\n",
    "es = cluster_utils.ElbowAndSilhouette(clustering_data_fp)\n",
    "es.compute_scores_for_models(clustering_type, clustering_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93f80fc",
   "metadata": {},
   "source": [
    "* Based on the number of clusters we'd like to use, we create a lookup-dictionary for the embedding and assigned cluster of each span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db65503",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_num_clusters = 5000\n",
    "cluster_model_to_use = f'output/{clustering_type}_{chosen_num_clusters}_clusters.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733bd1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids, assignments = pickle.load(open(cluster_model_to_use, 'rb'))\n",
    "\n",
    "unique_background_terms = [k for k in background_terms_c.keys()]\n",
    "cluster_dict_creator = cluster_utils.ClusterDict(unique_background_terms, \n",
    "                                                 unique_spans, \n",
    "                                                 standardised_clustering_data,  # important to use standardised  embeddings\n",
    "                                                 centroids, \n",
    "                                                 assignments,\n",
    "                                                 embedding_fp=\"output/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5692ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_cluster_dict, clusters_to_filter = cluster_dict_creator.prep_cluster_dict(chosen_num_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bbc5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print some insight in the clusters\n",
    "max_terms_to_show = 5\n",
    "for k in random.sample(phrase_cluster_dict.keys(), 10):\n",
    "    some_terms = [span for score, span in phrase_cluster_dict[k]]\n",
    "    if k in clusters_to_filter:\n",
    "        print(f\"Filtered: {some_terms[:max_terms_to_show]}\")\n",
    "    else:\n",
    "        print(f\"AEC domain: {some_terms[:max_terms_to_show]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5aa123c",
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_terms = [span for k, v in phrase_cluster_dict.items() for score, span in v if k in clusters_to_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824968e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_terms = [t for t in removed_terms if t in foreground_terms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97c6c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_terms = [span for k, v in phrase_cluster_dict.items() for score, span in v  if k not in clusters_to_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89aff4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(filtered_terms))\n",
    "print(\"Terms that were filtered:\")\n",
    "random.sample(filtered_terms,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24378f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(domain_terms))\n",
    "print(\"Terms that were kept:\")\n",
    "random.sample(domain_terms,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a2ecd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check the largest cluster(s)\n",
    "largest_cluster_size = np.max([len(v) for v in phrase_cluster_dict.values()])\n",
    "[v for v in phrase_cluster_dict.values() if len(v) == largest_cluster_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d79937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the smallest clusters\n",
    "[v for v in phrase_cluster_dict.values() if len(v) == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d0194b",
   "metadata": {},
   "source": [
    "### 4 Suggesting similar terms for Uniclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a16c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusterAssignment:\n",
    "    def __init__(self, pkl_model, embedder, unique_spans, standardised_clustering_data, phrase_cluster_dict, clusters_to_filter):\n",
    "        \"\"\"\n",
    "        It's important to use the standardised clustering data here!\n",
    "        \"\"\"\n",
    "        self.centroids, self.assignments = pickle.load(open(pkl_model, 'rb'))\n",
    "        self.num_clusters = int(max(self.assignments) + 1)\n",
    "        \n",
    "        # set up sklearn for prediction\n",
    "        cpu_centroids = np.nan_to_num(self.centroids, copy=True, nan=0.0, posinf=None, neginf=None)\n",
    "        self.cpu_clusterer = KMeans(self.num_clusters, init=cpu_centroids, n_init=1, max_iter=1, tol=0)\n",
    "        print(\"[Clustering] Initialising sklearn kMeans with 1 iteration on KMcuda outputs. This takes some time!\")\n",
    "        self.cpu_clusterer.fit(standardised_clustering_data)\n",
    "        \n",
    "        self.embedder = embedder\n",
    "        self.unique_span_dict = dict(zip(unique_spans, standardised_clustering_data))\n",
    "        self.cluster_dict = phrase_cluster_dict\n",
    "        self.clusters_to_filter = clusters_to_filter\n",
    "    \n",
    "\n",
    "    def sklearn_assign(self, to_be_clustered: List[cluster_utils.ToBeClustered]) -> List[cluster_utils.ToBeClustered]:\n",
    "        \"\"\"\n",
    "        Using SciKit Learn's CPU implementation of KMeans to assign cluster IDs to a span or list of spans.\n",
    "\n",
    "        :param to_be_predicted: A `str` or `list` of strings for which a cluster ID will be computed.\n",
    "        :return :   List of cluster IDs.\n",
    "        \"\"\"\n",
    "        # Note: we try to avoid iteratively assigning clusters for each item in a list - don't think we parallelize more\n",
    "        embeddings = [s.embedding for s in to_be_clustered]\n",
    "        arrays = [np.ones([1, 768]).astype(np.float32) for _ in to_be_clustered]\n",
    "\n",
    "        for i, e in enumerate(embeddings):\n",
    "            arrays[i][0] = np.stack(e.squeeze())  # .astype(np.float16)\n",
    "\n",
    "        assignments = []\n",
    "        for arr in arrays:\n",
    "            assignments.append(self.cpu_clusterer.predict(arr))\n",
    "\n",
    "        for idx, assignment in enumerate(assignments):\n",
    "            to_be_clustered[idx].cluster_id = str(int(assignment))\n",
    "            print(f\"assigned ID: {str(int(assignment))}\")\n",
    "            to_be_clustered[idx].distance_to_centroid = np.sum(np.absolute(embeddings[idx]-self.centroids[assignment]))\n",
    "            to_be_clustered[idx].all_neighbours = self.cluster_dict[str(int(assignment))]\n",
    "\n",
    "        return to_be_clustered\n",
    "    \n",
    "    def get_top_neighbours(self, text_inputs : Union[List[str], str], cosine_sim_threshold: float = 0.7, top_k: int = 3):\n",
    "        if type(text_inputs) == str:\n",
    "            tbc = cluster_utils.ToBeClustered(text_inputs, embedder)\n",
    "            [tbc] = self.sklearn_assign([tbc])\n",
    "            if tbc.cluster_id in self.clusters_to_filter:\n",
    "                print(f\"Potentially non-AEC domain: {tbc.text}\")\n",
    "            return tbc.get_top_k_neighbours(self.unique_span_dict, cosine_sim_threshold, top_k)\n",
    "        else:\n",
    "            tbcs = [cluster_utils.ToBeClustered(t, embedder) for t in text_inputs]\n",
    "            tbcs = self.sklearn_assign(tbcs)\n",
    "            neighbours = []\n",
    "            for tbc in tbcs:\n",
    "                print(tbc.text)\n",
    "                if tbc.cluster_id in self.clusters_to_filter:\n",
    "                    print(f\"Potentially non-AEC domain: {tbc.text}\")\n",
    "                neighbours.append(tbc.get_top_k_neighbours(self.unique_span_dict, cosine_sim_threshold, top_k))\n",
    "            return neighbours\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95dfe00",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_assigner = ClusterAssignment(cluster_model_to_use, embedder, unique_spans, standardised_clustering_data, phrase_cluster_dict, clusters_to_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65448be",
   "metadata": {},
   "source": [
    "* some examples of assigning clusters and identifying neighbours for spans:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30eab7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example str input\n",
    "cluster_assigner.get_top_neighbours(\"party wall\", cosine_sim_threshold=.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00101129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example list of str input\n",
    "cluster_assigner.get_top_neighbours([\"hollow structural member\", \"control equipment\"], cosine_sim_threshold=.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dd9968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example str input\n",
    "cluster_assigner.get_top_neighbours(\"Activities\", cosine_sim_threshold=.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc6b618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read uniclass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2993b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_ttl_lines(text):\n",
    "    groups = []\n",
    "    current_group = []\n",
    "    for idx, line in enumerate(text.split(\"\\n\")):\n",
    "        if line == '':\n",
    "            if current_group:\n",
    "                groups.append(current_group)\n",
    "            current_group = []\n",
    "        else:\n",
    "            current_group.append(line)\n",
    "            if idx+1 == len(text.split(\"\\n\")):\n",
    "                groups.append(current_group)\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9ccccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_uids_and_labels_with_definition(groups):\n",
    "    uid_dict = {}\n",
    "    for g in groups:\n",
    "        if any([line.startswith('  skos:prefLabel') for line in g]):\n",
    "            # only use group if a prefLabel exists\n",
    "            pref_label = ''\n",
    "            alt_labels = []\n",
    "            definition = ''\n",
    "            for line in g:\n",
    "                if line.startswith('  skos:prefLabel'):\n",
    "                    pref_label = line.split('\"')[1]\n",
    "                elif line.startswith('  skos:altLabel'):\n",
    "                    labels = line.split('\"')[1::2]\n",
    "                    alt_labels += labels\n",
    "                elif line.startswith('  skos:definition'):\n",
    "                    definition = line.split('\"')[1]\n",
    "\n",
    "            if pref_label:\n",
    "                uid = g[0].split()[0].split(\":\")[1]\n",
    "                uid_dict[uid] = {'pref_label': pref_label, \n",
    "                                 'alt_labels': alt_labels,\n",
    "                                 'definition': definition\n",
    "                                }\n",
    "    return uid_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49721c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_nodes(vocab_name):\n",
    "    processed_file = f\"{vocab_name}.json\"\n",
    "    \n",
    "    # check if file as processed before \n",
    "    NODES_LOADED = False\n",
    "    if os.path.exists(processed_file):\n",
    "        with open(processed_file) as f:\n",
    "            graph_dict = json.load(f)\n",
    "            \n",
    "        NODES_LOADED = True\n",
    "        print(f\"Loaded nodes and neighbours for: {vocab_name}\") \n",
    "    else: \n",
    "        print(f\"Will have to grab nodes for: {vocab_name}\")\n",
    "    \n",
    "        # compute the neighbours for each node\n",
    "        graph_dict = {}\n",
    "        \n",
    "        print(f\"Working on file: {vocab_name}\")\n",
    "        with open(vocab_name, 'r') as f:\n",
    "            text =  f.read()\n",
    "            \n",
    "        groups = group_ttl_lines(text)\n",
    "        print(\"Collecting nodes with definitions from dict\")\n",
    "        graph_dict = grab_uids_and_labels_with_definition(groups)\n",
    "        \n",
    "        # save the dictionary somewhere for reloading\n",
    "        with open(processed_file, 'w') as f:\n",
    "            json.dump(graph_dict, f)\n",
    "    return graph_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7be3062",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniclass_dict = grab_nodes(\"data/input/uniclass_2015.ttl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463df3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(uniclass_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deab3c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in uniclass_dict.items():\n",
    "    uniclass_label = v['pref_label']\n",
    "    if uniclass_label:\n",
    "        print(uniclass_label)\n",
    "        cluster_assigner.get_top_neighbours(uniclass_label, cosine_sim_threshold=.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4895421",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder.embed_a_span(\"Activities\") == embedder.combine_token_embeddings(embedder.embed_text(\"Activities\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84a0ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = tokenizer.tokenize(\"[CLS] \" + \"Activities\" + \" [SEP]\")\n",
    "indices = tokenizer.convert_tokens_to_ids(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2879f62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = bert_model(torch.tensor([indices]), torch.tensor([[1] * len(indices)]))[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8a9578",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeddings  = torch.stack(hidden_states, dim=0).squeeze(dim=1).permute(1, 0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdd650f",
   "metadata": {},
   "outputs": [],
   "source": [
    "IDF_weights = embedder.get_IDF_weights_for_indices(tokenized_text, indices)\n",
    "IDF_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e6f8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder.embed_text(\"Activities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d60aa44",
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_t_embs = []\n",
    "for token, IDF_w in zip(token_embeddings, IDF_weights):\n",
    "    # todo -- consider running more experiments with different BERT layers and combinations\n",
    "    # Combine the vectors from selected bert layers --> currently default to last layer only [12]\n",
    "    combined_vec = torch.mean(token.index_select(0, torch.tensor([12])), dim=0)\n",
    "    # Weight the vector by IDF value\n",
    "    if IDF_w > 1.5   :  # avoid multiplying by 0\n",
    "        print(\"happening\")\n",
    "        if IDF_w > 0:\n",
    "            weighted_t_embs.append(combined_vec * (1.5 * IDF_w))\n",
    "        else:\n",
    "            weighted_t_embs.append(combined_vec * (0.5 * IDF_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0240f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "IDF_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe201b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(token_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e0cde1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
