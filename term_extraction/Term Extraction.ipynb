{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "788a8bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KM cuda not found, defaulting to sklearn CPU version of kmeans\n"
     ]
    }
   ],
   "source": [
    "from typing import Union, List, Any, Optional, Dict\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "import urllib\n",
    "import requests\n",
    "import subprocess\n",
    "import concurrent.futures\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from textblob import TextBlob\n",
    "from threading import current_thread\n",
    "from collections import Counter\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "try:\n",
    "    from libKMCUDA import kmeans_cuda\n",
    "except:\n",
    "    print(\"KM cuda not found, defaulting to sklearn CPU version of kmeans\")\n",
    "    kmeans_cuda = None\n",
    "\n",
    "from utils import cleaning_utils\n",
    "from utils import cluster_utils\n",
    "from utils import IDF_computation\n",
    "from utils import embedding_utils as embedding\n",
    "from utils.customdocument import CustomDocument"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d081bf43",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> \n",
    "We will run our term extraction on the Merged Approved Documents, the .pdf file can be found in the `data/` directory. For filtering of out-of-domain terms we will also process a set of EU regulations for medical device design, the .html files for these can be found in the same directory.\n",
    "</div>\n",
    "\n",
    "1. Preprocessing will consist only of removing headers/footers from PDF files. \n",
    "\n",
    "2. Candidate terms are identified using SPaR.txt (Kruiper et al., 2021), sentence splitting is done with the PunkSentTokenizer (Strunk, 2006).\n",
    "\n",
    "3. Filtering of term candidates consists of:\n",
    "  * a set of regular expressions found in utils.py\n",
    "  * clustering of terms found in (1) the Approved Documents, and (2) a set of EU regulations for medical device design; any clusters containing terms from (2) will be designated as terms that are irrelevant to the AEC domain.\n",
    "  \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996a98d0",
   "metadata": {},
   "source": [
    "### 1 Preprocessing: get text from PDF and HTML\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "Here, we grab the text from our foreground and background corpora.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f74d6fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approved Documents: data/input/The Merged Approved Documents.pdf\n",
      "Reference corpus: ['data/input/EUR-Lex - 31993L0042 - EN.html', 'data/input/CELEX 32017R0746 EN TXT.html', 'data/input/CELEX 32017R0745 EN TXT.html', 'data/input/EUR-Lex - 31998L0079 - EN.html', 'data/input/EUR-Lex - 31990L0385 - EN.html']\n"
     ]
    }
   ],
   "source": [
    "merged_approved_pdf_file = glob.glob(\"data/input/*.pdf\")[0]\n",
    "eu_html_files = glob.glob(\"data/input/*.html\")\n",
    "print(f\"Approved Documents: {merged_approved_pdf_file}\")\n",
    "print(f\"Reference corpus: {eu_html_files}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271bdc02",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "First we grab the text from the Merged Approved Documents pdf file. Our implementation on based on the pdf conversion pipeline in Haystack.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e0d20e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf(file_path: Path, layout: bool = True, encoding: Optional[str] = \"Latin1\") -> List[str]:\n",
    "        \"\"\"\n",
    "        Extract pages from the pdf file at file_path; based on Haystack.\n",
    "\n",
    "        :param file_path: path of the pdf file\n",
    "        :param layout: whether to retain the original physical layout for a page. If disabled, PDF pages are read in\n",
    "                       the content stream order.\n",
    "        \"\"\"\n",
    "        if layout:\n",
    "            command = [\"pdftotext\", \"-enc\", encoding, \"-layout\", str(file_path), \"-\"]\n",
    "        else:\n",
    "            command = [\"pdftotext\", \"-enc\", encoding, str(file_path), \"-\"]\n",
    "        output = subprocess.run(command, stdout=subprocess.PIPE, shell=False)  # type: ignore\n",
    "        document = output.stdout.decode(errors=\"ignore\")\n",
    "        pages = document.split(\"\\f\")\n",
    "        pages = pages[:-1]  # the last page in the split is always empty.\n",
    "        return pages\n",
    "\n",
    "def convert_pdf_to_mydoc(source_file_path: Path, \n",
    "                         output_file_path: Path, \n",
    "                         meta: Optional[Dict[str, str]] = None,\n",
    "                         remove_header_and_footer: Optional[bool] = True,\n",
    "                         clean_whitespace: Optional[bool] = True,\n",
    "                         clean_empty_lines: Optional[bool] = True,\n",
    "                         encoding: Optional[str] = \"Latin1\") -> CustomDocument:\n",
    "        \"\"\"\n",
    "        Extract pages from the pdf file at file_path; based on Haystack.\n",
    "\n",
    "        :param output_file_path:    Path to the .json file to store the converted file.\n",
    "        :param source_file_path:    Path to the .pdf file you want to convert\n",
    "        :param meta: Optional dictionary with metadata that shall be attached to all resulting documents.\n",
    "                     Can be any custom keys and values.\n",
    "        :param encoding: Encoding that will be passed as -enc parameter to pdftotext. \"Latin 1\" is the default encoding\n",
    "                         of pdftotext. While this works well on many PDFs, it might be needed to switch to \"UTF-8\" or\n",
    "                         others if your doc contains special characters (e.g. German Umlauts, Cyrillic characters ...).\n",
    "                         Note: With \"UTF-8\" we experienced cases, where a simple \"fi\" gets wrongly parsed as\n",
    "                         \"xef\\xac\\x81c\" (see test cases). That's why we keep \"Latin 1\" as default here.\n",
    "                         (See list of available encodings by running `pdftotext -listenc` in the terminal)\n",
    "        \"\"\"\n",
    "        pages = read_pdf(source_file_path, layout=True, encoding=encoding)\n",
    "\n",
    "        if not pages:\n",
    "            # empty input file\n",
    "            return None\n",
    "        \n",
    "        pages = [\"\\n\".join(p.splitlines()) for p in pages]\n",
    "\n",
    "        # splitting text happens during preprocessing, so no split_size passed here;\n",
    "        # split_size will be set to -1 during conversion.\n",
    "        document = CustomDocument(output_file_path, source_file_path, split_size=-1)\n",
    "        \n",
    "        print(\"Converted PDF file to pages of text, combining to a single CustomDocument to keep track of page nrs.\")\n",
    "        for page_idx, page in tqdm(enumerate(pages)):\n",
    "            \n",
    "            # some simple cleaning -- roughly based on haystack.\n",
    "            lines = page.splitlines()\n",
    "            if remove_header_and_footer:\n",
    "                # simplest way for removing header and footer \n",
    "                lines = lines[1:-2]\n",
    "\n",
    "            if clean_whitespace:\n",
    "                cleaned_lines = []\n",
    "                for line in lines:\n",
    "                    line = line.strip()\n",
    "                    cleaned_lines.append(line)\n",
    "                text = \" \".join(cleaned_lines)\n",
    "\n",
    "            if clean_empty_lines:\n",
    "                text = re.sub(r\"\\n\\n+\", \"\\n\\n\", text)\n",
    "                text = re.sub(r\"[\\s]+\", \" \", text)\n",
    "            \n",
    "            # no splitting here yet, so simply using page_nr as a place holder and split_id is left blank\n",
    "            page_nr = str(page_idx + 1)\n",
    "            document.add_content(text=text, \n",
    "                                 page_nr=page_nr, \n",
    "                                 doc_title=source_file_path.rsplit('/',1)[1])   # we're using the pdf file name for simplicity\n",
    "\n",
    "        return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71184fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted PDF file to pages of text, combining to a single CustomDocument to keep track of page nrs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1274it [00:00, 7330.21it/s]\n"
     ]
    }
   ],
   "source": [
    "merged_approved_document = convert_pdf_to_mydoc(merged_approved_pdf_file, \"data/converted_documents/merged_approved.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1473ee8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum character length for a single block of text: 5537\n"
     ]
    }
   ],
   "source": [
    "print(f\"Maximum character length for a single block of text: {max([len(c.text) for c in merged_approved_document.all_contents])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203400ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84117041",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Second, we grab the text from the EU regulation HTML files. Because the text in HTML files isn't split into pages, the blocks of text are much longer.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5bb53f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_HTML_text_simple(file):\n",
    "    \"\"\"\n",
    "    All text in the EU htmls seems to be captured neatly in <p> tags, we don't care about structure currently.\n",
    "    We do remove all unicode characters, see `utils.remove_unicode_chars()`.\n",
    "    \"\"\" \n",
    "    with open(file, 'r') as f:\n",
    "        text = f.read()\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    return [cleaning_utils.remove_unicode_chars(x.text) for x in soup.body.find_all('p')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6630e6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_html_to_mydoc(source_file_path: Path, \n",
    "                          output_file_path: Path) -> CustomDocument:\n",
    "    \n",
    "    document = CustomDocument(output_file_path, source_file_path, split_size=-1)\n",
    "    document_paragraphs = []\n",
    "    list_of_paragraphs = grab_HTML_text_simple(html_file)\n",
    "    for paragraph in list_of_paragraphs:\n",
    "        if paragraph.strip() != '':\n",
    "            document_paragraphs.append(paragraph)\n",
    "    \n",
    "    for paragraph_idx, paragraph in tqdm(enumerate(document_paragraphs)):\n",
    "            # no splitting here yet, so simply using page_nr as a place holder and split_id is left blank\n",
    "            paragraph_nr = str(paragraph_idx + 1)\n",
    "            document.add_content(text=paragraph, \n",
    "                                 page_nr=paragraph_nr, \n",
    "                                 doc_title=source_file_path) # we're using the html file name for simplicity\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91b68b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                               | 0/5 [00:00<?, ?it/s]\n",
      "826it [00:00, 354206.64it/s]\n",
      "\n",
      "4344it [00:00, 351711.38it/s]\n",
      " 40%|██████████████████████████████████████████████████████████████████▊                                                                                                    | 2/5 [00:00<00:01,  2.27it/s]\n",
      "4799it [00:00, 366832.48it/s]\n",
      " 60%|████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                  | 3/5 [00:01<00:01,  1.65it/s]\n",
      "623it [00:00, 270979.09it/s]\n",
      "\n",
      "511it [00:00, 232688.02it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.81it/s]\n"
     ]
    }
   ],
   "source": [
    "eu_regulation_documents = []\n",
    "for html_file in tqdm(eu_html_files):\n",
    "    outfile = f\"data/converted_documents/{html_file.rsplit('/',1)[1]}.json\"\n",
    "    eu_regulation_documents.append(convert_html_to_mydoc(html_file, outfile))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "033634ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum character length for a single paragraph: 143428\n"
     ]
    }
   ],
   "source": [
    "print(f\"Maximum character length for a single paragraph: {max([len(c.text) for d in eu_regulation_documents for c in d.all_contents])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77347abc",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Third, if the output document doesn't exist (yet), we save the ConvertedDocument.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24237e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "foreground_corpus = [merged_approved_document]\n",
    "background_corpus = eu_regulation_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a00ed81",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in foreground_corpus + background_corpus:\n",
    "    if not os.path.exists(d.output_fp):\n",
    "        d.write_document()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f6e56a",
   "metadata": {},
   "source": [
    "### 2 Term extraction: identify object spans with SPaR.txt\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "For each of the sentences in our corpora, we run SPaR.txt for object identification.\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edf22a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download SPaR.txt if required\n",
    "if not os.path.exists(\"SPaR.txt/README.md\"):\n",
    "    !git clone https://github.com/rubenkruiper/SPaR.txt.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b19aa4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imp\n",
    "with open('SPaR.txt/spar_predictor.py', 'rb') as fp:\n",
    "    spar_predictor = imp.load_module(\n",
    "        'spar_predictor', fp, 'SPaR.txt.spar_predictor.py',\n",
    "        ('.py', 'rb', imp.PY_SOURCE)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9daef1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# trains a model if needed, otherwise loads from archive; \n",
    "# - best F1 on dev/validation in the paper is 80,96 trained on a GPU, CPU will be a bit lower ~77.x I think\n",
    "sp = spar_predictor.SparPredictor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0206ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example input: An example sentence to show how ACC terminology will be extracted from the British Standards.\n",
      "{'obj': ['An example sentence', 'ACC terminology', 'the British Standards']}\n",
      "Parsing took 0.11501312255859375\n"
     ]
    }
   ],
   "source": [
    "# example \n",
    "example = \"An example sentence to show how ACC terminology will be extracted from the British Standards.\"\n",
    "start_time = time.time()\n",
    "# prepare instance and run model on single instance\n",
    "docid = ''                  # ToDo - add doc_id during pre_processing?\n",
    "token_list = sp.predictor._dataset_reader.tokenizer.tokenize(example)\n",
    "instance = sp.predictor._dataset_reader.text_to_instance(docid,\n",
    "                                                      example,\n",
    "                                                      token_list,\n",
    "                                                      sp.predictor._dataset_reader._token_indexer)\n",
    "result = sp.predictor.predict_instance(instance)\n",
    "printable_result = sp.parse_output(result, ['obj'])\n",
    "print(f\"Example input: {example}\")\n",
    "print(printable_result)\n",
    "print(\"Parsing took {}\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9d58f4",
   "metadata": {},
   "source": [
    "* To run SPaR.txt, we split the text in our corpora into sentences. We set up multiple instances of SPaR.txt predictors to speed up the processing a little."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5f7c27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These should automatically run on your Nvidia GPU if available\n",
    "class SparInstance:\n",
    "    def __init__(self):\n",
    "        self.sp = spar_predictor.SparPredictor()\n",
    "    \n",
    "    def call(self, input_str:str=''):\n",
    "        if input_str:\n",
    "            # prepare instance and run model on single instance\n",
    "            docid = ''  # ToDo - add doc_id during pre_processing?\n",
    "            token_list = self.sp.predictor._dataset_reader.tokenizer.tokenize(input_str)\n",
    "\n",
    "            # truncating the input to SPaR.txt to maximum 512 tokens\n",
    "            token_length = len(token_list)\n",
    "            if token_length > 512:\n",
    "                token_list = token_list[:511] + [token_list[-1]]\n",
    "                token_length = 512\n",
    "\n",
    "            instance = self.sp.predictor._dataset_reader.text_to_instance(docid, input_str, token_list,\n",
    "                                                              self.sp.predictor._dataset_reader._token_indexer)\n",
    "            result = self.sp.predictor.predict_instance(instance)\n",
    "            printable_result = self.sp.parse_output(result, ['obj'])\n",
    "            return {\n",
    "                    \"prediction\": printable_result,\n",
    "                    \"num_input_tokens\": token_length,\n",
    "            }\n",
    "            \n",
    "        # If the input is None, or too long, return an empty list of objects\n",
    "        return {\n",
    "                \"prediction\": {'obj': []},\n",
    "                \"num_input_tokens\": 0\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb733c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TermExtractor:\n",
    "    \n",
    "    def __init__(self, split_length=300, max_num_cpu_threads=4):\n",
    "        \"\"\"\n",
    "        Initialise SPaR.txt predictors `max_num_cpu_threads` \n",
    "        \"\"\"\n",
    "        self.split_length = split_length   # in number of tokens\n",
    "        self.max_num_cpu_threads = max_num_cpu_threads\n",
    "        self.PREDICTORS = []\n",
    "        for i in range(max_num_cpu_threads + 1):\n",
    "            self.PREDICTORS.append(SparInstance())\n",
    "    \n",
    "    \n",
    "    def process_sentence(self, sentence: str = ''):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        predictor_to_use = int(current_thread().name.rsplit('_', 1)[1])\n",
    "        spartxt = self.PREDICTORS[predictor_to_use]\n",
    "\n",
    "        # SPaR doesn't handle ALL uppercase sentences well, which the OCR system sometimes outputs    \n",
    "        sentence = sentence.lower() if sentence.isupper() else sentence\n",
    "        prediction_dict =  spartxt.call(sentence)\n",
    "        if not prediction_dict:\n",
    "            return []\n",
    "\n",
    "        pred_labels = prediction_dict[\"prediction\"]\n",
    "        return pred_labels['obj']\n",
    "        \n",
    "\n",
    "    def split_into_sentences_and_run_spar(self, input_document):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        print(f\"Working on: {input_document.source_fp}\")\n",
    "        content_as_list_of_dicts = input_document.to_list_of_dicts()\n",
    "        total_number_of_sentences_found = 0\n",
    "        content_idx = 0\n",
    "        for content_dict in tqdm(content_as_list_of_dicts):\n",
    "\n",
    "            text = ' '.join([x for x in content_dict[\"content\"].split(' ') if x != ''])\n",
    "            # some really long paragraphs in the EU regulations are summations that should be split at ';'\n",
    "            if len(text) > 3000:\n",
    "                text = text.replace(\";\", \".\\n\")\n",
    "\n",
    "            # We'll split into sentences even if this has been done before, it doesn't take long\n",
    "            sentences = []\n",
    "            for part in text.split('\\n'):\n",
    "                # split into sentences using PunktSentTokenizer (TextBlob implements NLTK's version under the hood) \n",
    "                sentences += [str(s) for s in TextBlob(part).sentences if len(str(s)) > 10]\n",
    "\n",
    "            content_dict[\"meta\"][\"sentences\"] = '###'.join(sentences)\n",
    "                \n",
    "            total_number_of_sentences_found += len(sentences)\n",
    "\n",
    "            # process sentences in the content and add SPaR.txt object tags to the content dict.        \n",
    "            if not content_dict[\"meta\"][\"SPaR_labels\"]:\n",
    "                with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_num_cpu_threads) as executor:\n",
    "                    futures = [executor.submit(self.process_sentence, sentences[idx]) for idx in range(len(sentences))]\n",
    "\n",
    "                content_spar_objects = [f.result() for f in futures]\n",
    "                content_dict[\"meta\"][\"SPaR_labels\"] = ', '.join([tag for tags in content_spar_objects for tag in tags])\n",
    "                \n",
    "            # immediately update the list of content_dicts and every X iterations we save the file \n",
    "            content_as_list_of_dicts[content_idx] = content_dict\n",
    "            if content_idx // 5 == 0:\n",
    "                converted_document.replace_contents(content_as_list_of_dicts)\n",
    "                converted_document.write_document()\n",
    "            \n",
    "            content_idx += 1\n",
    "\n",
    "        print(f\"Number of sentences found: {total_number_of_sentences_found}\")\n",
    "        converted_document.replace_contents(content_as_list_of_dicts)\n",
    "        converted_document.write_document()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7dc2f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "te = TermExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3d086b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on: data/input/The Merged Approved Documents.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1274/1274 [00:02<00:00, 626.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences found: 17745\n",
      "Working on: data/input/EUR-Lex - 31993L0042 - EN.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 826/826 [00:00<00:00, 3391.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences found: 1679\n",
      "Working on: data/input/CELEX 32017R0746 EN TXT.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4344/4344 [00:01<00:00, 2528.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences found: 3405\n",
      "Working on: data/input/CELEX 32017R0745 EN TXT.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4799/4799 [00:01<00:00, 2722.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences found: 3838\n",
      "Working on: data/input/EUR-Lex - 31998L0079 - EN.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 623/623 [00:00<00:00, 2350.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences found: 1255\n",
      "Working on: data/input/EUR-Lex - 31990L0385 - EN.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 511/511 [00:00<00:00, 4245.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences found: 771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run SPaR.txt on all documents and write to file\n",
    "for converted_document in foreground_corpus + background_corpus:\n",
    "    # re-load the document from file, to make sure we don't overwrite existing SPaR.txt labels\n",
    "    converted_document = converted_document.load_document(converted_document.output_fp)\n",
    "    te.split_into_sentences_and_run_spar(converted_document)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4835185f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Number of sentences (longer than 10 characters) found in: <ul>\n",
    "    <li>Merged Approved documents: 17745</li>\n",
    "    <li>Background corpus (1679+3405+3838+1255+771): 10948</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b15878",
   "metadata": {},
   "source": [
    "### 3 Filtering\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "The objects identified by SPaR.txt contain a lot of noise. Here, we clean and filter them; based on our background corpus we try to identify which terms belong to the AEC domain. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b511762d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "First, load all terms from the processed files.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4fb5373f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of foreground terms: 114333\n",
      "Total number of UNIQUE foreground terms: 42657\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('the', 2130),\n",
       " ('a', 930),\n",
       " ('the building', 864),\n",
       " ('buildings', 839),\n",
       " ('guidance', 522),\n",
       " ('a building', 455),\n",
       " ('the Building Regulations', 434),\n",
       " ('document', 303),\n",
       " ('people', 301),\n",
       " ('the requirements', 297)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foreground_terms_lists = [c.NER_labels for d in foreground_corpus for c in d.load_document(d.output_fp).all_contents]\n",
    "foreground_terms = [t for t_list in foreground_terms_lists for t in t_list if t]\n",
    "foreground_terms_c = Counter(foreground_terms)\n",
    "print(f\"Total number of foreground terms: {len(foreground_terms)}\")\n",
    "print(f\"Total number of UNIQUE foreground terms: {len(foreground_terms_c)}\")\n",
    "foreground_terms_c.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8006f6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of background terms: 73124\n",
      "Total number of UNIQUE background terms: 10245\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('the', 2651),\n",
       " ('devices', 1641),\n",
       " ('the manufacturer', 1243),\n",
       " ('the device', 1187),\n",
       " ('Regulation', 624),\n",
       " ('the notified body', 595),\n",
       " ('information', 550),\n",
       " ('Member States', 489),\n",
       " ('a', 451),\n",
       " ('the market', 447)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "background_terms_lists = [c.NER_labels for d in background_corpus for c in d.load_document(d.output_fp).all_contents]\n",
    "background_terms = [t for t_list in background_terms_lists for t in t_list if t]\n",
    "background_terms_c = Counter(background_terms)\n",
    "print(f\"Total number of background terms: {len(background_terms)}\")\n",
    "print(f\"Total number of UNIQUE background terms: {len(background_terms_c)}\")\n",
    "background_terms_c.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e79727a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique spans identified by SPaR.txt:51296\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of unique spans identified by SPaR.txt:{len(foreground_terms_c+background_terms_c)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464451e8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Second, clean the terms with the regular expressions we've defined in utils.py\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2503708d",
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_filter = cleaning_utils.RegexFilter()\n",
    "def run_filters(input_counter):\n",
    "    cleaned_counter = Counter()\n",
    "    for k, v in input_counter.items():\n",
    "        # terms should occur twice at least\n",
    "        if v < 2:\n",
    "            continue\n",
    "\n",
    "        # todo; clean up these util functions and how to call them|\n",
    "        _, k = regex_filter.run_filter(k)\n",
    "        if k:\n",
    "            cleaned_k = cleaning_utils.custom_cleaning_rules(k)\n",
    "            if cleaned_k:\n",
    "                cleaned_counter[cleaned_k[0]] = v\n",
    "    return cleaned_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2e42c7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11187\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('guidance', 522),\n",
       " ('a building', 455),\n",
       " ('document', 303),\n",
       " ('the requirements', 297),\n",
       " ('requirements', 266),\n",
       " ('work', 242),\n",
       " ('the guidance', 240),\n",
       " ('the work', 232),\n",
       " ('Schedule 1', 226),\n",
       " ('the dwelling', 215)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_foreground_terms_c = run_filters(foreground_terms_c)\n",
    "print(len(cleaned_foreground_terms_c))\n",
    "cleaned_foreground_terms_c.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "899b1cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6189\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('devices', 1641),\n",
       " ('the device', 1187),\n",
       " ('the notified body', 595),\n",
       " ('Member States', 489),\n",
       " ('accordance', 429),\n",
       " ('a device', 405),\n",
       " ('conformity', 357),\n",
       " ('the requirements', 350),\n",
       " ('The notified body', 328),\n",
       " ('notified bodies', 324)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_background_terms_c = run_filters(background_terms_c)\n",
    "print(len(cleaned_background_terms_c))\n",
    "cleaned_background_terms_c.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406d6ca8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Third, we compute the embeddings for both foreground and background terms.\n",
    "</div>\n",
    "\n",
    "\n",
    "* Note: embeddings will be IDF weighted (IDF weights over both foreground and background corpora)\n",
    "  * Could add more sentences to the computation of IDF weights, e.g., definitions from vocabularies/WikiData."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0b177bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which tokenizer to use for IDF computation and Embedding;\n",
    "bert_model_name = 'bert-base-cased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "10278f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_terms_c = cleaned_foreground_terms_c + cleaned_background_terms_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8f403d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16441"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_terms_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a1000a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('devices', 1665),\n",
       " ('the device', 1195),\n",
       " ('the requirements', 647),\n",
       " ('accordance', 631),\n",
       " ('the notified body', 595),\n",
       " ('guidance', 539),\n",
       " ('Member States', 491),\n",
       " ('requirements', 457),\n",
       " ('a building', 455),\n",
       " ('a device', 412)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_terms_c.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ac5654c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of MWEs: 10833\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['construction features',\n",
       " 'mm Notes',\n",
       " 'mobility impairments',\n",
       " 'These extracts',\n",
       " 'Resistance to',\n",
       " 'roof tests',\n",
       " 'the conformity assessment bodies',\n",
       " 'the Building Regulations compliance certificate',\n",
       " 'melting point',\n",
       " 'single use',\n",
       " 'conformity assessment',\n",
       " 'Section 3 :',\n",
       " 'The operating instructions',\n",
       " 'period of',\n",
       " 'The local authority',\n",
       " 'storage area',\n",
       " 'a horizontal roof',\n",
       " 'part floor Compartment',\n",
       " 'a protected entrance hall',\n",
       " 'the U - values',\n",
       " 'safety concerns',\n",
       " 'horizontal ties',\n",
       " 'with pressure - jet burners',\n",
       " 'a fire and rescue service pumping appliance',\n",
       " 'Market surveillance activities',\n",
       " 'individual dwelling',\n",
       " 'procedural aspects',\n",
       " 'paragraph iv )',\n",
       " 'homogeneous batches',\n",
       " 'transitional period',\n",
       " 'a draft assessment report',\n",
       " 'where research',\n",
       " 'masonry inner leaf Junctions',\n",
       " 'Article 1  a )',\n",
       " 'Annex IV V',\n",
       " 'compartment wall',\n",
       " 'requirement B1',\n",
       " 'a full justification',\n",
       " 'a minimum Table B4 See Table B4',\n",
       " 'May 1989',\n",
       " 'the health institutions',\n",
       " 'sloping surfaces',\n",
       " 'the reception point',\n",
       " 'public areas',\n",
       " 'Guild of',\n",
       " 'Official Journal',\n",
       " 'level of traceability',\n",
       " 'water use',\n",
       " 'the relevant parts',\n",
       " 'plant rooms surface',\n",
       " 'mineral wool )',\n",
       " 'Light reflectance value',\n",
       " 'a clinical trial',\n",
       " 'Committee procedure',\n",
       " 'm - 1',\n",
       " 'type examination',\n",
       " 'The validation report',\n",
       " 'classification rules',\n",
       " 'requirement K5 1',\n",
       " 'the original condition',\n",
       " 'a cavity barrier',\n",
       " 'mm turning circle',\n",
       " 'a fire alarm system',\n",
       " 'covered car BS 5446 - 2 Fire detection fire alarm devices',\n",
       " 'paragraph B3 11',\n",
       " 'paragraph 5 8',\n",
       " 'the technical documentation 4 4',\n",
       " 'the essential components',\n",
       " 'the mobile platform',\n",
       " 'the claimed specification',\n",
       " 'a logistics unit',\n",
       " 'Width D4 Width',\n",
       " 'certified reference materials',\n",
       " 'usability enhancements',\n",
       " 'the Timber Research and Development Association',\n",
       " 'Annexes IX XI',\n",
       " 'Effect Gas Appliances',\n",
       " 'Part F ventilation',\n",
       " 'Section 4 3 second third subparagraphs',\n",
       " 'the applicable regulatory requirements',\n",
       " 'SI 2006 / 31 W5',\n",
       " 'Table B2',\n",
       " 'buildingregulations /',\n",
       " 'applicable 4 ) 4 )',\n",
       " 'a secure door',\n",
       " 'accessible corridors',\n",
       " 'the packaging system',\n",
       " 'a material change',\n",
       " 'Part K',\n",
       " 'electric shocks',\n",
       " 'the investigational site',\n",
       " 'safety performance',\n",
       " 'each input parameter',\n",
       " 'the Building Regulations 2000',\n",
       " 'introductory section',\n",
       " 'paragraph 6 6',\n",
       " 'buildings 24',\n",
       " 'pressurisation techniques',\n",
       " 'the relevant part',\n",
       " 'straight lines']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some insight in number of MWEs\n",
    "mwes = []\n",
    "for t in all_terms_c.keys():\n",
    "    words = t.split(' ')\n",
    "    if words[0] in ['the', 'a', 'The', 'A', 'an', 'An', 'any', 'Any', 'this', 'This']:\n",
    "        words = words[1:]\n",
    "    if len(words) > 1:\n",
    "        mwes.append(t)\n",
    "\n",
    "print(f\"Number of MWEs: {len(mwes)}\")\n",
    "random.sample(mwes, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d8d0d4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute IDF weights\n",
    "sentence_lists = [c.sentences for d in foreground_corpus + background_corpus for c in d.load_document(d.output_fp).all_contents]\n",
    "all_sentences = [s for sent_list in sentence_lists for s in sent_list if s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7bf4a15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28693\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['The Merged Approved Documents',\n",
       " 'How to use the Merged Approved Documents This document combines the approved documents into a single PDF.',\n",
       " 'Each approved document is self-contained and has its own introduction.',\n",
       " 'Each introduction relates only to the corresponding approved document.',\n",
       " \"Each introduction also contains information on when the document's guidance came into effect (or will come into effect).\",\n",
       " 'It is important to check that the version of each approved document you are using remains current and is the correct version for your project.',\n",
       " 'Please refer to the Ministry of Housing, Communities and Local Government website to check, and confirm with your building control body if in doubt.',\n",
       " 'Key features The Merged Approved Documents enable the user to: undertake a word search across all of the approved documents cut and paste text and diagrams into other documents add notes to a saved copy use an index to access individual sections of the guidance Correction to Approved Document K The heading in section 1.18 of the online version of Approved Document K have been corrected to match the print version.',\n",
       " \"Forthcoming changes Please check the Ministry of Housing, Communities and Local Government's website to ensure that each approved document you are using is current for your project.\",\n",
       " 'This is particularly important in relation to Approved Document B as this has been subject to frequent update.']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(all_sentences))\n",
    "all_sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c09c984e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing IDF weights.\n",
      "Printing some IDF values, should be subword units!\n",
      "['cluster']\n",
      "['##of']\n",
      "['deposits']\n",
      "['ultimate']\n",
      "['horizontal']\n",
      "['proximity']\n",
      "['estates']\n",
      "['exploration']\n",
      "['##most']\n",
      "['rendered']\n"
     ]
    }
   ],
   "source": [
    "IDF_c = IDF_computation.IdfComputer(\"data/IDF_weights.json\", bert_model_name=bert_model_name)\n",
    "IDF_path = IDF_c.compute_or_load_IDF_weights(all_sentences, overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "411b8d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed each of the terms identified by SPaR.txt, applies IDF weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5b1959bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique terms: 16441\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['the Merged Approved Documents',\n",
       " 'document',\n",
       " 'documents',\n",
       " 'Each introduction',\n",
       " 'approved',\n",
       " 'information',\n",
       " 'guidance',\n",
       " 'the version',\n",
       " 'project',\n",
       " 'the Ministry']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_terms = [k for k in all_terms_c.keys()] # counter keys, so already unique\n",
    "print(f\"Number of unique terms: {len(all_terms)}\")\n",
    "all_terms[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "26f20d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "bert_model = BertModel.from_pretrained(bert_model_name, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a669a4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = embedding.Embedder(tokenizer, bert_model, \n",
    "                              IDF_dict=json.load(open(IDF_path)), \n",
    "                              embedding_fp=\"output/\",\n",
    "                              layers_to_use = [12],         # we'll use the output of the last layer\n",
    "                              layer_combination = \"avg\",    # how to combine layers if multiple are used\n",
    "                              idf_threshold = 1.5,          # minimum IDF value for a token to contribute\n",
    "                              idf_weight_factor = 1.0,      # modify how strong the influence of IDF weighting is\n",
    "                              not_found_idf_value = 0.5)    # IDF value for tokens that weren't seen during IDF computation (doesn't apply here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ce59ea93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the embeddings, this is split into subsets so we don't overload your memory (adjust these values if needed)\n",
    "max_num_cpu_threads = 4\n",
    "subset_size = 1000\n",
    "\n",
    "# Checks which of the embeddings for the clustering cluster_data already exist, so they can be re-used\n",
    "term_subsets = cleaning_utils.split_list(all_terms, subset_size)\n",
    "embedding_files = glob.glob(embedder.embedding_fp + 'embeddings*.pkl')\n",
    "span_and_embedding_pairs = []\n",
    "if len(embedding_files) == len(term_subsets):\n",
    "    for e in embedding_files:\n",
    "        span_and_embedding_pairs += pickle.load(open(e, 'rb'))\n",
    "else:\n",
    "    print(f\"Preparing embeddings for {len(all_terms)} spans, in groups of: {subset_size}\")\n",
    "    subset_idx = 0            # iterator index outside of tqdm \n",
    "    for subset in tqdm(term_subsets):\n",
    "        subset_embeddings = []\n",
    "        subset_file_name = embedder.embedding_fp + \"embeddings_part_\" + '{}.pkl'.format(subset_idx)\n",
    "        subset_idx += 1\n",
    "        if os.path.exists(subset_file_name):\n",
    "            # already computed previously\n",
    "            continue\n",
    "        \n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=max_num_cpu_threads) as executor:\n",
    "            futures = [executor.submit(embedder.embed_a_span, subset[idx]) for idx in range(len(subset))]\n",
    "\n",
    "        subset_embeddings += [f.result() for f in futures if f.result()]\n",
    "\n",
    "        with open(subset_file_name, 'wb') as f:\n",
    "            pickle.dump(subset_embeddings, f)\n",
    "\n",
    "    # Once all embeddings are created; combine them in span_and_embedding_pairs\n",
    "    embedding_files = glob.glob(embedder.embedding_fp + \"embeddings_part_\" + '*.pkl')\n",
    "    for e in embedding_files:\n",
    "        span_and_embedding_pairs += pickle.load(open(e, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b4e8b64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalising and combining computed/existing 17 embeddings from files into single file\n"
     ]
    }
   ],
   "source": [
    "# Create a single file with all embeddings, in the meantime standardising the embeddings to improve the representation\n",
    "print(f\"Normalising and combining computed/existing {len(embedding_files)} embeddings from files into single file\")\n",
    "unique_spans, unique_embeddings = zip(*span_and_embedding_pairs)\n",
    "with open(embedder.embedding_fp + \"unique_spans.pkl\", 'wb') as f:\n",
    "    pickle.dump(unique_spans, f)\n",
    "\n",
    "with open(embedder.embedding_fp + \"unique_embeddings.pkl\", 'wb') as f:\n",
    "    # we average over the token embeddings in a term\n",
    "    unique_clustering_data = np.stack([np.mean(e, axis=0) if len(e.shape) > 1 else e for e in unique_embeddings])\n",
    "\n",
    "    # standardise the unique clustering data, as suggested by https://github.com/wtimkey/rogue-dimensions\n",
    "    embedder.emb_mean = unique_clustering_data.mean(axis=0)\n",
    "    embedder.emb_std = unique_clustering_data.std(axis=0)\n",
    "    pickle.dump(embedder.emb_mean, open(embedder.embedding_fp + \"standardisation_mean.pkl\", 'wb'))\n",
    "    pickle.dump(embedder.emb_std, open(embedder.embedding_fp + \"standardisation_std.pkl\", 'wb'))\n",
    "\n",
    "    standardised_clustering_data = (unique_clustering_data - embedder.emb_mean) / embedder.emb_std\n",
    "\n",
    "    pickle.dump(standardised_clustering_data, f)\n",
    "    \n",
    "# Store the standardised embeddings for reuse; could honeslty remove all the other embedding files but will keep them just in case\n",
    "pickle.dump(standardised_clustering_data, open(embedder.embedding_fp + \"standardised_embeddings.pkl\", 'wb'))\n",
    "spans_and_standardised_embeddings_dict = dict(zip(unique_spans, standardised_clustering_data))\n",
    "# pickle.dump(spans_and_standardised_embeddings_dict, open(embedder.embedding_fp + \"spans_and_standardised_embeddings_dict.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1096f9d5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Fourth, we cluster the embeddings with KMeans.\n",
    "</div>\n",
    "\n",
    "\n",
    "* Note: We try various values for K, the number of clusters. We'll  try to get some insight in a good value for K based on the Elbow score and Silhouette score. However, due to the sparsity of the set of terms found in the Approved Documents and the EU regulations these scores aren't very insightful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "30daa510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to compute clusters on either CPU or GPU\n",
    "def compute_clusters_sklearn(standardised_clustering_data, cluster_model_fp, num_clusters=10):\n",
    "    \"\"\"\n",
    "    Note that this clustering function relies on the CPU. It won't be able to compute clusters for large \n",
    "    amounts of inputs, e.g., 100.000 spans. When using a large number of clusters (e.g. 5000) it is also\n",
    "    a lot slower than a GPU implementation for. Or it may simply not converge! \n",
    "    For large inputs/num_clusters you'll need to use compute_clusters_kmcuda, and have access to a GPU.\n",
    "    \"\"\"\n",
    "    print(f\"Computing {num_clusters} clusters from scratch, using sklearn on the CPU\")\n",
    "    start_time = time.time()\n",
    "    sklearn_kmeans = KMeans(n_clusters=num_clusters, init='k-means++', max_iter=3, n_init=1, random_state=14,\n",
    "                            tol=0.0001, verbose=0)\n",
    "    assignments = sklearn_kmeans.fit_predict(standardised_clustering_data)\n",
    "    centroids = sklearn_kmeans.cluster_centers_\n",
    "    print(\"Clustering took {}\".format(time.time() - start_time))\n",
    "    with open(cluster_model_fp, 'wb') as f:\n",
    "        pickle.dump((centroids, assignments), f)\n",
    "        \n",
    "# compute clusters on CPU for now\n",
    "def compute_clusters_kmcuda(standardised_clustering_data, cluster_model_fp, num_clusters=10000):\n",
    "    \"\"\"\n",
    "    Won't implement cosine KMeans here, as I want to predict with sklearn in this notebook. \n",
    "    Also not sure if the results are really that much better.\n",
    "    \"\"\"\n",
    "    centroids, assignments = kmeans_cuda(standardised_clustering_data, num_clusters, init=\"k-means++\",\n",
    "                                                 verbosity=1, seed=14) # , device=0)\n",
    "    with open(cluster_model_fp, 'wb') as f:\n",
    "        pickle.dump((centroids, assignments), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b39d48c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn cluster file exists for 500 clusters\n",
      "sklearn cluster file exists for 1000 clusters\n",
      "sklearn cluster file exists for 1500 clusters\n",
      "sklearn cluster file exists for 2000 clusters\n",
      "sklearn cluster file exists for 2500 clusters\n",
      "sklearn cluster file exists for 3000 clusters\n",
      "sklearn cluster file exists for 3500 clusters\n",
      "sklearn cluster file exists for 4000 clusters\n",
      "sklearn cluster file exists for 4500 clusters\n",
      "sklearn cluster file exists for 5000 clusters\n",
      "sklearn cluster file exists for 5500 clusters\n",
      "sklearn cluster file exists for 6000 clusters\n",
      "sklearn cluster file exists for 6500 clusters\n",
      "sklearn cluster file exists for 7000 clusters\n",
      "sklearn cluster file exists for 7500 clusters\n",
      "sklearn cluster file exists for 8000 clusters\n"
     ]
    }
   ],
   "source": [
    "if not kmeans_cuda:\n",
    "    clustering_type = \"sklearn\"\n",
    "    # Computing clusters on the CPU\n",
    "    for num_clusters in range(500,8001, 500):\n",
    "        cluster_file = f\"output/sklearn_{num_clusters}_clusters.pkl\"\n",
    "        if not os.path.exists(cluster_file):\n",
    "            compute_clusters_sklearn(standardised_clustering_data, cluster_file, num_clusters)\n",
    "        else:\n",
    "            print(f\"sklearn cluster file exists for {num_clusters} clusters\")\n",
    "else:    \n",
    "    \"\"\"\n",
    "    Currently not implemented, would have to provide instructions for kmcuda installation as well probably.\n",
    "    Only needed once inputs (nr of terms in combination with nr of clusters) get very  large.\n",
    "    \"\"\"\n",
    "    clustering_type = \"kmcuda\" \n",
    "    for num_clusters in range(4000,10001, 500):\n",
    "        cluster_file = f\"output/kmcuda_{num_clusters}_clusters.pkl\"\n",
    "        if not os.path.exists(cluster_file):\n",
    "            compute_clusters_kmcuda(standardised_clustering_data, cluster_file, num_clusters) \n",
    "        else:\n",
    "            print(f\"kmcuda cluster file exists for {num_clusters} clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc97b112",
   "metadata": {},
   "source": [
    "* Select the 'best' cluster model using Elbow and Silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9b6810b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_type = \"sklearn\"\n",
    "clustering_files = glob.glob(f'output/{clustering_type}_*.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a8d87f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing elbow and silhouette (if not too many num_clusters) scores.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|█████████████████████████████████████████                                                                                                                           | 4/16 [00:00<00:00, 1037.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading values from existing csv file: output/sklearn_4500_clusters.pkl\n",
      "Loading values from existing csv file: output/sklearn_5000_clusters.pkl\n",
      "Loading values from existing csv file: output/sklearn_3500_clusters.pkl\n",
      "Loading values from existing csv file: output/sklearn_2000_clusters.pkl\n",
      "Loading values from existing csv file: output/sklearn_500_clusters.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/lp/l_mzhpjs6bg95plfkl_n_vsc0000gn/T/ipykernel_13943/951031151.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mclustering_data_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"output/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcluster_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mElbowAndSilhouette\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclustering_data_fp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_scores_for_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclustering_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclustering_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/dev/irec/term_extraction/utils/cluster_utils.py\u001b[0m in \u001b[0;36mcompute_scores_for_models\u001b[0;34m(self, clustering_type, pkl_files)\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0;31m# already computed so we'll reuse the values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                 \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlatest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlatest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_clusters'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_clusters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum_of_squared_distances\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sum_of_squared_distances'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msilhouette_avg_euclidean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'silhouette_avg_euclidean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msilhouette_avg_cosine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'silhouette_avg_cosine'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# This takes a bit of time; 6-10 minutes per cluster size for 2000-6000 clusters \n",
    "# but resulting scores are saved in a csv and reused\n",
    "clustering_data_fp = \"output/\"\n",
    "es = cluster_utils.ElbowAndSilhouette(clustering_data_fp)\n",
    "es.compute_scores_for_models(clustering_type, clustering_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93f80fc",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Fifth, we pick a value for K. This value will be used to determine which clusters are filtered (because a background term occurs in it), as well as for prediction; e.g., predicting which cluster a new term falls into, which neighbours exist in that cluster, etc.\n",
    "</div>\n",
    "\n",
    "* Based on the number of clusters we'd like to use, we create a lookup-dictionary for the embedding and assigned cluster of each span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db65503",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_num_clusters = 7000\n",
    "cluster_model_to_use = f'output/{clustering_type}_{chosen_num_clusters}_clusters.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733bd1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids, assignments = pickle.load(open(cluster_model_to_use, 'rb'))\n",
    "\n",
    "unique_background_terms = [k for k in background_terms_c.keys()]\n",
    "cluster_dict_creator = cluster_utils.ClusterDict(unique_background_terms, \n",
    "                                                 unique_spans, \n",
    "                                                 standardised_clustering_data,  # important to use standardised  embeddings\n",
    "                                                 centroids, \n",
    "                                                 assignments,\n",
    "                                                 embedding_fp=\"output/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5692ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_cluster_dict, clusters_to_filter = cluster_dict_creator.prep_cluster_dict(chosen_num_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bbc5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print some insight in the clusters\n",
    "max_terms_to_show = 5\n",
    "for k in random.sample(phrase_cluster_dict.keys(), 10):\n",
    "    some_terms = [span for score, span in phrase_cluster_dict[k]]\n",
    "    if k in clusters_to_filter:\n",
    "        print(f\"Filtered: {some_terms[:max_terms_to_show]}\")\n",
    "    else:\n",
    "        print(f\"AEC domain: {some_terms[:max_terms_to_show]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5aa123c",
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_terms = [span for k, v in phrase_cluster_dict.items() for score, span in v if k in clusters_to_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824968e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_terms = [t for t in removed_terms if t in foreground_terms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97c6c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_terms = [span for k, v in phrase_cluster_dict.items() for score, span in v  if k not in clusters_to_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89aff4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{} terms were filtered ({:.2f}%)\".format(len(filtered_terms), len(filtered_terms)/len(filtered_terms+domain_terms)*100))\n",
    "print(\"Terms that were filtered:\")\n",
    "random.sample(filtered_terms,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24378f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{} terms were kept ({:.2f}%)\".format(len(domain_terms), len(domain_terms)/len(filtered_terms+domain_terms)*100))\n",
    "print(\"Terms that were kept:\")\n",
    "random.sample(domain_terms,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a2ecd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check the largest cluster(s)\n",
    "largest_cluster_size = np.max([len(v) for v in phrase_cluster_dict.values()])\n",
    "[v for v in phrase_cluster_dict.values() if len(v) == largest_cluster_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d79937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the smallest clusters\n",
    "print(len([v for v in phrase_cluster_dict.values() if len(v) == 1]))\n",
    "[v for v in phrase_cluster_dict.values() if len(v) == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d509ef77",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_sizes = [len(v) for v in phrase_cluster_dict.values()]\n",
    "print(f'Average number of terms per cluster: {sum(cluster_sizes)/len(cluster_sizes)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d0194b",
   "metadata": {},
   "source": [
    "### 4 Suggesting similar terms for Uniclass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e26129e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Here we predict (1) expansion candidates and (2) potential inflections (based on Levenshtein distance) for a given term. \n",
    "</div>\n",
    "\n",
    "\n",
    "* We run our algorithm on all Uniclass terms to get a feel for the output we generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a16c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusterAssignment:\n",
    "    def __init__(self, pkl_model, embedder, unique_spans, standardised_clustering_data, phrase_cluster_dict, clusters_to_filter):\n",
    "        \"\"\"\n",
    "        It's important to use the standardised clustering data here!\n",
    "        \"\"\"\n",
    "        self.centroids, self.assignments = pickle.load(open(pkl_model, 'rb'))\n",
    "        self.num_clusters = int(max(self.assignments) + 1)\n",
    "        \n",
    "        # set up sklearn for prediction\n",
    "        cpu_centroids = np.nan_to_num(self.centroids, copy=True, nan=0.0, posinf=None, neginf=None)\n",
    "        self.cpu_clusterer = KMeans(self.num_clusters, init=cpu_centroids, n_init=1, max_iter=1, tol=0)\n",
    "        print(\"[Clustering] Initialising sklearn kMeans with 1 iteration on KMcuda outputs. This takes some time!\")\n",
    "        self.cpu_clusterer.fit(standardised_clustering_data)\n",
    "        \n",
    "        self.embedder = embedder\n",
    "        self.unique_span_dict = dict(zip(unique_spans, standardised_clustering_data))\n",
    "        self.cluster_dict = phrase_cluster_dict\n",
    "        self.clusters_to_filter = clusters_to_filter\n",
    "    \n",
    "\n",
    "    def sklearn_assign(self, to_be_clustered: List[cluster_utils.ToBeClustered]) -> List[cluster_utils.ToBeClustered]:\n",
    "        \"\"\"\n",
    "        Using SciKit Learn's CPU implementation of KMeans to assign cluster IDs to a span or list of spans.\n",
    "\n",
    "        :param to_be_predicted: A `str` or `list` of strings for which a cluster ID will be computed.\n",
    "        :return :   List of cluster IDs.\n",
    "        \"\"\"\n",
    "        # Note: we try to avoid iteratively assigning clusters for each item in a list - don't think we parallelize more\n",
    "        embeddings = [s.embedding for s in to_be_clustered]\n",
    "        arrays = [np.ones([1, 768]).astype(np.float32) for _ in to_be_clustered]\n",
    "\n",
    "        for i, e in enumerate(embeddings):\n",
    "            arrays[i][0] = np.stack(e.squeeze())  # .astype(np.float16)\n",
    "\n",
    "        assignments = []\n",
    "        for arr in arrays:\n",
    "            assignments.append(self.cpu_clusterer.predict(arr))\n",
    "\n",
    "        for idx, assignment in enumerate(assignments):\n",
    "            to_be_clustered[idx].cluster_id = str(int(assignment))\n",
    "#             print(f\"assigned ID: {str(int(assignment))}\")\n",
    "            to_be_clustered[idx].distance_to_centroid = np.sum(np.absolute(embeddings[idx]-self.centroids[assignment]))\n",
    "            to_be_clustered[idx].all_neighbours = self.cluster_dict[str(int(assignment))]\n",
    "\n",
    "        return to_be_clustered\n",
    "    \n",
    "    def get_top_neighbours(self, \n",
    "                           text_inputs : Union[List[str], str], \n",
    "                           cosine_sim_threshold: float = 0.7, \n",
    "                           top_k: int = 3,\n",
    "                           return_non_aec=False):\n",
    "        if type(text_inputs) == str:\n",
    "            NON_AEC = False\n",
    "            tbc = cluster_utils.ToBeClustered(text_inputs, embedder)\n",
    "            [tbc] = self.sklearn_assign([tbc])\n",
    "            if tbc.cluster_id in self.clusters_to_filter:\n",
    "#                 print(f\"Potentially non-AEC domain: {tbc.text}\")\n",
    "                NON_AEC = True\n",
    "                \n",
    "            if not return_non_aec and NON_AEC:\n",
    "                return [], []\n",
    "            else:\n",
    "                neighbours, inflections = tbc.get_top_k_neighbours(self.unique_span_dict, cosine_sim_threshold, top_k)\n",
    "                return neighbours, inflections\n",
    "        else:\n",
    "            tbcs = [cluster_utils.ToBeClustered(t, embedder) for t in text_inputs]\n",
    "            tbcs = self.sklearn_assign(tbcs)\n",
    "            neighbours = []\n",
    "            inflections = []\n",
    "            for tbc in tbcs:\n",
    "                NON_AEC = False\n",
    "                if tbc.cluster_id in self.clusters_to_filter:\n",
    "#                     print(f\"Potentially non-AEC domain: {tbc.text}\")\n",
    "                    NON_AEC = True\n",
    "                if not return_non_aec and NON_AEC:\n",
    "                    neighbours.append([])\n",
    "                    inflections.append([])\n",
    "                else:\n",
    "                    n, i = tbc.get_top_k_neighbours(self.unique_span_dict, cosine_sim_threshold, top_k)\n",
    "                    neighbours.append(n)\n",
    "                    inflections.append(i)\n",
    "            return neighbours, inflections\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95dfe00",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_assigner = ClusterAssignment(cluster_model_to_use, embedder, unique_spans, standardised_clustering_data, phrase_cluster_dict, clusters_to_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e34281",
   "metadata": {},
   "source": [
    "* some examples of assigning clusters and identifying neighbours for spans:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30eab7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example str input\n",
    "example_string = \"party wall\"\n",
    "neighbours, inflections = cluster_assigner.get_top_neighbours(example_string, cosine_sim_threshold=.7)\n",
    "print(\"neighbours\", neighbours)\n",
    "print(\"inflections\", inflections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00101129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example list of str input\n",
    "example_strings = [\"a structural member\", \"control equipment\"]\n",
    "neighbours, inflections = cluster_assigner.get_top_neighbours(example_strings, \n",
    "                                                              cosine_sim_threshold=.7, \n",
    "                                                              return_non_aec=True)\n",
    "for idx, (neighbour_list, inflection_list) in enumerate(zip(neighbours, inflections)):\n",
    "    print(example_strings[idx])\n",
    "    print(\"neighbours\", neighbour_list)\n",
    "    print(\"inflections\", inflection_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5462312d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "We read Uniclass terms from a .ttl file that we have previoulsy prepared.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99a77b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_ttl_lines(text):\n",
    "    groups = []\n",
    "    current_group = []\n",
    "    for idx, line in enumerate(text.split(\"\\n\")):\n",
    "        if line == '':\n",
    "            if current_group:\n",
    "                groups.append(current_group)\n",
    "            current_group = []\n",
    "        else:\n",
    "            current_group.append(line)\n",
    "            if idx+1 == len(text.split(\"\\n\")):\n",
    "                groups.append(current_group)\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11397f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_uids_and_labels_with_definition(groups):\n",
    "    uid_dict = {}\n",
    "    for g in groups:\n",
    "        if any([line.startswith('  skos:prefLabel') for line in g]):\n",
    "            # only use group if a prefLabel exists\n",
    "            pref_label = ''\n",
    "            alt_labels = []\n",
    "            definition = ''\n",
    "            for line in g:\n",
    "                if line.startswith('  skos:prefLabel'):\n",
    "                    pref_label = line.split('\"')[1]\n",
    "                elif line.startswith('  skos:altLabel'):\n",
    "                    labels = line.split('\"')[1::2]\n",
    "                    alt_labels += labels\n",
    "                elif line.startswith('  skos:definition'):\n",
    "                    definition = line.split('\"')[1]\n",
    "\n",
    "            if pref_label:\n",
    "                uid = g[0].split()[0].split(\":\")[1]\n",
    "                uid_dict[uid] = {'pref_label': pref_label, \n",
    "                                 'alt_labels': alt_labels,\n",
    "                                 'definition': definition\n",
    "                                }\n",
    "    return uid_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cd0009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_nodes(vocab_name):\n",
    "    processed_file = f\"{vocab_name}.json\"\n",
    "    \n",
    "    # check if file as processed before \n",
    "    NODES_LOADED = False\n",
    "    if os.path.exists(processed_file):\n",
    "        with open(processed_file) as f:\n",
    "            graph_dict = json.load(f)\n",
    "            \n",
    "        NODES_LOADED = True\n",
    "        print(f\"Loaded nodes and neighbours for: {vocab_name}\") \n",
    "    else: \n",
    "        print(f\"Will have to grab nodes for: {vocab_name}\")\n",
    "    \n",
    "        # compute the neighbours for each node\n",
    "        graph_dict = {}\n",
    "        \n",
    "        print(f\"Working on file: {vocab_name}\")\n",
    "        with open(vocab_name, 'r') as f:\n",
    "            text =  f.read()\n",
    "            \n",
    "        groups = group_ttl_lines(text)\n",
    "        print(\"Collecting nodes with definitions from dict\")\n",
    "        graph_dict = grab_uids_and_labels_with_definition(groups)\n",
    "        \n",
    "        # save the dictionary somewhere for reloading\n",
    "        with open(processed_file, 'w') as f:\n",
    "            json.dump(graph_dict, f)\n",
    "    return graph_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d27709",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniclass_dict = grab_nodes(\"data/input/uniclass_2015.ttl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5201d194",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(uniclass_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c361f4",
   "metadata": {},
   "source": [
    "* Feed the Uniclass labels to our predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bc0e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "expansion_candidates =  {}\n",
    "for k, v in tqdm(uniclass_dict.items()):\n",
    "    uniclass_label = v['pref_label']\n",
    "    if uniclass_label:\n",
    "        expansion_terms, inflections = cluster_assigner.get_top_neighbours(uniclass_label, \n",
    "                                                                           cosine_sim_threshold=.7,\n",
    "                                                                           top_k=5,\n",
    "                                                                           return_non_aec=False)\n",
    "        expansion_candidates[uniclass_label] = {\"expansions\": expansion_terms,\n",
    "                                                \"inflections\": inflections}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b886314a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_expansion_candidates(expansion_candidates: Dict[str, str]):\n",
    "    number_of_terms_with_expansion_candidates = 0\n",
    "    for k, v in expansion_candidates.items():\n",
    "        if v['expansions'] or v['inflections']:\n",
    "            print(f\"==========================\\nTerm: {k}\")\n",
    "            print(f\"Related: {v['expansions']}\")\n",
    "            print(f\"Inflections: {v['inflections']}\")\n",
    "            number_of_terms_with_expansion_candidates += 1\n",
    "\n",
    "    print(\"\\n\\n\\nAmount of terms with candidates:\\n{} ({:.2f}%)\".format(\n",
    "        number_of_terms_with_expansion_candidates,\n",
    "        number_of_terms_with_expansion_candidates/len(expansion_candidates)*100))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5229a6f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_expansion_candidates(expansion_candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03eea2fd",
   "metadata": {},
   "source": [
    "* Our own list of terms of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba14f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_of_interest = ['Timber frame floors and roofs',  'EPS',  'Water services',  'Subsystems',  'External thermal insulation composite system with rendering ',  'Smoke and fire-stopping',  'Timber baseboards',  'PV panels',  'Ceramic slates',  'Vertical smoke strip curtains',  'Hardwood deck boards',  'Roof type',  'Flat roof ',  'Aluminium external panels',  'Material',  'Ethylene propylene (EP) damp-proof courses and cavity trays',  'Vertical smoke curtains',  'Light-gauge steel floors and roofs',  'Fibre-cement',  'Fibre-reinforced cement boards',  'Services',  'Fire alarm systems',  'HVAC equipment',  'Covering',  'Mineral wool insulation',  'Components',  'Roof structure - flat',  'CLT',  'Aluminium cassette panels',  'Fibre-reinforced cement sheets',  'Weathering steel cassette panels',  'Clay roofing tiles',  'Batteries',  'Natural stone',  'Bitumen-based damp-proof courses and cavity trays',  'Board suspended ceiling systems',  'Green roofs/gardens',  'Maintenance access',  'Softwood baseboards',  'Polyurethane (PUR) foam boards',  'Sheet panels',  'Glulam',  'Flexible sheet for waterproofing',  'Panelled and framed modular systems',  'Vertical active fire curtain barriers',  'Gypsum core boards',  'Barriers',  'Light steel roof framing systems',  'Natural stone panels',  'Timber sheet panels',  'Mineral wool wire-reinforced mattress cavity barriers',  'Structural frame',  'Aluminium internal panels',  'Roof diaphragm',  'Structural connectors',  'Timber',  'Roof windows',  'Reinforced concrete',  'Ceramic tiles',  'LWSF - Light weight steel-frame',  'Panel of steel-wires with incorporated thermal insulation',  'Water tightness',  'Structural',  'Structural insulated panel systems',  'Cold-applied roofing membrane adhesive damp-proof course joint sealers',  'Mineral fibre slab insulation',  'Cross-laminated timber (CLT) panelled modules',  'Membranes',  'Ceiling and soffit systems',  'Green roof',  'Skylights',  'Extruded polyethylene (PE) foam boards',  'Plain tile roofing systems',  'Wood-based panels',  'Blue roof',  'Insulated damp-proof courses',  'Interfacial',  'Light-gauge steel frame panels',  'Aluminium structures',  'Timber board panels',  'Bitumen membrane shingles',  'Mastic asphalt (MA) damp-proof courses',  'Deck frame',  'ICT',  'Gypsum baseboards',  'Flexible stone wool mat insulation',  'Pitched roof ',  'Fire insulating caps',  'Active smoke barriers',  'Fibre cement slate roofing systems',  'Steel structures',  'Photovoltaic devices',  'Structural insulated panel (SIP) modules',  'Mineral wool slab insulation',  'XPS',  'Sound proofing',  'Ceiling cassettes',  'Highlighted relevant item',  'Hardboards',  'Intumescent sleeved mineral wool cavity fire barriers',  'Thermal insulation',  'Solar photovoltaic modules',  'Wood structures',  'Aluminium-faced aluminium core panels',  'Steel deck',  'P-DfMA Standards Database',  'Rain drainage',  'Polymeric damp-proof courses',  'Active Roof',  'Flexible intumescent gap seals',  'Bonded sheets',  'Underlays for discontinuous roofing',  'Bitumen sheets',  'Prefabricated framed and panelled structures',  'Panels',  'Composite',  'Active roof',  'Sealants',  'Building elements',  'Hardwood baseboards',  'Light gauge steel frame',  'Mineral wool flexible insulation',  'Energy storage',  'Air tightness',  'Passive roof',  'Ceramic panels',  'Vapour control layer',  'Mineral wool',  'Energy generation and storage',  'Softwood deck boards',  'Ceramic fibre fire-stopping',  'Intumescent linear gap seals',  'Gypsum plasterboards',  'Oriented strand boards',  'Non-loadbearing',  'Wood-based boards',  'Roof-mounted photovoltaic',  'Tapered insulation',  'Solar thermal systems',  'Carbon steel insulating sandwich panels',  'Fire detection',  'LVL',  'Fibre-cement slates',  'Insulation',  'Cold roof ',  'Self-supporting double skin metal faced insulating panels',  'Composite panels',  'Carbon steel framed vertical bar and rail panels',  'Mineral wool fire-stopping',  'Warm roof ',  'Damp-proof courses and cavity trays',  'Flexible plasterboards',  'Superstructure',  'Prefabricated building units ',  'Aluminium composite material (ACM) panels',  'Natural slates',  'Fire-fighting systems',  'Mechanical services',  'Bitumen membrane shingle roofing systems',  'Wood laminate strips and boards',  'Ceramic',  'PIR',  'Plastics membranes',  'Extruded polystyrene (XPS) boards',  'Composite lightweight panels',  'Bitumen membrane',  'Rainwater harvesting',  'Coated woven glass fibre cloth flexible cavity barriers',  'Wood fibre boards',  'Polyisocyanurate (PIR) foam boards',  'Timber lining boards',  'Pre-fabricated wood-based loadbearing stressed skin panels',  'External wall',  'Fully bonded pre applied flexible sheet for water proofing ',  'Plywood desk',  'Load bearing',  'Concrete roofing tiles',  'Expanded polystyrene (EPS) boards',  'Timber structures']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4791df29",
   "metadata": {},
   "outputs": [],
   "source": [
    "term_expansion_candidates =  {}\n",
    "for term in tqdm(terms_of_interest):\n",
    "#     term = term if term.isupper() else term.lower() # remove capitalisation if not an abbreviation\n",
    "    expansion_terms, inflections = cluster_assigner.get_top_neighbours(term, \n",
    "                                                                       cosine_sim_threshold=.7,\n",
    "                                                                       top_k=5,\n",
    "                                                                       return_non_aec=False)\n",
    "    term_expansion_candidates[term] = {\"expansions\": expansion_terms,\n",
    "                                       \"inflections\": inflections}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f522767d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_expansion_candidates(term_expansion_candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2af851c",
   "metadata": {},
   "source": [
    "* Identify candidate mappings based on overlap in expansion candidates and inflections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d544fb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for k1, v1 in term_expansion_candidates.items():\n",
    "    potential_uniclass_matches = []\n",
    "    for k2, v2 in expansion_candidates.items(): \n",
    "        if any([candidate for candidate in v1['expansions'] + v1['inflections'] if (candidate in v2['expansions'] or candidate in v2['inflections'])]):\n",
    "            potential_uniclass_matches.append(k2)\n",
    "    if potential_uniclass_matches:\n",
    "        print(f\"Match?===============\\nBRE: {k1}\\nUniclass: {potential_uniclass_matches}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77238450",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
