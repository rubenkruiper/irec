{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00aa64c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import pickle\n",
    "import glob, os\n",
    "import subprocess\n",
    "import json, random\n",
    "import requests, urllib\n",
    "import concurrent.futures\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from tqdm import tqdm\n",
    "from typing import List, Any, List, Dict, Tuple\n",
    "from pathlib import Path\n",
    "from textblob import TextBlob\n",
    "from threading import current_thread\n",
    "from itertools import combinations, combinations_with_replacement\n",
    "from collections import Counter\n",
    "from nltk.corpus import wordnet as wn\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "from rdflib import URIRef, BNode, Literal, Namespace, Graph\n",
    "from rdflib.namespace import XSD, RDF, RDFS, SKOS, NamespaceManager\n",
    "\n",
    "from utils.spar_utils import TermExtractor\n",
    "from utils.cluster_utils import levenshtein\n",
    "from utils.embedding_utils import Embedder\n",
    "from utils.cleaning_utils import custom_cleaning_rules, remove_unicode_chars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11663171",
   "metadata": {},
   "source": [
    "We would like to express the following features/relations:\n",
    "* Dictionary definition terms, which are always concepts\n",
    "  * We'll use the source as namespace, and corresponding concept identifier if it exists\n",
    "  * SKOS is used to establish a mapping (e.g., skos:exactMatch) and add the definition (skos:definition)\n",
    "* Special properties that we want to capture between words, which may help identify concepts:\n",
    "  * Word is part of MWE\n",
    "  * Morphologically similar words; stemming & Levenshtein distance\n",
    "  * Semantically similar words; distributed similarity (NNs)\n",
    "  * Acronyms\n",
    "  * Related, this is a generic relation, e.g., a `ampere` is related to `electric current`\n",
    "  * Domain-specificity; foreground or background term following our filtering procedure\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7038359e",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_output_fp = Path.cwd().joinpath(\"data\", \"graph_output\")\n",
    "graph_output_fp.mkdir(parents=True, exist_ok=True) # create directory if it doesn't exist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979b0616",
   "metadata": {},
   "source": [
    "### Prepare namespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2b60e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = Namespace(\"https://example.org/top_concept_for_visulisation/#\")\n",
    "WIKI = Namespace(\"https://www.wikidata.org/entity/#\")\n",
    "# Note: that UNICLASS is not a namespace (yet) only has identifiers \n",
    "UNICLASS = Namespace(\"https://www.example.org/uniclass/#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "098e92aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "IREC_ontology_URL = \"https://example.org/irec-schema#\"\n",
    "IREC_spans_URL = \"https://example.org/irec-spans#\"\n",
    "IREC_concepts_URL = \"https://example.org/irec-concepts#\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2e6b31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our custom namespace for the schema to store spans\n",
    "IREC = Namespace(IREC_ontology_URL)\n",
    "\n",
    "# create a custom namespace to store spans and concepts\n",
    "SPANS = Namespace(IREC_spans_URL)\n",
    "CONCEPTS = Namespace(IREC_concepts_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcef6e56",
   "metadata": {},
   "source": [
    "### graph creation utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2e8ce74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UID_assigner:\n",
    "    def __init__(self):\n",
    "        self.UIDs = {}\n",
    "        self.UID = 0\n",
    "        self.scheme_uids = {}\n",
    "        \n",
    "    def get_scheme_UID(self, namespace: Namespace):\n",
    "        \"\"\"\n",
    "        Determines which type of UID to assign, based on the namespace.\n",
    "        \"\"\"\n",
    "        return [x for x in self.UIDs[namespace._.defrag().__reduce__()[1][0]].values() if x == \"schemeUID\"][0]\n",
    "        \n",
    "    def assign_UID(self, text, namespace: Namespace):\n",
    "        \"\"\"\n",
    "        Determines which type of UID to assign, based on the namespace.\n",
    "        \"\"\"\n",
    "        if namespace == SPANS:\n",
    "            return self.span_UID(text)\n",
    "        elif namespace == CONCEPTS:\n",
    "            return self.concept_UID(text)\n",
    "        else:\n",
    "            print(\"UID assignment not set up for this namespace, maybe use UID_assigner.keep_track_of_existing_UID()\")\n",
    "            \n",
    "    \n",
    "    def span_UID(self, text):\n",
    "        \"\"\"\n",
    "        NOTE: each text span is a unique identifier in and of itself. We'll simply convert the text span to \n",
    "        a URL friendly representation.\n",
    "        \"\"\"\n",
    "        n_space = SPANS.placeholder.defrag().__reduce__()[1][0]\n",
    "        if n_space not in self.UIDs:\n",
    "            self.UIDs[n_space] = {}\n",
    "        \n",
    "        urltext = urllib.parse.quote(text)\n",
    "        if text not in self.UIDs[n_space]:\n",
    "            self.UIDs[n_space][text] = urltext\n",
    "            \n",
    "        return self.UIDs[n_space][text]\n",
    "        \n",
    "    def concept_UID(self, text):\n",
    "        \"\"\"\n",
    "        For now I'll create my own dumb interger-based UIDs for nodes as a simple shortcut, split per namespace\n",
    "        \"\"\"\n",
    "        n_space = CONCEPTS.placeholder.defrag().__reduce__()[1][0]\n",
    "        \n",
    "        if n_space not in self.UIDs:\n",
    "            self.UIDs[n_space] = {}\n",
    "        \n",
    "        if text not in self.UIDs[n_space]:\n",
    "            self.UID += 1\n",
    "            self.UIDs[n_space][text] = str(self.UID)\n",
    "        return self.UIDs[n_space][text]\n",
    "        \n",
    "    def keep_track_of_existing_UID(self, text:str, existing_uid: str, namespace:Namespace):\n",
    "        \"\"\"\n",
    "        Simply keep track of UIDs that exist in the provided namespace.\n",
    "        \"\"\"\n",
    "        n_space = namespace.placeholder.defrag().__reduce__()[1][0]\n",
    "        if n_space not in self.UIDs:\n",
    "            self.UIDs[n_space] = {}\n",
    "            \n",
    "        if text not in self.UIDs[n_space]:\n",
    "            # already seen by this UID assigner\n",
    "            self.UIDs[n_space][text] = existing_uid\n",
    "            \n",
    "        return existing_uid\n",
    "    \n",
    "    def retrieve_uid_by_text(self, node_text, namespace: Namespace = SPANS):\n",
    "        n_space = namespace.placeholder.defrag().__reduce__()[1][0]\n",
    "        if node_text in self.UIDs[n_space]:\n",
    "            return self.UIDs[n_space][node_text]\n",
    "        else:\n",
    "            return None \n",
    "        \n",
    "    def count_nodes_in_namespace(self, namespace: Namespace = SPANS):\n",
    "        n_space = namespace.placeholder.defrag().__reduce__()[1][0]\n",
    "        print(f\"Number of nodes in '{n_space}': {len(self.UIDs[n_space])}\")\n",
    "        return len(self.UIDs[n_space])\n",
    "        \n",
    "    def print_node_by_id(self, graph, node_id, namespace: Namespace = SPANS):\n",
    "        for s, p, o in graph.triples((namespace[str(node_id)],  None, None)):\n",
    "            print(f\"{s.split('#')[-1]} ; {p.split('#')[-1]} ; {o.split('#')[-1]}\")\n",
    "        \n",
    "    def print_node_by_text(self, graph, node_text, namespace: Namespace = SPANS):\n",
    "        n_space = namespace.placeholder.defrag().__reduce__()[1][0]\n",
    "        node_id = self.UIDs[n_space][node_text]\n",
    "        # find all triples with subject\n",
    "        for s, p, o in graph.triples((namespace[node_id],  None, None)):\n",
    "            print(f\"{s.split('#')[-1]} ; {p.split('#')[-1]} ; {o.split('#')[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "772b6692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These wrappers only exist to help me quickly and consistently add nodes to the graph\n",
    "\n",
    "# SKOS \n",
    "def skos_scheme(node_uid, namespace: Namespace=CONCEPTS) -> List[Tuple]:\n",
    "    \"\"\" Node that identifies the concep scheme with a URI, expecting/using as scheme root \"\"\"\n",
    "    return [(namespace[node_uid], RDF.type, SKOS.ConceptScheme)]\n",
    "\n",
    "def skos_top_concept(node_uid, top_concept_uid, \n",
    "                    namespace: Namespace=CONCEPTS, top_concept_namespace: Namespace=CONCEPTS) -> List[Tuple]:\n",
    "    \"\"\" Currently, we mainly use the top-concept for visualisation. \"\"\"\n",
    "    return [(namespace[node_uid], SKOS.hasTopConcept, top_concept_namespace[top_concept_uid]),\n",
    "            (top_concept_namespace[top_concept_uid], SKOS.topConceptOf, namespace[node_uid])]\n",
    "\n",
    "def skos_in_scheme(node_uid, scheme_uid, namespace: Namespace=CONCEPTS, scheme_namespace: Namespace=CONCEPTS) -> List[Tuple]:\n",
    "    \"\"\" Keep track of the scheme/source of a node. \"\"\"\n",
    "    return [(namespace[node_uid], SKOS.inScheme, scheme_namespace[scheme_uid])]\n",
    "\n",
    "def skos_node(node_uid, text, namespace: Namespace=CONCEPTS) -> List[Tuple]:\n",
    "    \"\"\" Add a concept with prefLabel to the graph in the CONCEPTS namespace, of type SKOS.Concept \"\"\"\n",
    "    return [(namespace[node_uid], RDF.type, SKOS.Concept), \n",
    "            (namespace[node_uid], SKOS.prefLabel, Literal(text, lang='en'))]\n",
    "\n",
    "def skos_prefLabel(node_uid, text, namespace: Namespace=CONCEPTS) -> List[Tuple]:\n",
    "    \"\"\" Add the text label for a node \"\"\"\n",
    "    return [(namespace[node_uid], SKOS.prefLabel, Literal(text, lang='en'))]\n",
    "\n",
    "def skos_altLabel(node_uid, alt_label_uid, namespace: Namespace=CONCEPTS)-> List[Tuple]:\n",
    "    \"\"\" Add an alternative text label for a concept node \"\"\"\n",
    "    return [(namespace[node_uid], SKOS.altLabel, namespace[alt_label_uid]), \n",
    "            (namespace[alt_label_uid], SKOS.altLabel, namespace[node_uid])]\n",
    "\n",
    "def skos_exact_match(subject_node_uid, object_node_uid,\n",
    "                subject_namespace: Namespace=SPANS, object_namespace: Namespace=CONCEPTS) -> List[Tuple]:\n",
    "    \"\"\" Denotes an exact match between two nodes, would expect the nodes to be in different vocabularies \"\"\"\n",
    "    return [(subject_namespace[subject_node_uid], SKOS.exactMatch, object_namespace[object_node_uid])]\n",
    "\n",
    "def skos_related(subject_node_uid, object_node_uid,\n",
    "                subject_namespace: Namespace=SPANS, object_namespace: Namespace=CONCEPTS) -> List[Tuple]:\n",
    "    \"\"\" Denotes a relation between two nodes, would expect the nodes to be in different vocabularies \"\"\"\n",
    "    return [(subject_namespace[subject_node_uid], SKOS.related, object_namespace[object_node_uid])]\n",
    "    \n",
    "def skos_broader(narrower_node_uid, broader_node_uid, \n",
    "                 narrower_namespace: Namespace=CONCEPTS, broader_namespace: Namespace=CONCEPTS) -> List[Tuple]:\n",
    "    \"\"\" Assuming narrower/broader is always reflexive, would expect the nodes to be in different vocabularies \"\"\"\n",
    "    return [(broader_namespace[narrower_node_uid], SKOS.narrower, narrower_namespace[broader_node_uid]),\n",
    "            (narrower_namespace[broader_node_uid], SKOS.broader, broader_namespace[narrower_node_uid])]\n",
    "    \n",
    "def skos_definition(node_uid, definition_text, namespace: Namespace=CONCEPTS) -> List[Tuple]:\n",
    "    \"\"\" The namespace indidcates the source of the definition? \"\"\"\n",
    "    return [(namespace[node_uid], SKOS.definition, Literal(definition_text, lang='en'))]\n",
    "\n",
    "def skos_note(node_uid, note_text, namespace: Namespace=CONCEPTS) -> List[Tuple]:\n",
    "    \"\"\" Some notes exist in the approved docs at least, containing useful information \"\"\"\n",
    "    return [(namespace[node_uid], SKOS.note, Literal(note_text, lang='en'))]\n",
    "\n",
    "\n",
    "\n",
    "# IREC functions and REFERENCE\n",
    "IREC.Span # A span is a sequence of characters that occurs verbatim in a text, either contiguous or discontiguos as extracted by SPaR.txt (Kruiper et al., 2021).   \n",
    "IREC.constitutes  # Indicates that a span constitutes another span, e.g., the Multi-Word Expression (MWE) Span `hot water storage system` the Span `storage`.\n",
    "IREC.isMorphologicallySimilarTo # Indicates that a Span is morphologically similar to another Span, e.g., they may have the same stem or a small Levenshtein distance.\n",
    "IREC.isSemanticallySimilarTo # Indicates that a Span is semantically similar to another Span, following a cosine similarity between their  embeddings.\n",
    "IREC.related # General way to indicate some relation between two spans, e.g., `ampere` is related to `electric current`\n",
    "IREC.hasAcronym # A Span can have an acronym, e.g., `British Standards Institute` has the acronym `BSI`.\n",
    "IREC.isAcronymOf # A Span can have an acronym, e.g., `BSI` is the acronym for `British Standards Institute`.\n",
    "IREC.hasAntonym # Property that relates a Span to another Span, each being each other's antonyms.\n",
    "\n",
    "def irec_span(node_uid, text, namespace: Namespace=SPANS) -> List[Tuple]:\n",
    "    \"\"\" Add a span node in the SPANS namespace, of type IREC.Span and the span text set as its RDF.label \"\"\"\n",
    "    # is preflabel a property? I would assume so\n",
    "    return [(namespace[node_uid], RDF.type, IREC.Span), \n",
    "            (namespace[node_uid], RDFS.label,  Literal(text, lang='en'))]\n",
    "\n",
    "def irec_constitutes(subject_node_uid, object_node_uid,\n",
    "                     subject_namespace: Namespace=SPANS, object_namespace: Namespace=SPANS) -> List[Tuple]:\n",
    "    \"\"\" Indicates that somewhere in the label of the first SPAN node, you can find the second span's label \"\"\"\n",
    "    return [(subject_namespace[subject_node_uid], IREC.constitutes, object_namespace[object_node_uid])]\n",
    "\n",
    "def irec_morp_sim(subject_node_uid, object_node_uid,\n",
    "                  subject_namespace: Namespace=SPANS, object_namespace: Namespace=SPANS) -> List[Tuple]:\n",
    "    \"\"\" Indicates that the labels of two SPAN nodes are morphologically similar \"\"\"\n",
    "    return [(subject_namespace[subject_node_uid], IREC.isMorphologicallySimilarTo, object_namespace[object_node_uid])]\n",
    "\n",
    "def irec_sem_sim(subject_node_uid, object_node_uid,\n",
    "                 subject_namespace: Namespace=SPANS, object_namespace: Namespace=SPANS) -> List[Tuple]:\n",
    "    \"\"\" Indicates that the labels of two SPAN nodes are semantically similar, following the distributed semantics hypothesis \"\"\"\n",
    "    return [(subject_namespace[subject_node_uid], IREC.isSemanticallySimilarTo, object_namespace[object_node_uid])]\n",
    "\n",
    "def irec_related(subject_node_uid, object_node_uid,\n",
    "                 subject_namespace: Namespace=SPANS, object_namespace: Namespace=SPANS) -> List[Tuple]:\n",
    "    \"\"\" Indicates that a Span is related in SOME way to another Span. \"\"\"\n",
    "    return [(subject_namespace[subject_node_uid], IREC.related, object_namespace[object_node_uid])]\n",
    "\n",
    "def irec_has_acronym(subject_node_uid, object_node_uid,\n",
    "                     subject_namespace: Namespace=SPANS, object_namespace: Namespace=SPANS) -> List[Tuple]:\n",
    "    \"\"\" Indicates that the label of the subject node has an acronym, ergo the label of the object node  \"\"\"\n",
    "    return [(subject_namespace[subject_node_uid], IREC.hasAcronym, object_namespace[object_node_uid]),\n",
    "            (object_namespace[object_node_uid], IREC.isAcronymOf, subject_namespace[subject_node_uid])]\n",
    "\n",
    "def irec_antonym(subject_node_uid, object_node_uid,\n",
    "                 subject_namespace: Namespace=SPANS, object_namespace: Namespace=SPANS) -> List[Tuple]:\n",
    "    \"\"\" Indicates that the label of the subject node is an antonym of the label of the object node  \"\"\"\n",
    "    return [(subject_namespace[subject_node_uid], IREC.hasAntonym, object_namespace[object_node_uid])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08060b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_tuples(graph, tuples):\n",
    "    \"\"\"\n",
    "    We'll never add the same tuple twice to a graph\n",
    "    \"\"\"\n",
    "    for t in tuples:\n",
    "        assert len(t) == 3\n",
    "    [graph.add(t) for t in tuples if t not in graph]\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fdc557",
   "metadata": {},
   "source": [
    "### Prepare namespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54a1e1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "irec_graph = Graph()\n",
    "\n",
    "irec_graph.bind(\"root\", ROOT)\n",
    "irec_graph.bind(\"wikipedia\", WIKI)\n",
    "irec_graph.bind(\"uniclass\", UNICLASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56acbeb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Graph identifier=N0431cf65e56e46f1ab18b0e6ee40943a (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bind our vocabulary of classes/relations\n",
    "graph_data_fp = Path.cwd().joinpath(\"data\", \"graph_data\")\n",
    "irec_graph.parse(graph_data_fp.joinpath(\"IREC.rdf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cce047cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_scheme_uid(graph: Graph, scheme_name: str, scheme_uid_label:str, namespace: Namespace) -> Graph:\n",
    "    top_concept_uid = 'ROOT'\n",
    "    graph = add_tuples(graph, skos_prefLabel(top_concept_uid, \"IREC Graph root node\", namespace = ROOT))\n",
    "    \n",
    "    scheme_uid = ua.keep_track_of_existing_UID(scheme_name, scheme_uid_label, namespace)\n",
    "    \n",
    "    graph = add_tuples(graph, skos_scheme(scheme_uid, SPANS))\n",
    "    graph = add_tuples(graph, skos_in_scheme(scheme_uid, scheme_uid, SPANS, SPANS)) # self-reference\n",
    "    graph = add_tuples(graph, skos_top_concept(scheme_uid, top_concept_uid, SPANS, ROOT))\n",
    "    return graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cbb5024e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ua = UID_assigner()\n",
    "\n",
    "# global UIDs for the schemes we'll be using\n",
    "top_concept_uid = 'ROOT'\n",
    "irec_graph = add_scheme_uid(irec_graph, \"IREC spans\", \"schemeUID\", SPANS)\n",
    "irec_graph = add_scheme_uid(irec_graph, \"IREC concepts\", \"schemeUID\", CONCEPTS)\n",
    "irec_graph = add_scheme_uid(irec_graph, \"IREC WikiData concepts\", \"schemeUID\", WIKI) # NON EXISTENT NODE\n",
    "irec_graph = add_scheme_uid(irec_graph, \"IREC Uniclass concepts\", \"schemeUID\", UNICLASS)  # NON EXISTENT NODE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fceef5",
   "metadata": {},
   "source": [
    "### Add base antonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7fe6de0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cold']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We want to capture antonyms: dichotomy in meaning of words, \n",
    "# For this we'll use NLTK's version of WordNet, which mainly captures antonyms for adjectives and adverbs.\n",
    "wordnet_antonyms = {}\n",
    "for i in wn.all_synsets():\n",
    "    if i.pos() in ['a', 's']:    # If synset is adj or satelite-adj.\n",
    "        for j in i.lemmas():     # Iterating through lemmas for each synset.\n",
    "            if j.antonyms():     # If adj has antonym.\n",
    "                wordnet_antonyms[str(j.name()).strip()] = [x.name() for x in j.antonyms()]\n",
    "\n",
    "# Example of a useful antonym for us\n",
    "wordnet_antonyms['hot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de3d0ae4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hot']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet_antonyms['cold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "afd035fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for span in wordnet_antonyms.keys():\n",
    "    span_uid = ua.assign_UID(span, SPANS)\n",
    "    \n",
    "    tuples =  irec_span(span_uid, span)\n",
    "    irec_graph = add_tuples(irec_graph, tuples)\n",
    "\n",
    "    antonyms = wordnet_antonyms[span]\n",
    "    for antonym in antonyms:\n",
    "        antonym_uid = ua.assign_UID(antonym, SPANS)\n",
    "\n",
    "        irec_graph = add_tuples(irec_graph, irec_span(antonym_uid, antonym))\n",
    "        irec_graph = add_tuples(irec_graph, skos_in_scheme(antonym_uid, 'schemeUID', SPANS, SPANS))\n",
    "\n",
    "        # add the antonym relation\n",
    "        irec_graph = add_tuples(irec_graph, irec_antonym(span_uid, antonym_uid))\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41f63e5",
   "metadata": {},
   "source": [
    "### Add domain terms extracted from the Approved documents as Spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8178ad1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_terms = pickle.load(open(graph_data_fp.joinpath('domain_terms.pkl'), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07f4b0f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['expansion water',\n",
       " 'the hot water system',\n",
       " 'the hot water',\n",
       " 'a hot water system',\n",
       " 'the hot tap',\n",
       " 'the water supply',\n",
       " 'PLY',\n",
       " 'Water Fittings',\n",
       " 'Fittings',\n",
       " 'the storage vessel',\n",
       " 'the Gas Safety Installation',\n",
       " 'the Gas Safety Installation Use',\n",
       " 'Gas Safety Installation',\n",
       " 'Regulations 1996',\n",
       " 'Regulations 1992',\n",
       " 'Regulations 1994',\n",
       " 'ductwork',\n",
       " 'ductwork serving',\n",
       " 'pipework',\n",
       " 'Electrical safety Dwellings',\n",
       " 'industrial processes',\n",
       " 'the stored water',\n",
       " 'a sanitary conveniences',\n",
       " 'sanitary conveniences',\n",
       " 'sanitary fittings',\n",
       " 'a sanitary convenience',\n",
       " 'a temperature relief valve',\n",
       " 'cistern lids',\n",
       " 'cylinders',\n",
       " 'ignition',\n",
       " 'cylinder',\n",
       " 'steam',\n",
       " 'engine',\n",
       " 'thermoplastic material',\n",
       " 'thermoplastics',\n",
       " 'thermoplastic product',\n",
       " 'thermoplastic',\n",
       " 'thermoplastic core',\n",
       " 'thermoplastic materials a )',\n",
       " 'thermoplastic panels',\n",
       " 'thermoplastic substrate',\n",
       " 'cistern',\n",
       " 'cisterns',\n",
       " 'the cistern',\n",
       " 'washing facilities',\n",
       " 'shower facilities',\n",
       " 'changing facilities',\n",
       " 'internal facilities',\n",
       " 'turning facilities',\n",
       " 'hand washing facilities',\n",
       " 'Water supply',\n",
       " 'the Water Supply',\n",
       " 'water heater',\n",
       " 'water heaters',\n",
       " 'storage water heaters',\n",
       " 'a space heater',\n",
       " 'BS 853 - 1 : 1996 Specification',\n",
       " 'Unvented',\n",
       " 'Calorifiers',\n",
       " 'a hot water storage system',\n",
       " 'hot water storage system',\n",
       " 'hot water storage systems',\n",
       " 'a solar hot water system',\n",
       " 'Open vented copper cylinders',\n",
       " 'ecification',\n",
       " 'pressure relief valve',\n",
       " 'heat source',\n",
       " 'heat sources',\n",
       " 'heat input',\n",
       " 'heat supplied',\n",
       " 'PLY AND',\n",
       " 'the operating pressure',\n",
       " 'a boiler',\n",
       " 'the boiler',\n",
       " 'curtilages',\n",
       " 'curtilage',\n",
       " 'WARNING',\n",
       " 'the pipe floor',\n",
       " 'the source rooms',\n",
       " 'the supply air',\n",
       " 'the water main',\n",
       " 'the source room measurements',\n",
       " 'the discharge pipe',\n",
       " 'the storage layer',\n",
       " 'The ventilation areas',\n",
       " 'the independent ceiling',\n",
       " 'the pipework',\n",
       " 'the heat panel form',\n",
       " 'the fan unit',\n",
       " 'the distribution pipes',\n",
       " 'the worktop',\n",
       " 'litres per',\n",
       " 'litres',\n",
       " 'litres /',\n",
       " 'litres capacity',\n",
       " 'paragraph 3 15',\n",
       " 'paragraph 3 13',\n",
       " 'paragraph 3 22',\n",
       " 'paragraph 3 25',\n",
       " 'paragraph 3 35',\n",
       " 'paragraph 3 21',\n",
       " 'paragraph 3 10',\n",
       " 'paragraph 3 30',\n",
       " 'paragraph 3 5',\n",
       " 'paragraph 3 60',\n",
       " 'paragraph 3 32',\n",
       " 'paragraph 3 68',\n",
       " 'paragraph 4 14',\n",
       " 'an energy cut - out',\n",
       " 'energy cut - out',\n",
       " 'BS EN 1155 Building hardware',\n",
       " 'BS EN 1125 Building hardware',\n",
       " 'BS EN 1490 : 2000 Building valves',\n",
       " 'thermostats',\n",
       " 'The BCB',\n",
       " 'the BCB',\n",
       " 'Discharge',\n",
       " 'a manifold',\n",
       " 'fixed immersion heaters',\n",
       " 'tundish',\n",
       " 'storage system',\n",
       " 'transfer space',\n",
       " 'sound absorption',\n",
       " 'bin storage',\n",
       " 'the tundish',\n",
       " 'air gap',\n",
       " 'air gaps',\n",
       " 'Factory made systems',\n",
       " 'solar water heating systems',\n",
       " 'bends',\n",
       " 'combustion installations',\n",
       " 'combustion installation',\n",
       " 'heating installations',\n",
       " 'lift installations',\n",
       " 'pipe sizes',\n",
       " 'pipe size',\n",
       " 'trap sizes',\n",
       " 'discharge pipes',\n",
       " 'circulation pipes',\n",
       " 'gas pipes',\n",
       " 'rainfall pipes',\n",
       " 'discharge pipe',\n",
       " 'a straight length',\n",
       " 'copper discharge pipe',\n",
       " 'copper discharge pipe  D2',\n",
       " 'a G temperature relief',\n",
       " 'Symbols',\n",
       " 'Basic polymers',\n",
       " 'discharge stack',\n",
       " 'discharge stacks',\n",
       " 'a discharge stack',\n",
       " 'a clear space',\n",
       " 'clear space',\n",
       " 'a safe place',\n",
       " 'branch pipe',\n",
       " 'branch pipes',\n",
       " 'wall',\n",
       " 'floor',\n",
       " 'floor a',\n",
       " 'walls',\n",
       " 'roof',\n",
       " 'outside',\n",
       " 'door a',\n",
       " 'Wall',\n",
       " 'residential',\n",
       " 'enclosure',\n",
       " 'drain',\n",
       " 'panel',\n",
       " 'plane',\n",
       " 'walkway',\n",
       " 'thatch',\n",
       " 'wall R',\n",
       " 'the water level',\n",
       " 'the water table',\n",
       " 'the water seal',\n",
       " 'BB',\n",
       " 'PB',\n",
       " 'Class F',\n",
       " 'Class B',\n",
       " 'Class S',\n",
       " 'a wire cage',\n",
       " 'hot water',\n",
       " 'cold water',\n",
       " 'domestic hot water',\n",
       " 'the fixed building service',\n",
       " 'the fixed building services',\n",
       " 'a fixed building service',\n",
       " 'fixed building service',\n",
       " 'fixed building services',\n",
       " 'fixed building',\n",
       " 'the fixed services',\n",
       " 'fixed building services i )',\n",
       " 'static completion',\n",
       " 'working order',\n",
       " 'PN 10 )',\n",
       " 'PE )',\n",
       " 'Low pressure',\n",
       " 'thermostatic mixing valves',\n",
       " 'existing dwellings',\n",
       " 'blending valves',\n",
       " 'the bathroom',\n",
       " 'the shower',\n",
       " 'a bathroom',\n",
       " 'the bath',\n",
       " 'The bathroom',\n",
       " 'the bedrooms',\n",
       " 'TMVs',\n",
       " 'full plans',\n",
       " 'a person registered',\n",
       " 'NVENIENCES',\n",
       " 'Sanitary conveniences',\n",
       " 'hand washing',\n",
       " 'the sanitary appliances',\n",
       " 'the sanitary',\n",
       " 'sanitary appliances',\n",
       " 'WC suites',\n",
       " 'vehicles',\n",
       " 'vehicle',\n",
       " 'businesses',\n",
       " 'BS 6465 - 2 : 1996 Sanitary installations',\n",
       " 'cold taps',\n",
       " 'sanitary appliance',\n",
       " 'a sanitary appliance',\n",
       " 'food preparation area',\n",
       " 'food preparation areas',\n",
       " 'a cubicle',\n",
       " 'WC cubicles',\n",
       " 'WC cubicle',\n",
       " 'sanitary facilities',\n",
       " 'sanitary accommodation',\n",
       " 'traps',\n",
       " 'trap',\n",
       " 'a branch discharge pipe',\n",
       " 'branch discharge pipes',\n",
       " 'a branch pipe',\n",
       " 'sewers',\n",
       " 'sewerage',\n",
       " 'sewer systems',\n",
       " 'cleaners',\n",
       " 'foul drains',\n",
       " 'a grating',\n",
       " 'a macerator',\n",
       " 'the macerator',\n",
       " 'Lifting plants',\n",
       " 'greywater recycling',\n",
       " 'the sewer',\n",
       " 'the drain',\n",
       " 'the basin',\n",
       " 'The sewer',\n",
       " 'the pool',\n",
       " 'the drain sewer',\n",
       " 'the sanitation',\n",
       " 'the trench',\n",
       " 'the wastewater',\n",
       " 'a washbasin',\n",
       " 'Part F',\n",
       " 'Part P',\n",
       " 'Part M',\n",
       " 'Part R',\n",
       " 'Part L',\n",
       " 'Part G H',\n",
       " 'Approved Document E',\n",
       " 'Approved Document C',\n",
       " 'Approved Document P',\n",
       " 'Approved Document G',\n",
       " 'Approved Document F',\n",
       " 'Approved Document K',\n",
       " 'Approved Document M',\n",
       " 'Approved Document B',\n",
       " 'Approved Document N',\n",
       " 'Approved Document J',\n",
       " 'Approved Document 7',\n",
       " 'Approved Document M : Access',\n",
       " 'See Approved Document B',\n",
       " 'Approved Document H How',\n",
       " 'Approved Docume',\n",
       " 'Scale of provision',\n",
       " 'faecal - free wastewater',\n",
       " 'a dishwasher',\n",
       " 'a kitchen',\n",
       " 'the kitchen',\n",
       " 'a garage',\n",
       " 'a chimney',\n",
       " 'a shower',\n",
       " 'the oven',\n",
       " 'In kitchens',\n",
       " 'an sink',\n",
       " 'a kitchen bathroom',\n",
       " 'appendix',\n",
       " 'ducts',\n",
       " 'linings',\n",
       " 'ducting',\n",
       " 'enclosures',\n",
       " 'ducted',\n",
       " 'panels',\n",
       " 'flue',\n",
       " 'sheeting',\n",
       " 'taps',\n",
       " 'duct provided',\n",
       " 'undulations',\n",
       " 'paragraphs A8 A10',\n",
       " 'paragraphs H2 J7',\n",
       " 'Taps',\n",
       " 'bath',\n",
       " 'baths',\n",
       " 'bathing',\n",
       " 'shower',\n",
       " 'the water efficiency',\n",
       " 'the water consumption',\n",
       " 'the water volume',\n",
       " 'the energy efficiency',\n",
       " 'the total water consumption',\n",
       " 'water consumption figures',\n",
       " 'overflow',\n",
       " 'dishwasher',\n",
       " 'Flow rate',\n",
       " 'Flow rates',\n",
       " 'litres per minute',\n",
       " 'dry load',\n",
       " 'ER EFFICIENCY',\n",
       " 'washing machine',\n",
       " 'washing machines',\n",
       " 'Shower outlets',\n",
       " 'BS 8519',\n",
       " 'BS 8515',\n",
       " 'BS 8519 Selection',\n",
       " 'water supply systems 1',\n",
       " 'water supply pipes',\n",
       " 'Rainfall',\n",
       " 'Percentage',\n",
       " 'total capacity',\n",
       " 'the regeneration cycle',\n",
       " 'regeneration cycle',\n",
       " 'Average number',\n",
       " 'a bedroom',\n",
       " 'the bedroom',\n",
       " 'a bedroom wall',\n",
       " 'the first bedroom',\n",
       " 'water softeners',\n",
       " 'a water softener',\n",
       " 'Table A3',\n",
       " 'Table B3',\n",
       " 'Table A below',\n",
       " 'gas fittings',\n",
       " 'pipe fittings',\n",
       " 'light fittings',\n",
       " 'water fittings',\n",
       " 'access fittings',\n",
       " 'terminal fittings',\n",
       " 'low fittings',\n",
       " 'gaskets',\n",
       " 'column a )',\n",
       " 'column b )',\n",
       " 'the average flow rate / volume',\n",
       " 'flow rate / volume',\n",
       " 'CALCULATOR FOR',\n",
       " 'litres / min )',\n",
       " 'litres / min',\n",
       " 'flow rate',\n",
       " 'flow rates',\n",
       " 'fitting type',\n",
       " 'litres per kilogram',\n",
       " 'per cent',\n",
       " 'the calculator',\n",
       " 'a U - value calculator',\n",
       " 'regeneration cycles per day',\n",
       " 'the total capacity',\n",
       " 'The total capacity',\n",
       " 'the installed capacity',\n",
       " 'column 2',\n",
       " 'column 1',\n",
       " 'column 4',\n",
       " 'Table A5 3',\n",
       " 'Table A5 2',\n",
       " 'Table A5 5',\n",
       " 'Table A5 1',\n",
       " 'Table A4 6',\n",
       " 'the greywater savings',\n",
       " 'the greywater supply',\n",
       " 'fittings',\n",
       " 'Constructions',\n",
       " 'fabrics',\n",
       " 'Controls',\n",
       " 'fittings WC',\n",
       " 'Fires assembly',\n",
       " 'plasters',\n",
       " 'workmanship',\n",
       " 'Litres per Number Quantity Greywater minute',\n",
       " 'the WC',\n",
       " 'The WC',\n",
       " 'a WC',\n",
       " 'hydraulic filter efficiency',\n",
       " 'Daily rainwater per',\n",
       " 'yield coefficient',\n",
       " 'the rainwater savings',\n",
       " 'the rainwater',\n",
       " 'mg / l',\n",
       " 'nitrite',\n",
       " 'Water Quality',\n",
       " 'Water supplied',\n",
       " 'water supplied',\n",
       " 'Regulations 2010',\n",
       " 'Regulations 2004',\n",
       " 'Regulations 2006',\n",
       " 'Regulations 2001',\n",
       " 'Regulations 2007',\n",
       " 'Regulations 1998',\n",
       " 'Regulations 2000',\n",
       " 'Regulations 1999',\n",
       " 'Regulations 2012',\n",
       " 'the Regulations 2010',\n",
       " 'the Regulations 2011',\n",
       " 'coli parameter',\n",
       " 'a parameter',\n",
       " 'Schedule I',\n",
       " 'parasite',\n",
       " 'any substance',\n",
       " 'Any substance',\n",
       " 'nitrate',\n",
       " 'Building Approved Inspectors',\n",
       " 'the Approved Inspectors',\n",
       " 'the Building Approved Inspectors',\n",
       " 'The Building Approved Inspectors',\n",
       " 'Approved Inspectors',\n",
       " 'an Approved Inspector',\n",
       " 'The Food Hygiene',\n",
       " 'the Food Hygiene Wales',\n",
       " 'water use',\n",
       " 'energy use',\n",
       " 'water consumption',\n",
       " 'water systems',\n",
       " 'domestic use',\n",
       " 'hmso',\n",
       " 'solid fuel - burning appliances',\n",
       " 'gas - burning appliances',\n",
       " 'Sanitary tapware',\n",
       " 'a type 1',\n",
       " 'a type 2 lift',\n",
       " 'type 2',\n",
       " 'the Type C requirements',\n",
       " 'wras',\n",
       " 'www wras co uk',\n",
       " 'BS EN 1634 - 1 Fire resistance test door and',\n",
       " 'Point smoke BS EN 1634 - 3 Smoke control test door',\n",
       " 'BS EN 54 - 7 Smoke detectors',\n",
       " 'BS EN 12050 - 1 : 2001 Wastewater lifting plants',\n",
       " 'Market Transformation Programme',\n",
       " 'economic feasibility',\n",
       " 'cibse',\n",
       " 'page 18',\n",
       " 'iti ed 15 20',\n",
       " 'underground drains',\n",
       " 'underground drainage',\n",
       " 'a sewerage undertaker',\n",
       " 'the sewerage undertaker',\n",
       " 'sewerage undertaker',\n",
       " 'The sewerage undertaker',\n",
       " 'sewerage undertakers',\n",
       " 'surface water sewers',\n",
       " 'private sewers',\n",
       " 'a surface water sewer',\n",
       " 'Pumping installations',\n",
       " 'Siting',\n",
       " 'siting',\n",
       " 'The siting',\n",
       " 'Drainage fields',\n",
       " 'Traps',\n",
       " 'Trap doors',\n",
       " 'Section 2 : Drainage',\n",
       " 'Cesspools',\n",
       " 'Combined systems',\n",
       " 'rainfall intensities',\n",
       " 'overflowing',\n",
       " 'the cesspool',\n",
       " 'a cesspool',\n",
       " 'cesspools',\n",
       " 'septic tanks',\n",
       " 'septic tank',\n",
       " 'Contaminated',\n",
       " 'seats',\n",
       " 'runoff',\n",
       " 'seated',\n",
       " 'cars',\n",
       " 'chair',\n",
       " 'Disused',\n",
       " 'gutters',\n",
       " 'gutter',\n",
       " 'rainwater pipes',\n",
       " 'rainwater drains',\n",
       " 'litres per second per square metre',\n",
       " 'paved areas',\n",
       " 'underground rainwater drainage',\n",
       " 'unventilated',\n",
       " 'nominal ring stiffness SN4',\n",
       " 'a shared drain / sewer',\n",
       " 'Bedding',\n",
       " 'Code of Practice L24',\n",
       " 'HSE Books 1992',\n",
       " 'HSE Books',\n",
       " 'ISBN 0 7176 0413 6',\n",
       " 'confined spaces',\n",
       " 'Relev',\n",
       " 'the Health and',\n",
       " 'Health and',\n",
       " 'the Health Work',\n",
       " 'March 2004',\n",
       " 'March 1998',\n",
       " 'March 1996',\n",
       " 'April 2002',\n",
       " 'March 2012',\n",
       " 'March 1983',\n",
       " 'foul water',\n",
       " 'a foul sewer',\n",
       " 'the foul water',\n",
       " 'foul air',\n",
       " 'foul water drainage',\n",
       " 'surface water drainage systems',\n",
       " 'a foul water drainage system',\n",
       " 'reclaimed water systems',\n",
       " 'blockage',\n",
       " 'blockages',\n",
       " 'a drainage system',\n",
       " 'the drainage system',\n",
       " 'drainage system',\n",
       " 'drainage systems',\n",
       " 'separate drainage systems',\n",
       " 'The foul drainage system',\n",
       " 'clearing blockages',\n",
       " 'seal depths',\n",
       " 'paragraph 2 11',\n",
       " 'paragraph 2 10',\n",
       " 'paragraph 2 7',\n",
       " 'paragraph 2 8',\n",
       " 'paragraph 1 11',\n",
       " 'paragraph 1 42',\n",
       " 'paragraph 2 29',\n",
       " 'paragraph 2 24',\n",
       " 'paragraph 1 39',\n",
       " 'paragraph 11 2',\n",
       " 'Gullies',\n",
       " 'a stub stack',\n",
       " 'sanitary pipework',\n",
       " 'sanitary',\n",
       " 'the tail',\n",
       " 'the bend',\n",
       " 'the stack',\n",
       " 'the pile',\n",
       " 'individual dwellings',\n",
       " 'single dwellings',\n",
       " 'private dwellings',\n",
       " 'the leaf',\n",
       " 'the leaves',\n",
       " 'the branch',\n",
       " 'the external leaf',\n",
       " 'water seals',\n",
       " 'slope',\n",
       " 'slopes',\n",
       " 'Air admittance valves',\n",
       " 'an air admittance valve',\n",
       " 'Stacks',\n",
       " 'Stack',\n",
       " 'the higher',\n",
       " 'the less',\n",
       " 'the highest',\n",
       " 'litres / sec )',\n",
       " 'litres / sec',\n",
       " 'surcharging',\n",
       " 'paragraph 2 15',\n",
       " 'paragraph 2 14',\n",
       " 'paragraph 2 13',\n",
       " 'paragraph 2 25',\n",
       " 'paragraph 2 17',\n",
       " 'paragraph 2 21',\n",
       " 'paragraph 2 5',\n",
       " 'paragraph 1 17',\n",
       " 'paragraph 2 44',\n",
       " 'paragraph 1 28',\n",
       " 'paragraph 1 31',\n",
       " 'paragraphs 2 35',\n",
       " 'Rodding points',\n",
       " 'lengths',\n",
       " 'corrosion',\n",
       " 'rodent control',\n",
       " 'PP )',\n",
       " 'PP',\n",
       " 'Polyethylene',\n",
       " 'PE',\n",
       " 'the effluent',\n",
       " 'The effluent',\n",
       " 'effluent',\n",
       " 'condensate',\n",
       " 'condensates',\n",
       " 'positive pressure',\n",
       " 'smoke testing',\n",
       " 'smoke tests',\n",
       " 'Smoke testing',\n",
       " 'pressure testing',\n",
       " 'a smoke test',\n",
       " 'smoke production',\n",
       " 'a pressure test',\n",
       " 'the pressure test',\n",
       " 'a water test',\n",
       " 'National Annexes NA',\n",
       " 'pipe',\n",
       " 'pipes',\n",
       " 'shafts',\n",
       " 'sewer',\n",
       " 'shaft',\n",
       " 'mortar',\n",
       " 'hand',\n",
       " 'the curtilage',\n",
       " 'the curtilages',\n",
       " 'the private curtilage',\n",
       " 'Appendix B : Standards',\n",
       " 'Appendix B : Performance of materials',\n",
       " 'Appendix F : Standards Fire resistance',\n",
       " 'Appendix C : Fire doorsets C1',\n",
       " 'Appendix H1 - B',\n",
       " 'Appendix A : Key terms Appendix B : Performance of',\n",
       " 'key terms Appendix B',\n",
       " 'the sewer system',\n",
       " 'the public sewer system',\n",
       " 'the sewerage',\n",
       " 'public sewers',\n",
       " 'low - lying sites',\n",
       " 'gully',\n",
       " 'the floor above',\n",
       " 'the floor below',\n",
       " 'the floor level',\n",
       " 'the floor roof above',\n",
       " 'the room below',\n",
       " 'the storey above',\n",
       " 'a floor level',\n",
       " 'the landing floor level',\n",
       " 'the ground below',\n",
       " 'The lowest floor of',\n",
       " 'private land',\n",
       " 'a public sewer',\n",
       " 'the public sewer',\n",
       " 'a public sewer )',\n",
       " 'a combined sewer',\n",
       " 'the developer',\n",
       " 'The developer',\n",
       " 'private sewer',\n",
       " 'wastewater treatment system',\n",
       " 'wastewater treatment systems',\n",
       " 'a wastewater treatment system',\n",
       " 'cesspool',\n",
       " 'prefabricated components',\n",
       " 'surcharge',\n",
       " 'one property',\n",
       " 'gullies',\n",
       " 'Vent',\n",
       " 'the gradient',\n",
       " 'a gradient',\n",
       " 'The gradient',\n",
       " 'gradients',\n",
       " 'the maximum gradients',\n",
       " 'straight lines',\n",
       " 'granular',\n",
       " 'inspection chambers',\n",
       " 'inspection chamber',\n",
       " 'manholes',\n",
       " 'the crown',\n",
       " 'the Crown',\n",
       " 'manhole',\n",
       " 'flexible joints',\n",
       " 'Special measures',\n",
       " 'an open channel',\n",
       " 'an openable',\n",
       " 'personnel entry',\n",
       " 'the local authority',\n",
       " 'a local authority',\n",
       " 'The local authority',\n",
       " 'Pipe gradients',\n",
       " 'paragraphs 0 10 3',\n",
       " 'paragraphs 0 20 0 25',\n",
       " 'premises 0 6',\n",
       " 'Sewers',\n",
       " 'watertight',\n",
       " 'flexible filler',\n",
       " 'fields Laid',\n",
       " 'light roads Laid',\n",
       " 'BS EN 295',\n",
       " 'BS EN 520',\n",
       " 'bedding factor',\n",
       " 'bedding types',\n",
       " 'Alternative designs',\n",
       " 'pipe strengths',\n",
       " 'pipe strength',\n",
       " 'economic options',\n",
       " 'BS EN 1295',\n",
       " 'BS EN 12845',\n",
       " 'Mini depth',\n",
       " 'roads',\n",
       " 'traffic',\n",
       " 'BS 5911',\n",
       " 'm deep',\n",
       " 'mm x mm',\n",
       " 'Drains',\n",
       " 'highways',\n",
       " 'public open space',\n",
       " 'basements',\n",
       " 'driveways',\n",
       " 'porches',\n",
       " 'sheds',\n",
       " 'construction traffic',\n",
       " 'piling works',\n",
       " 'covers',\n",
       " 'cover',\n",
       " 'the piling',\n",
       " 'trial holes',\n",
       " 'Piling',\n",
       " 'Manholes',\n",
       " 'fixed ladders',\n",
       " 'BS EN 1610',\n",
       " 'BS EN 13141',\n",
       " 'the depth',\n",
       " 'The depth',\n",
       " 'a depth of',\n",
       " 'test lengths',\n",
       " 'BS EN 752 - 4',\n",
       " 'BS EN 752',\n",
       " 'BS EN 845 - 1',\n",
       " 'BS EN 12056',\n",
       " 'gas appliances',\n",
       " 'fire appliances',\n",
       " 'combustion appliances',\n",
       " 'fixed appliances',\n",
       " 'oil appliances',\n",
       " 'Fire appliances',\n",
       " 'ground floor appliances',\n",
       " 'heating stoves',\n",
       " 'a stairway ground',\n",
       " 'a stairway shaft',\n",
       " 'a stairway b',\n",
       " 'a ladder',\n",
       " 'fire stairway b ) resistance',\n",
       " 'a flight steps',\n",
       " 'a gully drain',\n",
       " 'Pipework',\n",
       " 'Reclaimed',\n",
       " 'Approved Document L2A',\n",
       " 'Approved Document L1A',\n",
       " 'Approved Document L1B',\n",
       " 'Approved Document L2B',\n",
       " 'Approved Document H2',\n",
       " 'Approved Document L1A c',\n",
       " 'Approved Document H1',\n",
       " 'existing dwellings Approved Document L2A',\n",
       " 'the Water Regulations Advisory Scheme',\n",
       " 'leaflet No',\n",
       " 'Section 48',\n",
       " 'Repair',\n",
       " 'a nuisance',\n",
       " 'Drainage of',\n",
       " 'water tight',\n",
       " 'gas - tight',\n",
       " 'grout filled',\n",
       " 'Section 102',\n",
       " 'Section 104',\n",
       " 'Section 106',\n",
       " 'Agreements',\n",
       " 'Right',\n",
       " 'Requisition',\n",
       " 'Adoption',\n",
       " 'construction works',\n",
       " 'renovation works',\n",
       " 'disposal works',\n",
       " 'the exit',\n",
       " 'the storey exit',\n",
       " 'The exit',\n",
       " 'the side access',\n",
       " 'the exits',\n",
       " 'the accessible',\n",
       " 'the return wall',\n",
       " 'the guidance 5 14',\n",
       " 'the aspects 1 13',\n",
       " '- 2 25',\n",
       " 'foundation',\n",
       " 'man entry',\n",
       " 'a power failure',\n",
       " 'a septic tank',\n",
       " 'the septic tank',\n",
       " 'drainage fields',\n",
       " 'the soakage capacity',\n",
       " 'soakage capacity',\n",
       " 'drainage mounds',\n",
       " 'a drainage field',\n",
       " 'a drainage field /',\n",
       " 'waterlogged',\n",
       " 'mains drainage',\n",
       " 'watercourse',\n",
       " 'BS 8297 : 2000 Code of practice',\n",
       " 'BS 6297 : 1983 Code of practice',\n",
       " 'BS 5628 - 3 : 2001 Code of practice',\n",
       " 'a watercourse',\n",
       " 'Septic tanks',\n",
       " 'Access covers',\n",
       " 'corrosive nature',\n",
       " 'downslope',\n",
       " 'a vehicle access',\n",
       " 'free - flowing',\n",
       " 'BS EN 12566 - 1',\n",
       " 'The mortar',\n",
       " 'the mortar',\n",
       " 'cementsand ratio',\n",
       " 'BS 5328',\n",
       " 'soakaways',\n",
       " 'underground',\n",
       " 'the 300mm',\n",
       " 'the shaft',\n",
       " 'The shaft',\n",
       " 'the shafts',\n",
       " 'The shaft structure',\n",
       " 'the shaft corridor c',\n",
       " 'the hole',\n",
       " 'subsoils',\n",
       " 'soil',\n",
       " 'soils',\n",
       " 'land',\n",
       " 'mineral',\n",
       " 'groundwater',\n",
       " 'organic',\n",
       " 'smooth',\n",
       " 'sand',\n",
       " 'sediments',\n",
       " 'earth',\n",
       " 'percolation characteristics',\n",
       " 'a soakaway',\n",
       " 'shingle',\n",
       " 'vents',\n",
       " 'venting',\n",
       " 'vent',\n",
       " 'seepage',\n",
       " 'm long',\n",
       " 'm wide',\n",
       " 'ammonia',\n",
       " 'the gravel bed',\n",
       " 'Horizontal flow systems',\n",
       " 'reed beds',\n",
       " 'plant',\n",
       " 'leaves',\n",
       " 'plants',\n",
       " 'roots',\n",
       " 'leaf',\n",
       " 'BRE Good Building Guide No',\n",
       " 'Marking',\n",
       " 'building owners',\n",
       " 'building owner',\n",
       " 'building occupants',\n",
       " 'flushing',\n",
       " 'wetting',\n",
       " 'emptying',\n",
       " 'a licensed contractor',\n",
       " 'fortnightly',\n",
       " 'small sewage treatment works',\n",
       " 'Offences',\n",
       " 'polluting',\n",
       " 'the Water Resources Act',\n",
       " 'lake',\n",
       " 'the Public Health Act 1936',\n",
       " 'settlement tank',\n",
       " 'the inflow',\n",
       " 'the overflow',\n",
       " 'inflow',\n",
       " 'the pores',\n",
       " 'a monthly basis',\n",
       " 'the occupier',\n",
       " 'the occupiers',\n",
       " 'the first occupier',\n",
       " 'principal entrance',\n",
       " 'Principal entrance',\n",
       " 'rainfall intensity',\n",
       " 'a gutter',\n",
       " 'the gutter',\n",
       " 'parapet gutters',\n",
       " 'flat roof',\n",
       " 'Flat roof',\n",
       " 'flat roofs',\n",
       " 'sheet roof',\n",
       " 'a rainwater pipe',\n",
       " 'Information about',\n",
       " 'rainwater drainage systems',\n",
       " 'catchments',\n",
       " 'channels',\n",
       " 'a detention tank',\n",
       " 'paragraph 3 18',\n",
       " 'paragraph 3 6',\n",
       " 'paragraph 3 24',\n",
       " 'paragraph 3 11',\n",
       " 'paragraph 3 29',\n",
       " 'paragraph 3 28',\n",
       " 'paragraph 3 34',\n",
       " 'paragraph 3 36',\n",
       " 'paragraph 3 2',\n",
       " 'paragraph 3 1',\n",
       " 'larger areas',\n",
       " 'oil separators',\n",
       " 'Safety Executive',\n",
       " 'disposal main',\n",
       " 'the owner',\n",
       " 'The owner',\n",
       " 'the owners',\n",
       " 'vesting',\n",
       " 'appendix H1 - C paragraph C 7',\n",
       " 'the consent',\n",
       " 'the waste collection authority',\n",
       " 'Section 46',\n",
       " 'Section 47',\n",
       " 'Section 45',\n",
       " 'industrial waste',\n",
       " 'commercial waste',\n",
       " 'household waste',\n",
       " 'waste containers',\n",
       " 'shelter',\n",
       " 'rail',\n",
       " 'rails',\n",
       " 'railway',\n",
       " 'Rails',\n",
       " 'pedestrian',\n",
       " 'amenity',\n",
       " 'an wall',\n",
       " 'an floor',\n",
       " 'a surface',\n",
       " 'a wall common',\n",
       " 'an layer',\n",
       " 'a ceiling',\n",
       " 'an enclosure',\n",
       " 'An wall',\n",
       " 'These walls',\n",
       " 'Enclosures',\n",
       " 'recyclable',\n",
       " 'a polluted',\n",
       " 'disuse',\n",
       " 'secure containers',\n",
       " 'the top bottom',\n",
       " 'a top bottom',\n",
       " 'Drain and',\n",
       " 'amd 14640 2003',\n",
       " 'amd 15040 2004',\n",
       " 'amd 10440 1999',\n",
       " 'amd 14639 2003',\n",
       " 'Vitrified clay pipes',\n",
       " 'pipe joints',\n",
       " 'joints',\n",
       " 'movement joints',\n",
       " 'copper tubes',\n",
       " 'Plumbing fittings',\n",
       " 'compression ends',\n",
       " 'cast iron spigot',\n",
       " 'Generalities',\n",
       " 'buried pipelines',\n",
       " 'Plastics piping systems',\n",
       " 'AMD 10984 2000',\n",
       " 'AMD 13189 2001',\n",
       " 'Unplasticized',\n",
       " 'PVC - U )',\n",
       " 'Hydraulic design',\n",
       " 'amd 15442 2005',\n",
       " 'high temperature',\n",
       " 'Polypropylene',\n",
       " 'building structure',\n",
       " 'building design',\n",
       " 'building construction',\n",
       " 'building sites',\n",
       " 'internal structure',\n",
       " 'vinyl chloride',\n",
       " 'poly',\n",
       " 'amd 13815 2002',\n",
       " 'amd 13018 2000',\n",
       " 'amd 7114 1992',\n",
       " 'unreinforced',\n",
       " 'BS EN 1917 : 2002 Concrete manholes',\n",
       " 'amd 15359 2004',\n",
       " 'amd 15289 2004',\n",
       " 'amd 15406 2004',\n",
       " 'amd 14857 2004',\n",
       " 'amd 13189 2001',\n",
       " 'amd 15333 2004',\n",
       " 'jacking pipes',\n",
       " 'steel fibre',\n",
       " 'Glass fibre',\n",
       " 'Gravity drainage systems',\n",
       " 'Layout',\n",
       " 'Lifting',\n",
       " 'Other publications',\n",
       " 'WRAS',\n",
       " 'www netregs gov uk',\n",
       " 'carbon monoxide alarms',\n",
       " 'solid fuel appliances',\n",
       " 'older houses',\n",
       " 'liquid biofuel',\n",
       " 'solid biofuel',\n",
       " 'liquid biofuel conform',\n",
       " 'mineral oil',\n",
       " 'heating oil',\n",
       " 'secondary containment',\n",
       " 'informative',\n",
       " 'relining',\n",
       " ...]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domain_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c506907d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simply adding the extracted spans\n",
    "for span in domain_terms:\n",
    "    span_uid = ua.assign_UID(span, SPANS)\n",
    "    \n",
    "    irec_graph = add_tuples(irec_graph, irec_span(span_uid, span))\n",
    "    irec_graph = add_tuples(irec_graph, skos_in_scheme(span_uid, 'schemeUID', SPANS, SPANS))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8c4c91",
   "metadata": {},
   "source": [
    "### Add Acronyms that were grabbed from the text\n",
    "\n",
    "These can help:\n",
    "* remove terms where the boundary detection is off\n",
    "* avoid suggesting similar acronyms, e.g., suggest that EPC and EPS are similar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "434b8738",
   "metadata": {},
   "outputs": [],
   "source": [
    "acronyms = {'PAS': ['ecification', 'Specification'],  'GSIUR': ['Regulations 1998'],  'HSE': ['Regulations 2000',   'water systems',   'Safety Executive',   'Health and Safety Executive'],  'PE': ['Polyethylene', 'polyethylene'],  'DN': ['pipe'],  'DCLG': ['land', 'Local Government', 'England', 'ment'],  'PP': ['Polypropylene'],  'BCB': ['Control Body',   'the building control body',   'Building control body',   'building control body',   'Building Control Body'],  'SRHRV': ['ventilator',   'single room heat recovery ventilator',   'a single room heat recovery ventilator'],  'MVHR': ['blocks', 'heat recovery'],  'WC': ['sets'],  'TFA': ['the total floor area'],  'LRV': ['Light reflectance value'],  'BER': ['Building CO2 Emission Rate', 'CO2 Emission Rate'],  'TER': ['CO2 Emission Rate',   'the Target CO2 Emission Rate',   'Target CO2 Emission Rate'],  'DER': ['CO2 Emission Rate', 'the Dwelling CO2 Emission Rate'],  'EPC': ['energy performance certificate'],  'TFEE': ['Target Fabric Energy Efficiency',   'Fixed building services',   'Energy Efficiency'],  'DHF': ['the Door and Hardware Federation', 'Door and Hardware Federation'],  'REI': ['fire resistance', 'bility'],  'PHE': ['horizontal evacuation'],  'W': ['the final exit', 'final exit'],  'DWELLINGS': ['RESIDENTIAL'],  'OTHER': ['RESIDENTIAL'],  'TSO': ['Office', 'The Stationery Office'],  'FPA': ['the Fire Protection Association', 'Association'],  'A': ['absorption area'],  'AT': ['absorption area'],  'DECC': ['Climate Change'],  'NCM': ['the National Calculation Methodology'],  'ADCAS': ['Allied Services'],  'DFEE': ['Energy Efficiency'],  'LPA': ['the local planning authority', 'planning authority'],  'UKAS': ['the United Kingdom Accreditation Service'],  'BSI': ['the British Standards Institution'],  'EA': ['Accreditation'],  'BGS': ['British Geological Survey'],  'HBN': ['Notes'],  'GGF': ['Glazing Federation'],  'E': ['terms of integrity'],  'TRADA': ['the Timber Research and Development Association', 'Association'],  'ACOP': ['Code of Practice'],  'ATTMA': ['Association'],  'RVA': ['Association', 'the Residential Ventilation Association'],  'TEHVA': ['Association'],  'DSA': ['Association'],  'CIRIA': ['Association'],  'MCRMA': ['Association'],  'DSMA': ['Association'],  'OFTEC': ['Association'],  'WHO': ['Organisation'],  'GAI': ['Architectural Ironmongers'],  'MEV': ['mechanical extract', 'extract ventilation'],  'VST': ['Vicat softening temperature'],  'SCI': ['Guild Steel Construction Institute'],  'FBE': ['the Built Environment', 'ment'],  'DSER': ['Rating'],  'WER': ['Rating'],  'CIWM': ['ment', 'Wastes Management'],  'EOTA': ['ment'],  'GQRA': ['ment'],  'BRE': ['ment', 'the Building Research Establishment'],  'PPS': ['ment'],  'PSV': ['Passive stack ventilation'],  'EST': ['the Energy Saving Trust'],  'CIBSE': ['Ventilation hygiene toolkit', 'Building Services Engineers'],  'AGS': ['Geoenvironmental Specialists'],  'SPAB': ['Ancient Buildings'],  'UF': ['urea formaldehyde'],  'ODPM': ['the Deputy Prime Minister']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aaf33be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for acronym, spans in acronyms.items():\n",
    "    \n",
    "    acronym_uid = ua.assign_UID(acronym, SPANS)\n",
    "   \n",
    "    irec_graph = add_tuples(irec_graph, irec_span(acronym_uid, acronym))\n",
    "    irec_graph = add_tuples(irec_graph, skos_in_scheme(span_uid, 'schemeUID', SPANS, SPANS))\n",
    "    \n",
    "    for span in spans:\n",
    "        span_uid = ua.assign_UID(span, SPANS) \n",
    "        \n",
    "        # todo; \n",
    "        #  could do some filtering here of the clearly erroneous span-acronym combinations\n",
    "        #  or leave this until later, using the graph...\n",
    "    \n",
    "        irec_graph = add_tuples(irec_graph, irec_has_acronym(acronym_uid, span_uid))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953c8e57",
   "metadata": {},
   "source": [
    "### Add CONCEPTS: defined terms from the Approved Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1af2368f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from csv file\n",
    "definitions = pd.read_excel(graph_data_fp.joinpath(\"Approved Documents and derived terms.xlsx\"), sheet_name=\"Definitions\", keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "77530dc1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Term</th>\n",
       "      <th>Definition</th>\n",
       "      <th>Alternative labels</th>\n",
       "      <th>Note</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Absorption</td>\n",
       "      <td>Conversion of sound energy to heat, often by t...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Absorption coefficient</td>\n",
       "      <td>A quantity characterising the effectiveness of...</td>\n",
       "      <td></td>\n",
       "      <td>See BS EN 20354:1993.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Absorptive material</td>\n",
       "      <td>Material that absorbs sound energy.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Term                                         Definition  \\\n",
       "0              Absorption  Conversion of sound energy to heat, often by t...   \n",
       "1  Absorption coefficient  A quantity characterising the effectiveness of...   \n",
       "2     Absorptive material                Material that absorbs sound energy.   \n",
       "\n",
       "  Alternative labels                   Note  \n",
       "0                                            \n",
       "1                     See BS EN 20354:1993.  \n",
       "2                                            "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "definitions[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9d066368",
   "metadata": {},
   "outputs": [],
   "source": [
    "concepts_definitions_dict = {} # keep track of definitions for parsing later\n",
    "\n",
    "# create graph from definitions first\n",
    "for i, row in definitions.iloc[1:].iterrows():\n",
    "    term = row['Term'] if row['Term'].isupper() else row['Term'].lower()\n",
    "    alternative_labels = row['Alternative labels']\n",
    "    definition = row['Definition']\n",
    "    note = row['Note']\n",
    "\n",
    "    # add the term as a CONCEPT and as a SPAN\n",
    "    concept_uid = ua.assign_UID(term, CONCEPTS)\n",
    "    irec_graph = add_tuples(irec_graph, skos_node(concept_uid, term))\n",
    "    irec_graph = add_tuples(irec_graph, skos_in_scheme(concept_uid, 'schemeUID', CONCEPTS, CONCEPTS))\n",
    "    \n",
    "    span_uid = ua.assign_UID(term, SPANS)\n",
    "    irec_graph = add_tuples(irec_graph, irec_span(span_uid, term))\n",
    "    irec_graph = add_tuples(irec_graph, skos_in_scheme(span_uid, 'schemeUID', SPANS, SPANS))\n",
    "    \n",
    "    \n",
    "    # always expecting a definition\n",
    "    irec_graph = add_tuples(irec_graph, skos_definition(concept_uid, definition))\n",
    "    \n",
    "    if note: \n",
    "        irec_graph = add_tuples(irec_graph, skos_note(concept_uid, note))\n",
    "    \n",
    "    if alternative_labels:\n",
    "        # lowercase if not an acronym\n",
    "        alt_labels = [x.strip() if x.isupper() else x.lower().strip() for x in alternative_labels.split(\", \")]  \n",
    "        \n",
    "        for alt_label in alt_labels:\n",
    "            # add the altlabel to the concept node\n",
    "            alt_label_concept_uid = ua.assign_UID(alt_label, CONCEPTS)\n",
    "            irec_graph = add_tuples(irec_graph, skos_altLabel(concept_uid, alt_label_concept_uid))\n",
    "            # also add as a span\n",
    "            alt_label_span_uid = ua.assign_UID(alt_label, SPANS)\n",
    "            irec_graph = add_tuples(irec_graph, irec_span(alt_label_span_uid, alt_label))\n",
    "            irec_graph = add_tuples(irec_graph, skos_in_scheme(alt_label_span_uid, 'schemeUID', SPANS, SPANS))\n",
    "    \n",
    "    if concept_uid not in concepts_definitions_dict: \n",
    "        concepts_definitions_dict[concept_uid] = [{'prefLabel': term, 'definition': definition, 'note': note}]  \n",
    "    else:\n",
    "        concepts_definitions_dict[concept_uid].append({'prefLabel': term, 'definition': definition, 'note': note})  \n",
    "                                                      \n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86a2f6c",
   "metadata": {},
   "source": [
    "### Add SPANS: glossary/index terms from the Approved Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "92cbdde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_terms = pd.read_excel(graph_data_fp.joinpath(\"Approved Documents and derived terms.xlsx\"), sheet_name=\"Index terms\", keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "42ddb6c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Term</th>\n",
       "      <th>AltLabel(s)</th>\n",
       "      <th>Related terms</th>\n",
       "      <th>Broader term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abbreviated eaves</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>eaves</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Access floors</td>\n",
       "      <td>access floor</td>\n",
       "      <td>Platform floors</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Access for fire service</td>\n",
       "      <td>fire access</td>\n",
       "      <td></td>\n",
       "      <td>Fire service facilities</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Term   AltLabel(s)    Related terms  \\\n",
       "0        abbreviated eaves                                  \n",
       "1            Access floors  access floor  Platform floors   \n",
       "2  Access for fire service   fire access                    \n",
       "\n",
       "              Broader term  \n",
       "0                    eaves  \n",
       "1                           \n",
       "2  Fire service facilities  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_terms[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843da834",
   "metadata": {},
   "source": [
    "* add triples from index terms / glossaries; we will treat these terms as SPANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4f1358bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in index_terms.iloc[1:].iterrows():\n",
    "    term = row['Term'].strip() if row['Term'].isupper() else row['Term'].lower().strip()\n",
    "    alternative_labels = row['AltLabel(s)'] \n",
    "    related_terms = row['Related terms']\n",
    "    broader_term = row['Broader term']\n",
    "    \n",
    "    # add the term as a SPAN only\n",
    "    span_uid = ua.assign_UID(term, SPANS)\n",
    "    irec_graph = add_tuples(irec_graph, irec_span(span_uid, term))\n",
    "    irec_graph = add_tuples(irec_graph, skos_in_scheme(span_uid, 'schemeUID', SPANS, SPANS))\n",
    "        \n",
    "    if alternative_labels:\n",
    "        # lowercase if not an acronym\n",
    "        alt_labels = [x.strip() if x.isupper() else x.lower().strip() for x in alternative_labels.split(\", \")]  \n",
    "        \n",
    "        for alt_label in alt_labels:\n",
    "            # add alt-label as a span only (as well)\n",
    "            alt_label_uid = ua.assign_UID(alt_label, SPANS)\n",
    "            irec_graph = add_tuples(irec_graph, irec_span(alt_label_uid, alt_label))\n",
    "            irec_graph = add_tuples(irec_graph, skos_in_scheme(alt_label_uid, 'schemeUID', SPANS, SPANS))\n",
    "            \n",
    "            if alt_label.isupper():\n",
    "                # there are acronyms among the alternative labels\n",
    "                irec_graph = add_tuples(irec_graph, irec_has_acronym(span_uid, alt_label_uid))\n",
    "            else:\n",
    "                ### Should I use skos altlabels between spans? maybe create IREC alternative label?\n",
    "                ### Should I use skos altlabels between spans? maybe create IREC alternative label?\n",
    "                ### Should I use skos altlabels between spans? maybe create IREC alternative label?\n",
    "                irec_graph = add_tuples(irec_graph, skos_altLabel(span_uid, alt_label_uid))\n",
    "                \n",
    "\n",
    "    if related_terms:\n",
    "        rel_terms = [x.strip() if x.isupper() else x.lower().strip() for x in related_terms.split(\", \")]\n",
    "        for rel_term in rel_terms:\n",
    "            # add related terms as a span (as well)\n",
    "            related_uid = ua.assign_UID(rel_term, SPANS)\n",
    "            irec_graph = add_tuples(irec_graph, irec_span(related_uid, rel_term))\n",
    "            irec_graph = add_tuples(irec_graph, skos_in_scheme(related_uid, 'schemeUID', SPANS, SPANS))\n",
    "            \n",
    "            if rel_term.isupper():\n",
    "                # there are acronyms among the related labels as well\n",
    "                irec_graph = add_tuples(irec_graph, irec_has_acronym(span_uid, related_uid))\n",
    "            else:\n",
    "                irec_graph = add_tuples(irec_graph, irec_related(span_uid, related_uid)) \n",
    "    \n",
    "    if broader_term:\n",
    "        # We do not expect that the broader term is necessarily a concept.\n",
    "        # Currently, it is simply a feature for future reference.\n",
    "        # We expect 1 broader term at most, assuming the final conceptualisation would\n",
    "        # be structured like a tree (Directed Acyclic Graph with 1 parent at most).\n",
    "        b_term = broader_term.strip().lower() if not broader_term.isupper() else broader_term.strip()\n",
    "        # also broader term as a span\n",
    "        b_term_uid = ua.assign_UID(b_term, SPANS)\n",
    "        irec_graph = add_tuples(irec_graph, irec_span(b_term_uid, b_term)) \n",
    "        irec_graph = add_tuples(irec_graph, skos_in_scheme(b_term_uid, 'schemeUID', SPANS, SPANS))\n",
    "        \n",
    "        ### Should I use skos broader between spans? maybe create an IREC broader?\n",
    "        ### Should I use skos broader between spans? maybe create an IREC broader?\n",
    "        irec_graph = add_tuples(irec_graph, skos_broader(span_uid, b_term_uid, SPANS, SPANS)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "40952822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Graph identifier=N0431cf65e56e46f1ab18b0e6ee40943a (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irec_graph.serialize(destination=graph_output_fp.joinpath(\"approved_doc_terms_only.ttl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fbf590",
   "metadata": {},
   "source": [
    "### Print some insight in the graph so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "14daa5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes in 'https://example.org/irec-spans': 12742\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12742"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ua.count_nodes_in_namespace(SPANS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "204cfd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes in 'https://example.org/irec-concepts': 352\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "352"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ua.count_nodes_in_namespace(CONCEPTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bdfeb278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "273 ; type ; Concept\n",
      "273 ; prefLabel ; sanitary accommodation\n",
      "273 ; inScheme ; schemeUID\n",
      "273 ; definition ; A space containing one or more water closets or urinals, whether or not it also contains other sanitary appliances. Sanitary accommodation containing one or  more cubicles counts as a single space if there is free circulation of air throughout the space.\n"
     ]
    }
   ],
   "source": [
    "ua.print_node_by_id(irec_graph, 273, CONCEPTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e6ad1b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sanitary%20accommodation ; type ; Span\n",
      "sanitary%20accommodation ; label ; sanitary accommodation\n",
      "sanitary%20accommodation ; inScheme ; schemeUID\n",
      "sanitary%20accommodation ; related ; sanitary%20appliance\n"
     ]
    }
   ],
   "source": [
    "ua.print_node_by_id(irec_graph, urllib.parse.quote('sanitary accommodation'), SPANS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1ddcb895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sanitary%20accommodation ; type ; Span\n",
      "sanitary%20accommodation ; label ; sanitary accommodation\n",
      "sanitary%20accommodation ; inScheme ; schemeUID\n",
      "sanitary%20accommodation ; related ; sanitary%20appliance\n"
     ]
    }
   ],
   "source": [
    "ua.print_node_by_text(irec_graph, 'sanitary accommodation', SPANS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1989fb60",
   "metadata": {},
   "source": [
    "As you can see in the examples above, the concept `wet room` and the span `sanitary accomdodation` are related:\n",
    "* The concept `wet room` is provided with a note in the merged approved documents.\n",
    "* The text inside this node describes how, for part F of the approved documents, `sanitary accomodation` is regarded as a `wet room`. \n",
    "\n",
    "Based on the above, we'd like to link the span `sanitary accomodation` to the concept `wet room`. While we could parse the note in more detail, and identify that a `skos:altLabel` relation should be added, we'll use a more generic approach:\n",
    "* Any span that is found inside a definition or note of a concept will be linked through `irec:related`\n",
    "* Based on the definitions above, potential spans related to the `wet room` concept then become: `sanitary accomdoation`, `airborn moisture`, `kitchen`, `utility room`, `bathroom`, `WC`, `tanking`, `drainage`, `gulley`, `shower`.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "We believe that the types of relations described above can be valuable and would like to provide more definitions for more terms, to help interrelate more spans and concepts. To this end, we first try to find WikiData definitions for all concepts and spans. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407358e6",
   "metadata": {},
   "source": [
    "### Grab wikipedia definitions for Concept nodes, and store locally for re-use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05db9542",
   "metadata": {},
   "source": [
    "* First, we try to grab all wiki definitions for all spans and concepts that are in the graph (so far)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f2e41964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the SPARQL endpoint for wikidata\n",
    "\n",
    "sparql_wrapper = SPARQLWrapper(\"https://query.wikidata.org/sparql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "12e9e280",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wiki_matches(graph_sparql_endpoint: SPARQLWrapper,\n",
    "                     jargon_term_and_uids: List):\n",
    "\n",
    "    all_wiki_definitions = {}\n",
    "    # we want to grab the term (subject), any definition (subjectDescription) and the class (subjectClass)\n",
    "    sparql_q = \"\"\"\n",
    "               SELECT ?subject ?subjectDescription ?classUID ?className WHERE {\n",
    "                  ?subject rdfs:label \"$QUERY\"@en.\n",
    "                  ?subject wdt:P31|wdt:P279 ?classUID.\n",
    "                  SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\".}\n",
    "                  ?classUID  rdfs:label ?className  FILTER(LANG(?className) = \"en\").\n",
    "                }\n",
    "               \"\"\"\n",
    "    \n",
    "    for term, uid in tqdm(jargon_term_and_uids):\n",
    "        # make the call to \n",
    "        temp_q = sparql_q.replace(\"$QUERY\", term)\n",
    "        graph_sparql_endpoint.setQuery(temp_q)\n",
    "        graph_sparql_endpoint.setReturnFormat(JSON)\n",
    "        try:\n",
    "            json_output = graph_sparql_endpoint.query().convert()\n",
    "        except:\n",
    "            # If no result, wait 2s; One client is allowed 30 error queries per minute\n",
    "            print(f\"Error for query, you should what's wrong with the term: {term}\")\n",
    "            time.sleep(2)\n",
    "            continue\n",
    "            \n",
    "        # sometimes multiple Wiki UIDs for a single term, we grab them all here\n",
    "        bindings = [v for v in json_output['results']['bindings']]\n",
    "            \n",
    "\n",
    "        for v in bindings:\n",
    "            class_uid = v['classUID']['value'] if 'classUID' in v else \"\"\n",
    "            class_label = v['className']['value'] if 'className' in v else \"\"\n",
    "            \n",
    "            if 'subjectDescription' in v:\n",
    "                if uid not in all_wiki_definitions:\n",
    "                    all_wiki_definitions[uid] = [{'prefLabel': term,\n",
    "                                                  'class_uid': class_uid,\n",
    "                                                  'class_label': class_label,\n",
    "                                                  'WikiUID': v['subject']['value'],\n",
    "                                                  'WikiDefinition': v['subjectDescription']['value']}]\n",
    "                else:\n",
    "                    all_wiki_definitions[uid].append({'prefLabel': term,\n",
    "                                                      'class_uid': class_uid,\n",
    "                                                      'class_label': class_label,\n",
    "                                                      'WikiUID': v['subject']['value'],\n",
    "                                                      'WikiDefinition': v['subjectDescription']['value']})\n",
    "            elif 'subject' in v:\n",
    "                # no description found, simply adding wiki UID if that exists\n",
    "                if uid not in all_wiki_definitions:\n",
    "                    all_wiki_definitions[uid] = [{'prefLabel': term,\n",
    "                                                  'class_uid': class_uid,\n",
    "                                                  'class_label': class_label,\n",
    "                                                  'WikiUID': v['subject']['value']}]\n",
    "                else:\n",
    "                    all_wiki_definitions[uid].append({'prefLabel': term,\n",
    "                                                      'class_uid': class_uid,\n",
    "                                                      'class_label': class_label,\n",
    "                                                      'WikiUID': v['subject']['value']})\n",
    "    return all_wiki_definitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6b57bf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "concepts_and_uids = [(k, v) for k, v in ua.UIDs[CONCEPTS.placeholder.defrag().__reduce__()[1][0]].items()]\n",
    "spans_and_uids = [(k, v) for k, v in ua.UIDs[SPANS.placeholder.defrag().__reduce__()[1][0]].items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3f9b3c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First run for the Concepts\n",
    "concept_wiki_dict_fp = graph_output_fp.joinpath(\"concept_wiki_dict.json\")\n",
    "if not concept_wiki_dict_fp.exists():\n",
    "    concept_wiki_dict = get_wiki_matches(sparql_wrapper, concepts_and_uids)#{'test': 1, 'conductor':2})\n",
    "    with open(concept_wiki_dict_fp, 'w') as f:\n",
    "        json.dump(concept_wiki_dict, f, indent=2)\n",
    "else:\n",
    "    with open(concept_wiki_dict_fp, 'r') as f:\n",
    "        concept_wiki_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1b368865",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now run for the spans\n",
    "span_wiki_dict_fp = graph_output_fp.joinpath(\"span_wiki_dict.json\")\n",
    "if not span_wiki_dict_fp.exists():\n",
    "    span_wiki_dict = get_wiki_matches(sparql_wrapper, spans_and_uids)#{'test': 1, 'conductor':2})\n",
    "    with open(span_wiki_dict_fp, 'w') as f:\n",
    "        json.dump(span_wiki_dict, f, indent=2)\n",
    "else:\n",
    "    with open(span_wiki_dict_fp, 'r') as f:\n",
    "        span_wiki_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "10f67b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes in 'https://example.org/irec-concepts': 296\n",
      "Number of concepts with WikiData definitions: 244 (82.43%)\n",
      "Number of nodes in 'https://example.org/irec-spans': 12590\n",
      "Number of spans with WikiData definitions: 655 (5.20%)\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of concepts with WikiData definitions: {} ({:.2f}%)\".format(len(concept_wiki_dict), len(concept_wiki_dict)/ua.count_nodes_in_namespace(CONCEPTS)*100))\n",
    "print(\"Number of spans with WikiData definitions: {} ({:.2f}%)\".format(len(span_wiki_dict), len(span_wiki_dict)/ua.count_nodes_in_namespace(SPANS)*100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4dec3a",
   "metadata": {},
   "source": [
    "* Some examples of/insight in definitions from different sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0dfa4268",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'prefLabel': 'absorption coefficient',\n",
       "  'definition': 'A quantity characterising the effectiveness of a sound absorbing surface. The proportion of sound energy absorbed is given as a number between zero (for a fully reflective surface) and one (for a fully absorptive surface). Note that sound absorption coefficients determined from laboratory measurements may have values slightly larger than one.',\n",
       "  'note': 'See BS EN 20354:1993.'}]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of definitions from approved documents\n",
    "concepts_definitions_dict['1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e746463c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'prefLabel': 'absorption coefficient',\n",
       "  'class_uid': 'http://www.wikidata.org/entity/Q107715',\n",
       "  'class_label': 'physical quantity',\n",
       "  'WikiUID': 'http://www.wikidata.org/entity/Q97368968',\n",
       "  'WikiDefinition': 'measure for the exponential reduction of a quantity along a path due to absorption',\n",
       "  'Spans in definitions and notes': ['a path due',\n",
       "   'a quantity',\n",
       "   'the exponential reduction']}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of definitions for the same concept, from WikiData\n",
    "concept_wiki_dict['1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2971f4f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['absorbent']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'prefLabel': 'absorbent',\n",
       "  'class_uid': 'http://www.wikidata.org/entity/Q3505845',\n",
       "  'class_label': 'state',\n",
       "  'WikiUID': 'http://www.wikidata.org/entity/Q110147344',\n",
       "  'WikiDefinition': 'having the ability or tendency to absorb; able to soak up liquid easily; absorptive.',\n",
       "  'Spans in definitions and notes': ['liquid easily', 'the ability tendency']}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of definitions for a related span, from WikiData\n",
    "print([k for k in span_wiki_dict.keys() if 'absor' in k])\n",
    "span_wiki_dict['absorbent']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699c6acb",
   "metadata": {},
   "source": [
    "### Only keep WikiData definitions that belong to classes that we've annotated\n",
    "* We have previously annotated the relevance of all WikiData classes returned for the defined terms and index terms in the Approved Documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c5aa0969",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_wikidata_classes_df = pd.read_csv(graph_data_fp.joinpath(\"wiki_classes_annotated.csv\"), index_col=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c66f2893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WikiData class</th>\n",
       "      <th>Annotation</th>\n",
       "      <th>Example spans</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WikiData UIDs</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>['Q107715']</th>\n",
       "      <td>physical quantity</td>\n",
       "      <td>y</td>\n",
       "      <td>['sound pressure level', 'density', 'area', 's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>['Q82799']</th>\n",
       "      <td>name</td>\n",
       "      <td>n</td>\n",
       "      <td>['access point']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>['Q180160']</th>\n",
       "      <td>metadata</td>\n",
       "      <td>n</td>\n",
       "      <td>['access point']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  WikiData class Annotation  \\\n",
       "WikiData UIDs                                 \n",
       "['Q107715']    physical quantity          y   \n",
       "['Q82799']                  name          n   \n",
       "['Q180160']             metadata          n   \n",
       "\n",
       "                                                   Example spans  \n",
       "WikiData UIDs                                                     \n",
       "['Q107715']    ['sound pressure level', 'density', 'area', 's...  \n",
       "['Q82799']                                      ['access point']  \n",
       "['Q180160']                                     ['access point']  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotated_wikidata_classes_df[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1f587141",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikiclass_dict = {}\n",
    "for row in annotated_wikidata_classes_df.iterrows():\n",
    "    uid_list_string, class_annotations_examples = row\n",
    "    uid_list = uid_list_string[2:-2].split(',')\n",
    "    for uid in uid_list:\n",
    "        wikiclass_dict[uid] = {\n",
    "            'Class': class_annotations_examples['WikiData class'],\n",
    "            'Annotation': class_annotations_examples['Annotation'],\n",
    "            'Example spans': class_annotations_examples['Example spans']\n",
    "        }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dde9e8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_wikidata_classes(wiki_class_dict, term_dict):\n",
    "    new_term_dict = {}\n",
    "    removed_definitions = []\n",
    "    for uid, definition_dict_list in term_dict.items():\n",
    "        for definition_dict in definition_dict_list:\n",
    "            class_uid = definition_dict['class_uid'].rsplit(\"/\", 1)[1]\n",
    "            if class_uid in wikiclass_dict:\n",
    "                class_name = wikiclass_dict[class_uid][\"Class\"]\n",
    "                if wikiclass_dict[class_uid][\"Annotation\"] == 'y':\n",
    "                    if uid not in new_term_dict:\n",
    "                        new_term_dict[uid] = [definition_dict]\n",
    "                    else:\n",
    "                        new_term_dict[uid].append(definition_dict)\n",
    "                else:\n",
    "                    removed_definitions.append(definition_dict)\n",
    "                        \n",
    "    return new_term_dict, removed_definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e49d20d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_wiki_dict, removed_definitions = filter_wikidata_classes(wikiclass_dict, concept_wiki_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "78aefda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "span_wiki_dict, removed_definitions = filter_wikidata_classes(wikiclass_dict, span_wiki_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fae49f",
   "metadata": {},
   "source": [
    "### Parse all definitions (including WikiData) to identify additional spans\n",
    "* Add the spar objects found in definitions to respective dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5d5429cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "te = TermExtractor(max_num_cpu_threads=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bfdd2f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_spar_labels(input_dict: Dict[str, str], term_extractor: TermExtractor):\n",
    "    number_of_definitions = 0\n",
    "    for uid, definition_dict_list in tqdm(input_dict.items()):\n",
    "        for idx, definition_dict in enumerate(definition_dict_list):\n",
    "            if 'Spans in definitions and notes' in definition_dict:\n",
    "                # spans already computed for this definition_dict, continuing to check next\n",
    "                continue\n",
    "            \n",
    "            spartxt_objects = []\n",
    "            for k in definition_dict.keys():\n",
    "                if k in ['WikiDefinition', 'definition', 'note']:\n",
    "                    to_be_parsed = definition_dict[k]\n",
    "                    number_of_definitions += 1\n",
    "                    sentences = term_extractor.split_into_sentences(to_be_parsed)\n",
    "                    # cleaning spans as well;\n",
    "                    sentences = [remove_unicode_chars(s).encode(\"ascii\", \"ignore\").decode() for s in sentences]\n",
    "                    spartxt_objects += custom_cleaning_rules(term_extractor.process_sentences(sentences))\n",
    "                    \n",
    "            input_dict[uid][idx]['Spans in definitions and notes'] = spartxt_objects\n",
    "    print(f\"Processed {number_of_definitions} definitions\")\n",
    "    return input_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "46bf46cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading previously computed concepts_definitions_dict with SPaR.txt objects from file\n"
     ]
    }
   ],
   "source": [
    "concept_definitions_dict_fp = graph_output_fp.joinpath(\"concepts_definitions_dict.json\")\n",
    "if not concept_definitions_dict_fp.exists():\n",
    "    print(\"Computing SPaR.txt objects for concepts_definitions_dict\")\n",
    "    concepts_definitions_dict = add_spar_labels(concepts_definitions_dict, te)\n",
    "    with open(concept_definitions_dict_fp, 'w') as f:\n",
    "        json.dump(concepts_definitions_dict, f, indent=2)\n",
    "else:\n",
    "    print(\"Loading previously computed concepts_definitions_dict with SPaR.txt objects from file\")\n",
    "    with open(concept_definitions_dict_fp, 'r') as f:\n",
    "        concepts_definitions_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6746b48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 244/244 [00:00<00:00, 483196.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 definitions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "concept_wiki_dict = add_spar_labels(concept_wiki_dict, te)\n",
    "# Save the updated concept_wiki_dict, which will be loaded in previous cells anyway\n",
    "with open(concept_wiki_dict_fp, 'w') as f:\n",
    "    json.dump(concept_wiki_dict, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ae7de0d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 655/655 [00:00<00:00, 724490.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 definitions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "span_wiki_dict = add_spar_labels(span_wiki_dict, te)\n",
    "# Save the updated span_wiki_dict, which will be loaded in previous cells anyway\n",
    "with open(span_wiki_dict_fp, 'w') as f:\n",
    "    json.dump(span_wiki_dict, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d77b070",
   "metadata": {},
   "source": [
    "### Add any new spans to the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0ff7e865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of defined terms:  992\n",
      "Number of definitions/notes found: 2472\n",
      "Number of new spans:  3429\n"
     ]
    }
   ],
   "source": [
    "all_spartxt_objects = {}\n",
    "total_num_definitions = 0\n",
    "for x in [concepts_definitions_dict, concept_wiki_dict, span_wiki_dict]:\n",
    "    for definition_dict_list in x.values():\n",
    "        for definition_dict in definition_dict_list:\n",
    "            \n",
    "            for k in definition_dict.keys():\n",
    "                if k in ['WikiDefinition', 'definition', 'note']:\n",
    "                    total_num_definitions += 1\n",
    "            \n",
    "            defined_term = definition_dict['prefLabel']\n",
    "            spar_objects_in_dict = custom_cleaning_rules(definition_dict['Spans in definitions and notes'])\n",
    "            if defined_term not in all_spartxt_objects:\n",
    "                all_spartxt_objects[defined_term] = spar_objects_in_dict\n",
    "            else:\n",
    "                all_spartxt_objects[defined_term] += spar_objects_in_dict\n",
    "                \n",
    "unique_spans_from_defined_terms = list(set([s for v in all_spartxt_objects.values() for s in v]))\n",
    "print(\"Number of defined terms: \", len(all_spartxt_objects))\n",
    "print(\"Number of definitions/notes found:\", total_num_definitions)\n",
    "print(\"Number of new spans: \", len(unique_spans_from_defined_terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5ea3998b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tool',\n",
       " 'mooring line',\n",
       " 'brick - fireplace',\n",
       " 'Dubai',\n",
       " 'evaporation',\n",
       " 'machines',\n",
       " 'art objects',\n",
       " 'operation',\n",
       " 'electric potential',\n",
       " 'layers']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(unique_spans_from_defined_terms, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8336dcaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sanitary%20accommodation'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_space = SPANS._.defrag().__reduce__()[1][0]\n",
    "ua.UIDs[n_space]['sanitary accommodation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3a5969d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a related label between the defined concept or span, and the span found in a definition\n",
    "for i, (term, related_spans) in enumerate(all_spartxt_objects.items()):\n",
    "\n",
    "    # add the term as a span if needed\n",
    "    term_uid, new_uid = ua.assign_UID(term, SPANS)\n",
    "    if new_uid:\n",
    "        irec_graph = add_tuples(irec_graph, irec_span(term_uid, term))\n",
    "    \n",
    "    # Check if the term is actually already identified as a concept\n",
    "    term_is_concept = False\n",
    "    if term in ua.UIDs[CONCEPTS._.defrag().__reduce__()[1][0]]:\n",
    "        term_is_concept = True\n",
    "\n",
    "    rel_spans = [x.strip() if x.isupper() else x.lower().strip() for x in related_spans]\n",
    "    for rel_term in rel_spans:\n",
    "        related_uid, new_uid = ua.assign_UID(rel_term, SPANS)\n",
    "        if new_uid:\n",
    "            irec_graph = add_tuples(irec_graph, irec_span(related_uid, rel_term))\n",
    "        \n",
    "        irec_graph = add_tuples(irec_graph, irec_related(term_uid, related_uid)) # Add relation between spans\n",
    "        if term_is_concept:\n",
    "            irec_graph = add_tuples(irec_graph, irec_related(term_uid, related_uid, CONCEPTS, SPANS)) # Add relation between concept and spans\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec817464",
   "metadata": {},
   "source": [
    "### Add WikiData definitions to graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c270684a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_wiki_definitions(irec_graph: Graph, wiki_dict: Dict[str, str], dict_namespace: Namespace):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    for i, (term_uid, definition_dict_list) in enumerate(wiki_dict.items()):\n",
    "        for definition_dict in definition_dict_list:\n",
    "            # Add the WIKI node to our graph\n",
    "            wiki_term = definition_dict['prefLabel']\n",
    "            wiki_uid = definition_dict['WikiUID'].rsplit('/', 1)[1]\n",
    "\n",
    "            # keep track of uid that is added to the graph\n",
    "            _, _ = ua.keep_track_of_existing_UID(wiki_term, wiki_uid, WIKI)\n",
    "\n",
    "            # add to graph, in WIKI namespace\n",
    "            irec_graph = add_tuples(irec_graph, skos_node(wiki_uid, wiki_term, WIKI))\n",
    "\n",
    "            # Add the WIKI definition to the node if it exists\n",
    "            if 'WikiDefinition' in definition_dict:            \n",
    "                definition = definition_dict['WikiDefinition']\n",
    "                irec_graph = add_tuples(irec_graph, skos_definition(wiki_uid, definition, WIKI))\n",
    "\n",
    "            # Add an exact match between the wiki node and our concept/span node\n",
    "            irec_graph = add_tuples(irec_graph, skos_exact_match(term_uid, wiki_uid, dict_namespace, WIKI))\n",
    "    return irec_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "43cf5dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "irec_graph = add_wiki_definitions(irec_graph, concept_wiki_dict, CONCEPTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "25128df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "irec_graph = add_wiki_definitions(irec_graph, span_wiki_dict, SPANS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f99770a",
   "metadata": {},
   "source": [
    "### We will also add the Uniclass terms that we found in the text to the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "82fe7391",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(graph_data_fp.joinpath(\"uniclass_terms_in_text.pkl\"), 'rb') as f:\n",
    "    uniclass_terms_in_text = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1076ed68",
   "metadata": {},
   "outputs": [],
   "source": [
    "for uniclass_uid, definition_dict in uniclass_terms_in_text.items():\n",
    "    # Add the Uniclass node to our graph\n",
    "    uniclass_term = definition_dict['pref_label']\n",
    "    # keep track of uid that is added to the graph\n",
    "    _, _ = ua.keep_track_of_existing_UID(uniclass_term, uniclass_uid, UNICLASS)\n",
    "    # add to graph, in UNICLASS namespace\n",
    "    irec_graph = add_tuples(irec_graph, skos_node(uniclass_uid, uniclass_term, UNICLASS))\n",
    "    \n",
    "    # Determine the corresponding term_uid in SPANS\n",
    "    if ua.retrieve_uid_by_text(uniclass_term): # First as is (no lowercasing, despite Uniclass casing)\n",
    "        # Add an exact match between the Uniclass node and the corresponding span\n",
    "        span_uid = ua.retrieve_uid_by_text(uniclass_term)\n",
    "        irec_graph = add_tuples(irec_graph, skos_exact_match(span_uid, uniclass_uid, SPANS, UNICLASS))\n",
    "    elif ua.retrieve_uid_by_text(term.lower()):\n",
    "        # Add an exact match between the wiki node and the corresponding lowercased version in SPANS\n",
    "        span_uid = ua.retrieve_uid_by_text(uniclass_term.lower())\n",
    "        irec_graph = add_tuples(irec_graph, skos_exact_match(span_uid, uniclass_uid, SPANS, UNICLASS))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e978d7",
   "metadata": {},
   "source": [
    "### Compute properties between spans\n",
    "* link definitions to spans (if span occurs verbatim; linkwith irec:related)\n",
    "* link spans to spans:\n",
    "  * constitutes; x occurs in y, thus y might be an extended phrase for x and perhaps a subclass, or x may be a material property, and so on\n",
    "  * morphological similarity, x may be an inflection of y or somehow related\n",
    "  * semantic similarity, x and y might be alternative labels or have the same superclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e4b50ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_span_in_definition(span:str, definition: str):\n",
    "    if span in definition:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0bbf2ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def span_constitutes_span(span_one, span_two):\n",
    "    if span_two in span_one:  # (span_one, irec:constitues, span_two)\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e46c83a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_model_name = \"bert-base-cased\"\n",
    "embedding_output_fp = Path.cwd().joinpath(\"data\", \"term_embedding\")\n",
    "IDF_path = embedding_output_fp.joinpath(\"IDF_weights.json\")\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "bert_model = BertModel.from_pretrained(bert_model_name, output_hidden_states=True)\n",
    "embedder = Embedder(tokenizer, bert_model, \n",
    "                      IDF_dict=json.load(open(IDF_path)), \n",
    "                      embedding_fp=embedding_output_fp,\n",
    "                      layers_to_use = [12],         # we'll use the output of the last layer\n",
    "                      layer_combination = \"avg\",    # how to combine layers if multiple are used\n",
    "                      idf_threshold = 1.5,          # minimum IDF value for a token to contribute\n",
    "                      idf_weight_factor = 1.0,      # modify how strong the influence of IDF weighting is\n",
    "                      not_found_idf_value = 0.5)    # IDF value for tokens that weren't seen during IDF computation (doesn't apply here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4f06a659",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantically_similar(embedder: Embedder, span_one: str, span_two: str, cosine_sim_threshold:float = .7):\n",
    "    embedding_one = embedder.embed_and_normalise_span(span_one)\n",
    "    embedding_two = embedder.embed_and_normalise_span(span_two)\n",
    "    # If the cosine similarity is over a given threshold: return True\n",
    "    if (1 - cosine(embedding_one, embedding_two)) > cosine_sim_threshold:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d5a907fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def morphologically_similar(span_one, span_two):\n",
    "    if levenshtein(span_one, span_two):\n",
    "        return True\n",
    "    else:\n",
    "        # if all but one word the same for 2 or more words\n",
    "        span_one_words = span_one.split(' ')\n",
    "        span_two_words = span_two.split(' ')\n",
    "        sow_len = len(span_one_words)\n",
    "        stw_len = len(span_two_words)\n",
    "        if sow_len > 1 and stw_len > 1:\n",
    "            overlap = list(set(span_one_words) & set(span_two_words))\n",
    "            if len(overlap) >= (sow_len - 1) and len(overlap) >= (stw_len - 1):\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5a61f002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "morphologically similar:  False\n",
      "semantically similar:  False\n"
     ]
    }
   ],
   "source": [
    "test_1 = \"acoustic\"\n",
    "test_2 = \"thermal\"\n",
    "print(\"morphologically similar: \", morphologically_similar(test_1, test_2))\n",
    "print(\"semantically similar: \", semantically_similar(embedder, test_1, test_2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c5cdcc07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "morphologically similar:  True\n",
      "semantically similar:  False\n"
     ]
    }
   ],
   "source": [
    "test_1 = \"solar system\"\n",
    "test_2 = \"photo-voltaic system\"\n",
    "print(\"morphologically similar: \", morphologically_similar(test_1, test_2))\n",
    "print(\"semantically similar: \", semantically_similar(embedder, test_1, test_2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e207d87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "morphologically similar:  True\n",
      "semantically similar:  False\n"
     ]
    }
   ],
   "source": [
    "test_1 = \"damp proof course\"\n",
    "test_2 = \"damp proof membrane\"\n",
    "print(\"morphologically similar: \", morphologically_similar(test_1, test_2))\n",
    "print(\"semantically similar: \", semantically_similar(embedder, test_1, test_2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2ac1d190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "morphologically similar:  False\n",
      "semantically similar:  False\n"
     ]
    }
   ],
   "source": [
    "test_1 = \"hot water storage\"\n",
    "test_2 = \"cold water system\"\n",
    "print(\"morphologically similar: \", morphologically_similar(test_1, test_2))\n",
    "print(\"semantically similar: \", semantically_similar(embedder, test_1, test_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "95204ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Honestly, semantic similarity is not good enough... in the future maybe have a little NN with attention over \n",
    "# left-hand and right-hand spans. Almost like Machine Translation?\n",
    "\n",
    "# todo; buff up the morpholgicall similarity function; improve the similarity feature, \n",
    "# - stemming\n",
    "# - antonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5b875cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features(argument_list):\n",
    "    graph, span_one, span_two, span_one_uid, span_two_uid, n1, n2 = argument_list\n",
    "    feature_tuples = []\n",
    "    \n",
    "    found_in_n_definitions = 0\n",
    "    for s, p, definition in graph.triples((n2[span_two_uid], SKOS.definition, None)):\n",
    "        # for each definition of span_two  # TEXTBLOB WORDS? \n",
    "        if check_for_span_in_definition(span_one, definition):\n",
    "            found_in_n_definitions += 1\n",
    "            feature_tuples.append(irec_related(span_one_uid, span_two_uid, n1, n2))\n",
    "            \n",
    "    if span_constitutes_span(span_one, span_two):\n",
    "        feature_tuples.append(irec_constitutes(span_one_uid, span_two_uid, n1, n2))\n",
    "        \n",
    "    if morphologically_similar(span_one, span_two):\n",
    "        feature_tuples.append(irec_morp_sim(span_one_uid, span_two_uid, n1, n2))\n",
    "        \n",
    "#     if semantically_similar(embedder, span_one, span_two):\n",
    "#         irec_graph = irec_sem_sim(irec_graph, span_one_uid, span_two_uid, n1, n2)\n",
    "        \n",
    "    return feature_tuples, found_in_n_definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89479e2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "29daf2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on: https://example.org/irec-spans# & https://example.org/irec-spans#\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|                                                                                                                                                             | 167/15345 [02:30<3:48:26,  1.11it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/lp/l_mzhpjs6bg95plfkl_n_vsc0000gn/T/ipykernel_59434/2166718391.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconcurrent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfutures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mThreadPoolExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_num_cpu_threads\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mfutures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompute_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margument_combinations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margument_combinations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mlists_of_tuples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_times_found_in_defintions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/lp/l_mzhpjs6bg95plfkl_n_vsc0000gn/T/ipykernel_59434/2166718391.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconcurrent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfutures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mThreadPoolExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_num_cpu_threads\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mfutures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompute_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margument_combinations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margument_combinations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mlists_of_tuples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_times_found_in_defintions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/haystack/lib/python3.9/concurrent/futures/thread.py\u001b[0m in \u001b[0;36msubmit\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_work_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adjust_thread_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0msubmit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/haystack/lib/python3.9/concurrent/futures/thread.py\u001b[0m in \u001b[0;36m_adjust_thread_count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_adjust_thread_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;31m# if idle threads are available, don't spin new threads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_idle_semaphore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/haystack/lib/python3.9/threading.py\u001b[0m in \u001b[0;36macquire\u001b[0;34m(self, blocking, timeout)\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0mrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0mendtime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mblocking\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/haystack/lib/python3.9/threading.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "namespace_combos = [(SPANS, SPANS), (SPANS, CONCEPTS), (SPANS, WIKI), (SPANS, UNICLASS)]\n",
    "max_num_cpu_threads = 8\n",
    "# - what about span-span relations?\n",
    "## TODO; need to think of ways to speed up this process\n",
    "## - reuse embeddings of uniquespans\n",
    "## - think of ways to remove spans that occur very often in definitions (over 30% of definitions makes sense)\n",
    "## - create a method, run concurrent 8 times\n",
    "\n",
    "for n1, n2 in namespace_combos:\n",
    "    print(f\"Working on: {n1} & {n2}\")\n",
    "    # n1 is always SPANS\n",
    "    n1_uid = n1.placeholder.defrag().__reduce__()[1][0]\n",
    "    n2_uid = n2.placeholder.defrag().__reduce__()[1][0]\n",
    "    \n",
    "    for span_one in tqdm(ua.UIDs[n1_uid]):\n",
    "        span_one_uid = ua.UIDs[n1_uid][span_one]\n",
    "\n",
    "        argument_combinations = []\n",
    "        for span_two in ua.UIDs[n2_uid]:\n",
    "            span_two_uid = ua.UIDs[n2_uid][span_two]\n",
    "            argument_combinations.append([irec_graph, span_one, span_two, span_one_uid, span_two_uid, n1, n2])\n",
    "            \n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=max_num_cpu_threads) as executor:\n",
    "            futures = [executor.submit(compute_features, argument_combinations[idx]) for idx in range(len(argument_combinations))]\n",
    "\n",
    "        lists_of_tuples, number_of_times_found_in_defintions = zip(*[f.result() for f in futures])\n",
    "        tuples_to_add = [t[0] for t_list in lists_of_tuples for t in t_list]\n",
    "\n",
    "        if sum(number_of_times_found_in_defintions) > (.30 * total_num_definitions):\n",
    "            print(\"This is not a useful span: \", span_one)\n",
    "            tuples_to_add = [t for t in tuples_to_add if IREC.related not in t]\n",
    "        \n",
    "        irec_graph = add_tuples(irec_graph, tuples_to_add)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf754ed1",
   "metadata": {},
   "source": [
    "#### Save final graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "83372d92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Graph identifier=N54abfd01320444458859d5e0e1c2b6d6 (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irec_graph.serialize(destination=graph_output_fp.joinpath(\"initial_graph.ttl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5b3b39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca28aeaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89337b6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
