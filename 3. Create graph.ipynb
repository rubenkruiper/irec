{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00aa64c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import time\n",
    "import pickle\n",
    "import glob, os\n",
    "import subprocess\n",
    "import json, random\n",
    "import requests, urllib\n",
    "import concurrent.futures\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from tqdm import tqdm\n",
    "from typing import List, Any, List, Dict, Tuple\n",
    "from pathlib import Path\n",
    "from textblob import TextBlob\n",
    "from threading import current_thread\n",
    "from itertools import islice, combinations, product\n",
    "from collections import Counter\n",
    "from nltk.corpus import wordnet as wn\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "from sklearn.neighbors import kneighbors_graph, KNeighborsClassifier\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "from rdflib import URIRef, BNode, Literal, Namespace, Graph\n",
    "from rdflib.namespace import XSD, RDF, RDFS, SKOS, NamespaceManager\n",
    "\n",
    "from utils.spar_utils import TermExtractor\n",
    "from utils.cluster_utils import levenshtein\n",
    "from utils.embedding_utils import Embedder\n",
    "from utils.cleaning_utils import split_list, custom_cleaning_rules, remove_unicode_chars, remove_determiners"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11663171",
   "metadata": {},
   "source": [
    "We are going to create a graph that captures the following features/relations.\n",
    "\n",
    "* Concepts\n",
    "  * We'll use the source as namespace (e.g., WIKI, UNICLASS) and corresponding concept identifier if it exists\n",
    "  * SKOS is used to establish a mapping between a concept and a span (e.g., skos:exactMatch) \n",
    "  * For defined concepts from the Merged Approved Documents we add the definition (skos:definition) to the concept node, for WikiData concepts we add the definitions to the span nodes (irec:wikiDefinition)\n",
    "  \n",
    "  \n",
    "* Special properties that we want to capture between spans, which may help identify concepts (from spans):\n",
    "  * Word or MWE occurs in another MWE \n",
    "  * Morphologically similar words; stemming & Levenshtein distance\n",
    "  * Semantically similar words; distributed similarity (NNs)\n",
    "  * Acronyms\n",
    "  * Related, this is a generic relation, e.g., a `ampere` is related to `electric current`\n",
    "  * Domain-specificity; foreground or background term following our filtering procedure\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7038359e",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_output_fp = Path.cwd().joinpath(\"data\", \"graph_output\")\n",
    "graph_output_fp.mkdir(parents=True, exist_ok=True) # create directory if it doesn't exist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979b0616",
   "metadata": {},
   "source": [
    "## Prepare namespaces and graph creation methods\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "We rely on methods to create the graph triples, in order to make sure a standardised approach is used each time a similar type of triple is added to the graph.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2b60e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = Namespace(\"https://example.org/top_concept_for_visulisation/#\")\n",
    "WIKI = Namespace(\"https://www.wikidata.org/wiki/\")\n",
    "# Note: that UNICLASS is not a namespace (yet) only has identifiers \n",
    "UNICLASS = Namespace(\"https://www.example.org/uniclass/#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4081e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROV = Namespace(\"http://www.w3.org/ns/prov#\")\n",
    "DCT = Namespace(\"http://purl.org/dc/terms/#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "788665e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.wikidata.org/wiki/placeholder'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WIKI.placeholder.defrag().__reduce__()[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "098e92aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example/placeholder URLs for the IReC project \n",
    "IREC_ontology_URL = \"https://schema.irec.org/#\"\n",
    "IREC_spans_URL = \"https://spans.irec.org/#\"\n",
    "IREC_concepts_URL = \"https://concepts.irec.org/#\"\n",
    "\n",
    "# create our custom namespace for the schema to store spans\n",
    "IREC = Namespace(IREC_ontology_URL)\n",
    "\n",
    "# create a custom namespace to store spans and concepts\n",
    "SPANS = Namespace(IREC_spans_URL)\n",
    "CONCEPTS = Namespace(IREC_concepts_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcef6e56",
   "metadata": {},
   "source": [
    "### Graph creation methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2e8ce74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UID_assigner:\n",
    "    def __init__(self):\n",
    "        self.UIDs = {}\n",
    "        self.UID = 0\n",
    "        self.scheme_uids = {}\n",
    "        \n",
    "    def get_scheme_UID(self, namespace: Namespace):\n",
    "        \"\"\"\n",
    "        Determines which type of UID to assign, based on the namespace.\n",
    "        \"\"\"\n",
    "        # not sure if we want to keep this, especially for spans\n",
    "        return [x for x in self.UIDs[namespace._.defrag().__reduce__()[1][0]].values() if x == \"schemeUID\"][0]\n",
    "        \n",
    "    def assign_UID(self, text, namespace: Namespace):\n",
    "        \"\"\"\n",
    "        Determines which type of UID to assign, based on the namespace.\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            raise Exception(\"Not text label provided to assign a UID.\")\n",
    "        if namespace == SPANS:\n",
    "            return self.span_UID(text)\n",
    "        elif namespace == CONCEPTS:\n",
    "            return self.concept_UID(text)\n",
    "        else:\n",
    "            print(\"UID assignment not set up for this namespace, maybe use UID_assigner.keep_track_of_existing_UID()\")\n",
    "            \n",
    "    \n",
    "    def span_UID(self, text):\n",
    "        \"\"\"\n",
    "        NOTE: each text span is a unique identifier in and of itself. We'll simply convert the text span to \n",
    "        a URL friendly representation.\n",
    "        \"\"\"\n",
    "        text = text.strip().encode(\"ascii\", \"ignore\").decode()\n",
    "        n_space = SPANS.placeholder.defrag().__reduce__()[1][0]\n",
    "        if n_space not in self.UIDs:\n",
    "            self.UIDs[n_space] = {}\n",
    "        \n",
    "        urltext = urllib.parse.quote(text)\n",
    "        if text not in self.UIDs[n_space]:\n",
    "            self.UIDs[n_space][text] = urltext\n",
    "            \n",
    "        return self.UIDs[n_space][text]\n",
    "        \n",
    "    def concept_UID(self, text):\n",
    "        \"\"\"\n",
    "        For now I'll create my own dumb interger-based UIDs for nodes as a simple shortcut, split per namespace\n",
    "        \"\"\"\n",
    "        n_space = CONCEPTS.placeholder.defrag().__reduce__()[1][0]\n",
    "        text = text.strip().encode(\"ascii\", \"ignore\").decode()\n",
    "        if n_space not in self.UIDs:\n",
    "            self.UIDs[n_space] = {}\n",
    "        \n",
    "        if text not in self.UIDs[n_space]:\n",
    "            self.UID += 1\n",
    "            self.UIDs[n_space][text] = str(self.UID)\n",
    "            \n",
    "        return self.UIDs[n_space][text]\n",
    "        \n",
    "    def keep_track_of_existing_UID(self, text:str, existing_uid: str, namespace:Namespace):\n",
    "        \"\"\"\n",
    "        Simply keep track of UIDs that exist in the provided namespace.\n",
    "        \"\"\"\n",
    "        text = text.strip()\n",
    "        n_space = namespace.placeholder.defrag().__reduce__()[1][0]\n",
    "        if n_space not in self.UIDs:\n",
    "            self.UIDs[n_space] = {}\n",
    "            \n",
    "        if text not in self.UIDs[n_space]:\n",
    "            # already seen by this UID assigner\n",
    "            self.UIDs[n_space][text] = existing_uid\n",
    "            \n",
    "        return existing_uid\n",
    "    \n",
    "    def retrieve_uid_by_text(self, node_text, namespace: Namespace = SPANS):\n",
    "        n_space = namespace.placeholder.defrag().__reduce__()[1][0]\n",
    "        node_text = node_text.strip()\n",
    "        if node_text in self.UIDs[n_space]:\n",
    "            return self.UIDs[n_space][node_text]\n",
    "        else:\n",
    "            return None \n",
    "        \n",
    "    def count_nodes_in_namespace(self, namespace: Namespace = SPANS):\n",
    "        n_space = namespace.placeholder.defrag().__reduce__()[1][0]\n",
    "        print(f\"Number of nodes in '{n_space}': {len(self.UIDs[n_space])}\")\n",
    "        return len(self.UIDs[n_space])\n",
    "        \n",
    "    def print_node_by_id(self, graph, node_id, namespace: Namespace = SPANS):\n",
    "        for s, p, o in graph.triples((namespace[str(node_id)],  None, None)):\n",
    "            print(f\"{s.split('#')[-1]} ; {p.split('#')[-1]} ; {o.split('#')[-1]}\")\n",
    "        \n",
    "    def print_node_by_text(self, graph, node_text, namespace: Namespace = SPANS):\n",
    "        n_space = namespace.placeholder.defrag().__reduce__()[1][0]\n",
    "        node_text = node_text.strip()\n",
    "        node_id = self.UIDs[n_space][node_text]\n",
    "        # find all triples with subject\n",
    "        for s, p, o in graph.triples((namespace[node_id],  None, None)):\n",
    "            print(f\"{s.split('#')[-1]} ; {p.split('#')[-1]} ; {o.split('#')[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "772b6692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These wrappers only exist to help me consistently add nodes to the graph\n",
    "\n",
    "def dct_title(node_uid: str, title: str, namespace: Namespace) -> List[Tuple]:\n",
    "    return [(namespace[node_uid], DCT.title,  Literal(title.strip(), lang='en'))]\n",
    "\n",
    "def provenance(node_uid: str, source: URIRef, namespace: Namespace) -> List[Tuple]:\n",
    "    \"\"\" temp source attribution \"\"\"\n",
    "    return [(namespace[node_uid], PROV.hadPrimarySource, source)]\n",
    "\n",
    "def prov_agent(node_uid: str, agent: URIRef, namespace: Namespace) -> List[Tuple]:\n",
    "    \"\"\" temp agent attribution (for Spans generated by SPaR.txt) \"\"\"\n",
    "    return [(namespace[node_uid], PROV.wasAttributedTo, agent)]\n",
    "\n",
    "def rdf_type(subject_node_uid, object_node_uid,\n",
    "                subject_namespace: Namespace=SPANS, object_namespace: Namespace=SPANS) -> List[Tuple]:\n",
    "    \"\"\" temp agent attribution (for Spans generated by SPaR.txt) \"\"\"\n",
    "    return [(subject_namespace[subject_node_uid], RDF.type, object_namespace[object_node_uid])]\n",
    "\n",
    "# SKOS \n",
    "def skos_scheme(node_uid, namespace: Namespace=CONCEPTS) -> List[Tuple]:\n",
    "    \"\"\" Node that identifies the concep scheme with a URI, expecting/using as scheme root \"\"\"\n",
    "    return [(namespace[node_uid], RDF.type, SKOS.ConceptScheme)]\n",
    "\n",
    "def skos_top_concept(node_uid, top_concept_uid, \n",
    "                    namespace: Namespace=CONCEPTS, top_concept_namespace: Namespace=CONCEPTS) -> List[Tuple]:\n",
    "    \"\"\" Currently, we mainly use the top-concept for visualisation. \"\"\"\n",
    "    return [(namespace[node_uid], SKOS.hasTopConcept, top_concept_namespace[top_concept_uid]),\n",
    "            (top_concept_namespace[top_concept_uid], SKOS.topConceptOf, namespace[node_uid])]\n",
    "\n",
    "def skos_in_scheme(node_uid, scheme_uid, namespace: Namespace=CONCEPTS, scheme_namespace: Namespace=CONCEPTS) -> List[Tuple]:\n",
    "    \"\"\" Keep track of the scheme/source of a node. \"\"\"\n",
    "    return [(namespace[node_uid], SKOS.inScheme, scheme_namespace[scheme_uid])]\n",
    "\n",
    "def skos_node(node_uid, text, namespace: Namespace=CONCEPTS) -> List[Tuple]:\n",
    "    \"\"\" Add a concept with prefLabel to the graph in the CONCEPTS namespace, of type SKOS.Concept \"\"\"\n",
    "    return [(namespace[node_uid], RDF.type, SKOS.Concept), \n",
    "            (namespace[node_uid], SKOS.prefLabel, Literal(text.strip(), lang='en'))]\n",
    "\n",
    "def skos_prefLabel(node_uid, text, namespace: Namespace=CONCEPTS) -> List[Tuple]:\n",
    "    \"\"\" Add the text label for a node \"\"\"\n",
    "    return [(namespace[node_uid], SKOS.prefLabel, Literal(text.strip(), lang='en'))]\n",
    "\n",
    "def skos_altLabel(node_uid, alt_label_text, namespace: Namespace=CONCEPTS)-> List[Tuple]:\n",
    "    \"\"\" Add an alternative text label for a concept node \"\"\"\n",
    "    return [(namespace[node_uid], SKOS.altLabel, Literal(alt_label_text.strip(), lang='en'))]\n",
    "\n",
    "def skos_exact_match(subject_node_uid, object_node_uid,\n",
    "                subject_namespace: Namespace=SPANS, object_namespace: Namespace=CONCEPTS) -> List[Tuple]:\n",
    "    \"\"\" Denotes an exact match between two nodes, would expect the nodes to be in different vocabularies \"\"\"\n",
    "    return [(subject_namespace[subject_node_uid], SKOS.exactMatch, object_namespace[object_node_uid])]\n",
    "\n",
    "def skos_related(subject_node_uid, object_node_uid,\n",
    "                subject_namespace: Namespace=SPANS, object_namespace: Namespace=CONCEPTS) -> List[Tuple]:\n",
    "    \"\"\" Denotes a relation between two nodes, would expect the nodes to be in different vocabularies \"\"\"\n",
    "    return [(subject_namespace[subject_node_uid], SKOS.related, object_namespace[object_node_uid])]\n",
    "    \n",
    "def skos_broader(narrower_node_uid, broader_node_uid, \n",
    "                 narrower_namespace: Namespace=CONCEPTS, broader_namespace: Namespace=CONCEPTS) -> List[Tuple]:\n",
    "    \"\"\" Assuming narrower/broader is always reflexive, would expect the nodes to be in different vocabularies \"\"\"\n",
    "    return [(broader_namespace[narrower_node_uid], SKOS.narrower, narrower_namespace[broader_node_uid]),\n",
    "            (narrower_namespace[broader_node_uid], SKOS.broader, broader_namespace[narrower_node_uid])]\n",
    "    \n",
    "def skos_definition(node_uid, definition_text, namespace: Namespace=CONCEPTS) -> List[Tuple]:\n",
    "    \"\"\" Add a definition, if provided in the Merged Approved Documents \"\"\"\n",
    "    return [(namespace[node_uid], SKOS.definition, Literal(definition_text.strip(), lang='en'))]\n",
    "\n",
    "def skos_note(node_uid, note_text, namespace: Namespace=CONCEPTS) -> List[Tuple]:\n",
    "    \"\"\" Some notes exist in the approved docs at least, containing useful information \"\"\"\n",
    "    return [(namespace[node_uid], SKOS.note, Literal(note_text.strip(), lang='en'))]\n",
    "\n",
    "\n",
    "\n",
    "# IREC CharacterSpan class and properties (see the IREC.rdf file in data/graph_data/)\n",
    "IREC.CharacterSpan # A span is a sequence of characters that occurs verbatim in a text, either contiguous or discontiguos as extracted by SPaR.txt (Kruiper et al., 2021).   \n",
    "IREC.constitutes  # Indicates that a span constitutes another span, e.g., the Multi-Word Expression (MWE) Span `hot water storage system` the Span `storage`.\n",
    "IREC.isMorphologicallySimilarTo # Indicates that a Span is morphologically similar to another Span, e.g., they may have the same stem or a small Levenshtein distance.\n",
    "IREC.isSemanticallySimilarTo # Indicates that a Span is semantically similar to another Span, following a cosine similarity between their  embeddings.\n",
    "IREC.related # General way to indicate some relation between two spans, e.g., `ampere` is related to `electric current`\n",
    "IREC.definitionRelation # One span occurs in the definition of the other span.\n",
    "IREC.hasAcronym # A Span can have an acronym, e.g., `British Standards Institute` has the acronym `BSI`.\n",
    "IREC.isAcronymOf # A Span can have an acronym, e.g., `BSI` is the acronym for `British Standards Institute`.\n",
    "IREC.hasAntonym # Property that relates a Span to another Span, each being each other's antonyms.\n",
    "IREC.wikiDefinition # One of potentially multiple WikiData definitions for the irec:CharacterSpan node.\n",
    "# notably, wiki class labels are assigned with rdf:type \n",
    "\n",
    "def irec_span(node_uid, text, namespace: Namespace=SPANS) -> List[Tuple]:\n",
    "    \"\"\" Add a span node in the SPANS namespace, of type IREC.CharacterSpan and the span text set as its RDF.label \"\"\"\n",
    "    # is preflabel a property? I would assume so\n",
    "    return [(namespace[node_uid], RDF.type, IREC.CharacterSpan), \n",
    "            (namespace[node_uid], RDFS.label,  Literal(text.strip(), lang='en'))]\n",
    "\n",
    "def irec_constitutes(subject_node_uid, object_node_uid,\n",
    "                     subject_namespace: Namespace=SPANS, object_namespace: Namespace=SPANS) -> List[Tuple]:\n",
    "    \"\"\" Indicates that somewhere in the label of the first SPAN node, you can find the second span's label \"\"\"\n",
    "    return [(subject_namespace[subject_node_uid], IREC.constitutes, object_namespace[object_node_uid])]\n",
    "\n",
    "def irec_morp_sim(subject_node_uid, object_node_uid,\n",
    "                  subject_namespace: Namespace=SPANS, object_namespace: Namespace=SPANS) -> List[Tuple]:\n",
    "    \"\"\" Indicates that the labels of two SPAN nodes are morphologically similar \"\"\"\n",
    "    return [(subject_namespace[subject_node_uid], IREC.isMorphologicallySimilarTo, object_namespace[object_node_uid])]\n",
    "\n",
    "def irec_sem_sim(subject_node_uid, object_node_uid,\n",
    "                 subject_namespace: Namespace=SPANS, object_namespace: Namespace=SPANS) -> List[Tuple]:\n",
    "    \"\"\" Indicates that the labels of two SPAN nodes are semantically similar, following the distributed semantics hypothesis \"\"\"\n",
    "    return [(subject_namespace[subject_node_uid], IREC.isSemanticallySimilarTo, object_namespace[object_node_uid])]\n",
    "\n",
    "def irec_related(subject_node_uid, object_node_uid,\n",
    "                 subject_namespace: Namespace=SPANS, object_namespace: Namespace=SPANS) -> List[Tuple]:\n",
    "    \"\"\" Indicates that a Span is related in SOME way to another Span. \"\"\"\n",
    "    return [(subject_namespace[subject_node_uid], IREC.related, object_namespace[object_node_uid])]\n",
    "\n",
    "def irec_definition_related(subject_node_uid, object_node_uid,\n",
    "                 subject_namespace: Namespace=SPANS, object_namespace: Namespace=SPANS) -> List[Tuple]:\n",
    "    \"\"\" Indicates that a Span is related because it occurs in the definition of another Span. \"\"\"\n",
    "    return [(subject_namespace[subject_node_uid], IREC.definitionRelation, object_namespace[object_node_uid])]\n",
    "\n",
    "def irec_has_acronym(subject_node_uid, object_node_uid,\n",
    "                     subject_namespace: Namespace=SPANS, object_namespace: Namespace=SPANS) -> List[Tuple]:\n",
    "    \"\"\" Indicates that the label of the subject node has an acronym, ergo the label of the object node  \"\"\"\n",
    "    return [(subject_namespace[subject_node_uid], IREC.hasAcronym, object_namespace[object_node_uid]),\n",
    "            (object_namespace[object_node_uid], IREC.isAcronymOf, subject_namespace[subject_node_uid])]\n",
    "\n",
    "def irec_antonym(subject_node_uid, object_node_uid,\n",
    "                 subject_namespace: Namespace=SPANS, object_namespace: Namespace=SPANS) -> List[Tuple]:\n",
    "    \"\"\" Indicates that the label of the subject node is an antonym of the label of the object node  \"\"\"\n",
    "    return [(subject_namespace[subject_node_uid], IREC.hasAntonym, object_namespace[object_node_uid])]\n",
    "\n",
    "def irec_wikidef(node_uid, definition_text, namespace: Namespace=SPANS) -> List[Tuple]:\n",
    "    \"\"\" Add a candidate definition, as retrieved from WikiData for the Span \"\"\"\n",
    "    return [(namespace[node_uid], IREC.wikiDefinition, Literal(definition_text.strip(), lang='en'))]\n",
    "\n",
    "def irec_domain(node_uid, domain_text=\"Out of domain\", namespace: Namespace=SPANS) -> List[Tuple]:\n",
    "    \"\"\" Add a candidate class label, as retrieved from WikiData for the Span \"\"\"\n",
    "    return [(namespace[node_uid], IREC.domain, Literal(domain_text.strip(), lang='en'))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08060b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_tuples(graph, tuples):\n",
    "    \"\"\"\n",
    "    We'll never add the same tuple twice to a graph\n",
    "    \"\"\"\n",
    "    for t in tuples:\n",
    "        assert len(t) == 3\n",
    "    [graph.add(t) for t in tuples if t not in graph]\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e804a54",
   "metadata": {},
   "source": [
    "## Create graph\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "We're going to start with adding the concepts and spans from the Merged Approved Documents, then the Uniclass concepts that occur in the Merged Approved Documents, and then spans then occur in WikiData. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fdc557",
   "metadata": {},
   "source": [
    "### Prepare graph\n",
    "* Currently creating a single graph to hold all information. \n",
    "* Relevant information gathered from external resources is added; primarily class labels and definitions from WikiData."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54a1e1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "irec_graph = Graph()\n",
    "\n",
    "irec_graph.bind(\"root\", ROOT)\n",
    "irec_graph.bind(\"wikipedia\", WIKI)\n",
    "irec_graph.bind(\"uniclass\", UNICLASS)\n",
    "irec_graph.bind(\"dct\", DCT)\n",
    "irec_graph.bind(\"prov\", PROV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56acbeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bind our vocabulary of classes/relations\n",
    "graph_data_fp = Path.cwd().joinpath(\"data\", \"graph_data\")\n",
    "irec_graph.parse(graph_data_fp.joinpath(\"IREC.rdf\"))\n",
    "irec_graph.bind(\"spans\", SPANS)\n",
    "irec_graph.bind(\"concepts\", CONCEPTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ccd4a8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# primary sources and agents\n",
    "irec_IRI = URIRef(\"https://github.com/rubenkruiper/irec\")\n",
    "\n",
    "merged_approved_documents_IRI = URIRef(\"https://www.gov.uk/government/collections/approved-documents\")\n",
    "wikidata_IRI = URIRef(\"https://www.wikidata.org/\")\n",
    "uniclass_IRI = URIRef(\"https://en.wikipedia.org/wiki/Uniclass\")\n",
    "spart_txt_IRI = URIRef(\"http://dx.doi.org/10.18653/v1/2021.nllp-1.14\")\n",
    "\n",
    "irec_graph = add_tuples(irec_graph, \n",
    "                        [\n",
    "                            (irec_IRI, PROV.type, PROV.PrimarySource),\n",
    "                            (merged_approved_documents_IRI, PROV.type, PROV.PrimarySource),\n",
    "                            (wikidata_IRI, PROV.type, PROV.PrimarySource),\n",
    "                            (uniclass_IRI, PROV.type, PROV.PrimarySource),\n",
    "                            (spart_txt_IRI, PROV.type, PROV.SoftwareAgent)       \n",
    "                        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67b6fd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_scheme_uid(graph: Graph, primary_source: URIRef, scheme_name: str, scheme_uid_label:str, namespace: Namespace) -> Graph:\n",
    "    # We'll set the UID ourselves\n",
    "    scheme_uid = ua.keep_track_of_existing_UID(scheme_name, scheme_uid_label, namespace)\n",
    "    # add title\n",
    "    graph = add_tuples(graph, dct_title(scheme_uid, scheme_name, namespace)) \n",
    "    # add source note  \n",
    "    graph = add_tuples(graph, provenance(scheme_uid, primary_source, namespace)) \n",
    "    # is of type skos:ConceptScheme\n",
    "    graph = add_tuples(graph, skos_scheme(scheme_uid, SPANS))\n",
    "    # self-reference being in scheme\n",
    "    graph = add_tuples(graph, skos_in_scheme(scheme_uid, scheme_uid, namespace, namespace))\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbb5024e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ua = UID_assigner()\n",
    "\n",
    "# global UIDs for the schemes we'll be using\n",
    "irec_graph = add_scheme_uid(irec_graph, irec_IRI, \"IREC spans\", \"schemeUID\", SPANS)\n",
    "irec_graph = add_scheme_uid(irec_graph, irec_IRI, \"IREC concepts\", \"schemeUID\", CONCEPTS)\n",
    "\n",
    "# irec_graph = add_scheme_uid(irec_graph, \"IREC WikiData concepts\", \"schemeUID\", WIKI) # NON EXISTENT NODE\n",
    "# irec_graph = add_scheme_uid(irec_graph, \"IREC Uniclass concepts\", \"schemeUID\", UNICLASS)  # NON EXISTENT NODE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41f63e5",
   "metadata": {},
   "source": [
    "### Add domain terms extracted from the Approved documents as Spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8178ad1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4958\n"
     ]
    }
   ],
   "source": [
    "domain_terms = pickle.load(open(graph_data_fp.joinpath('domain_terms.pkl'), 'rb'))\n",
    "print(len(domain_terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2eb28e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Unvented',\n",
       " 'mechanical ventilation systems',\n",
       " 'vent pipe',\n",
       " 'smoke vents',\n",
       " 'venting',\n",
       " 'unventilated',\n",
       " 'mechanical ventilation system',\n",
       " 'air vent',\n",
       " 'ventilation rate',\n",
       " 'ventilated',\n",
       " 'ventilation intakes',\n",
       " 'continuous mechanical extract ventilation',\n",
       " 'ventilation system',\n",
       " 'ventilators',\n",
       " 'Extract ventilation',\n",
       " 'Mechanical ventilation',\n",
       " 'ventilation systems',\n",
       " 'ventilation appliances',\n",
       " 'vents',\n",
       " 'normal background ventilation mode',\n",
       " 'background ventilator',\n",
       " 'Purge ventilation',\n",
       " 'ventilation strategy',\n",
       " 'background ventilators',\n",
       " 'natural ventilation',\n",
       " 'means ventilation',\n",
       " 'ventilation requirements',\n",
       " 'vent',\n",
       " 'mechanical extract ventilation',\n",
       " 'ventilator',\n",
       " 'cross - ventilation',\n",
       " 'ventilation solutions',\n",
       " 'air vents',\n",
       " 'ventilation openings',\n",
       " 'open air vents',\n",
       " 'ventilated discharge stack',\n",
       " 'Background ventilators',\n",
       " 'ventilation ducts',\n",
       " 'single - room heat recovery ventilator',\n",
       " 'ventilation provisions',\n",
       " 'conventions',\n",
       " 'extract ventilation',\n",
       " 'ventilation rates',\n",
       " 'purge ventilation',\n",
       " 'ventilation provision',\n",
       " 'roof venting',\n",
       " 'ventilating pipe',\n",
       " 'ventilator sizes',\n",
       " 'ventilating duct',\n",
       " 'ventilation standards',\n",
       " 'Natural ventilation',\n",
       " 'fixed system ventilation',\n",
       " 'Open vented copper cylinders',\n",
       " 'ventilation opening',\n",
       " 'purge ventilation 1 26',\n",
       " 'mechanical ventilation']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in domain_terms if \"vent\" in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07f4b0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique nr of domain terms: 4958\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['hinge side',\n",
       " 'dwelling 1',\n",
       " 'Furniture',\n",
       " 'thermal bridging',\n",
       " 'U - values',\n",
       " 'pipework insulation',\n",
       " 'outwards',\n",
       " 'offence',\n",
       " 'Planning Listed Buildings',\n",
       " 'minimum core width']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"unique nr of domain terms:\", len(domain_terms))\n",
    "random.sample(domain_terms, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c506907d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simply adding the extracted spans\n",
    "for span in domain_terms:\n",
    "    span_uid = ua.assign_UID(span, SPANS)\n",
    "    \n",
    "    irec_graph = add_tuples(irec_graph, irec_span(span_uid, span))\n",
    "    irec_graph = add_tuples(irec_graph, skos_in_scheme(span_uid, 'schemeUID', SPANS, SPANS))\n",
    "    irec_graph = add_tuples(irec_graph, provenance(span_uid, merged_approved_documents_IRI, SPANS))\n",
    "    # add agent for spans generated by SPaR.txt\n",
    "    irec_graph = add_tuples(irec_graph, prov_agent(span_uid, spart_txt_IRI, SPANS))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8c4c91",
   "metadata": {},
   "source": [
    "### Add Acronyms that were grabbed from the text\n",
    "Even though we don't grab many from the Merged Approved Documents right now, in the future they may help:\n",
    "* remove terms where the SPaR.txt boundary detection is way off\n",
    "* avoid suggesting similar acronyms, e.g., suggest that EPC and EPS are similar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "434b8738",
   "metadata": {},
   "outputs": [],
   "source": [
    "acronyms = pickle.load(open(graph_data_fp.joinpath('acronyms_found_in_text.pkl'), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aaf33be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for acronym, spans in acronyms.items():\n",
    "    \n",
    "    acronym_uid = ua.assign_UID(acronym, SPANS)\n",
    "   \n",
    "    irec_graph = add_tuples(irec_graph, irec_span(acronym_uid, acronym))\n",
    "    irec_graph = add_tuples(irec_graph, skos_in_scheme(acronym_uid, 'schemeUID', SPANS, SPANS))\n",
    "    irec_graph = add_tuples(irec_graph, provenance(acronym_uid, merged_approved_documents_IRI, SPANS))\n",
    "    \n",
    "    for span in spans:\n",
    "        span_uid = ua.assign_UID(span, SPANS) \n",
    "        irec_graph = add_tuples(irec_graph, irec_span(span_uid, span))\n",
    "        irec_graph = add_tuples(irec_graph, skos_in_scheme(span_uid, 'schemeUID', SPANS, SPANS))\n",
    "        irec_graph = add_tuples(irec_graph, provenance(span_uid, merged_approved_documents_IRI, SPANS))\n",
    "\n",
    "        # These spans should have been \n",
    "        \n",
    "        # todo; \n",
    "        #  could do some filtering here of the clearly erroneous span-acronym combinations\n",
    "        #  or leave this until later, using the graph...\n",
    "    \n",
    "        irec_graph = add_tuples(irec_graph, irec_has_acronym(acronym_uid, span_uid))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953c8e57",
   "metadata": {},
   "source": [
    "### Add CONCEPTS: defined terms from the Approved Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1af2368f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from csv file\n",
    "definitions = pd.read_excel(graph_data_fp.joinpath(\"Approved Documents and derived terms.xlsx\"), sheet_name=\"Definitions\", keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77530dc1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Term</th>\n",
       "      <th>Definition</th>\n",
       "      <th>Alternative labels</th>\n",
       "      <th>Note</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Absorption</td>\n",
       "      <td>Conversion of sound energy to heat, often by t...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Absorption coefficient</td>\n",
       "      <td>A quantity characterising the effectiveness of...</td>\n",
       "      <td></td>\n",
       "      <td>See BS EN 20354:1993.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Absorptive material</td>\n",
       "      <td>Material that absorbs sound energy.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Term                                         Definition  \\\n",
       "0              Absorption  Conversion of sound energy to heat, often by t...   \n",
       "1  Absorption coefficient  A quantity characterising the effectiveness of...   \n",
       "2     Absorptive material                Material that absorbs sound energy.   \n",
       "\n",
       "  Alternative labels                   Note  \n",
       "0                                            \n",
       "1                     See BS EN 20354:1993.  \n",
       "2                                            "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "definitions[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f965de82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "299"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(definitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d066368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total defined terms found in spreadsheet:  295\n",
      "Total defined terms and altlabels:  349\n"
     ]
    }
   ],
   "source": [
    "concepts_definitions_dict = {} # keep track of definitions for parsing later\n",
    "concepts = []\n",
    "\n",
    "# create graph from definitions first\n",
    "for i, row in definitions.iloc[1:].iterrows():\n",
    "    # These terms all start with a capital; lowercase them\n",
    "    term = row['Term'].strip() if row['Term'].isupper() else row['Term'].lower().strip()\n",
    "    alternative_labels = row['Alternative labels']\n",
    "    definition = row['Definition']\n",
    "    note = row['Note']\n",
    "    concepts.append(term)\n",
    "\n",
    "    # add the term as a CONCEPT and as a SPAN\n",
    "    concept_uid = ua.assign_UID(term, CONCEPTS)\n",
    "    irec_graph = add_tuples(irec_graph, skos_node(concept_uid, term))\n",
    "    irec_graph = add_tuples(irec_graph, skos_in_scheme(concept_uid, 'schemeUID', CONCEPTS, CONCEPTS))\n",
    "    irec_graph = add_tuples(irec_graph, provenance(concept_uid, merged_approved_documents_IRI, CONCEPTS))\n",
    "    \n",
    "    span_uid = ua.assign_UID(term, SPANS)\n",
    "    irec_graph = add_tuples(irec_graph, irec_span(span_uid, term))\n",
    "    irec_graph = add_tuples(irec_graph, skos_in_scheme(span_uid, 'schemeUID', SPANS, SPANS))\n",
    "    irec_graph = add_tuples(irec_graph, provenance(span_uid, merged_approved_documents_IRI, SPANS))\n",
    "    \n",
    "    # link the concept and the span # as a skos:exactMatch? or smt else?\n",
    "    irec_graph = add_tuples(irec_graph, skos_exact_match(concept_uid, span_uid, CONCEPTS, SPANS))\n",
    "    \n",
    "    # always expecting a definition\n",
    "    irec_graph = add_tuples(irec_graph, skos_definition(concept_uid, definition))\n",
    "    \n",
    "    if note: \n",
    "        irec_graph = add_tuples(irec_graph, skos_note(concept_uid, note))\n",
    "    \n",
    "    if alternative_labels:\n",
    "        # These terms all start with a capital; lowercase if not an acronym\n",
    "        alt_labels = [x.strip() if x.isupper() else x.lower().strip() for x in alternative_labels.split(\", \")]  \n",
    "        \n",
    "        for alt_label in alt_labels:\n",
    "            if not alt_label:\n",
    "                continue\n",
    "                \n",
    "            concepts.append(alt_label)\n",
    "            \n",
    "            # add the altlabel to the concept node\n",
    "            irec_graph = add_tuples(irec_graph, skos_altLabel(concept_uid, alt_label))\n",
    "            \n",
    "            # also add as a span\n",
    "            alt_label_span_uid = ua.assign_UID(alt_label, SPANS)\n",
    "            irec_graph = add_tuples(irec_graph, irec_span(alt_label_span_uid, alt_label))\n",
    "            irec_graph = add_tuples(irec_graph, skos_in_scheme(alt_label_span_uid, 'schemeUID', SPANS, SPANS))\n",
    "            irec_graph = add_tuples(irec_graph, provenance(alt_label_span_uid, merged_approved_documents_IRI, SPANS))\n",
    "            \n",
    "            # link the concept to the altlabel span\n",
    "            irec_graph = add_tuples(irec_graph, skos_exact_match(concept_uid, alt_label_span_uid, CONCEPTS, SPANS))\n",
    "    \n",
    "    if concept_uid not in concepts_definitions_dict: \n",
    "        concepts_definitions_dict[concept_uid] = [{'prefLabel': term, 'definition': definition, 'note': note}]  \n",
    "    else:\n",
    "        concepts_definitions_dict[concept_uid].append({'prefLabel': term, 'definition': definition, 'note': note})  \n",
    "                                                      \n",
    "    \n",
    "print(\"Total defined terms found in spreadsheet: \", len(list(set(concepts_definitions_dict))))    \n",
    "print(\"Total defined terms and altlabels: \", len(list(set(concepts))))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86a2f6c",
   "metadata": {},
   "source": [
    "### Add SPANS: glossary/index terms from the Approved Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "92cbdde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_terms = pd.read_excel(graph_data_fp.joinpath(\"Approved Documents and derived terms.xlsx\"), sheet_name=\"Index terms\", keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "42ddb6c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Term</th>\n",
       "      <th>AltLabel(s)</th>\n",
       "      <th>Related terms</th>\n",
       "      <th>Broader term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abbreviated eaves</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>eaves</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Access floors</td>\n",
       "      <td>access floor</td>\n",
       "      <td>Platform floors</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Access for fire service</td>\n",
       "      <td>fire access</td>\n",
       "      <td></td>\n",
       "      <td>Fire service facilities</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Term   AltLabel(s)    Related terms  \\\n",
       "0        abbreviated eaves                                  \n",
       "1            Access floors  access floor  Platform floors   \n",
       "2  Access for fire service   fire access                    \n",
       "\n",
       "              Broader term  \n",
       "0                    eaves  \n",
       "1                           \n",
       "2  Fire service facilities  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_terms[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843da834",
   "metadata": {},
   "source": [
    "* add triples from index terms / glossaries; we will treat these terms as SPANS\n",
    "* some of these terms were added manually on top of the index terms found in the Mergeds Approved documents, so we'll avoid adding the provenance relation to these\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f1358bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total index terms found in spreadsheet:  1396\n"
     ]
    }
   ],
   "source": [
    "all_index_terms = []\n",
    "for i, row in index_terms.iloc[1:].iterrows():\n",
    "    # Many of these terms start with a capital; lowercase them\n",
    "    term = row['Term'].strip() if row['Term'].isupper() else row['Term'].lower().strip()\n",
    "    alternative_labels = row['AltLabel(s)']\n",
    "    related_terms = row['Related terms']\n",
    "    broader_term = row['Broader term']\n",
    "    \n",
    "    # add the term as a SPAN only\n",
    "    span_uid = ua.assign_UID(term, SPANS)\n",
    "    irec_graph = add_tuples(irec_graph, irec_span(span_uid, term))\n",
    "    irec_graph = add_tuples(irec_graph, skos_in_scheme(span_uid, 'schemeUID', SPANS, SPANS))\n",
    "#     irec_graph = add_tuples(irec_graph, provenance(span_uid, merged_approved_documents_IRI, SPANS))\n",
    "    all_index_terms.append(term)\n",
    "        \n",
    "    if alternative_labels:\n",
    "        # Many of these terms start with a capital; lowercase them\n",
    "        alt_labels = [x.strip() if x.isupper() else x.lower().strip() for x in alternative_labels.split(\", \")]  \n",
    "        \n",
    "        for alt_label in alt_labels:\n",
    "            if not alt_label:\n",
    "                continue\n",
    "            # add alt-label as a span only (as well)\n",
    "            alt_label_uid = ua.assign_UID(alt_label, SPANS)\n",
    "            irec_graph = add_tuples(irec_graph, irec_span(alt_label_uid, alt_label))\n",
    "            irec_graph = add_tuples(irec_graph, skos_in_scheme(alt_label_uid, 'schemeUID', SPANS, SPANS))\n",
    "#             irec_graph = add_tuples(irec_graph, provenance(alt_label_uid, merged_approved_documents_IRI, SPANS))\n",
    "            all_index_terms.append(alt_label)\n",
    "            \n",
    "            if alt_label.isupper():\n",
    "                # there are acronyms among the alternative labels\n",
    "                irec_graph = add_tuples(irec_graph, irec_has_acronym(span_uid, alt_label_uid))\n",
    "            else:\n",
    "                ### Should I use skos altlabels between spans? maybe create IREC alternative label?\n",
    "                ### Should I use skos altlabels between spans? maybe create IREC alternative label?\n",
    "                ### Should I use skos altlabels between spans? maybe create IREC alternative label?\n",
    "                irec_graph = add_tuples(irec_graph, skos_altLabel(span_uid, alt_label_uid))\n",
    "                \n",
    "\n",
    "    if related_terms:\n",
    "        # Many of these terms start with a capital; lowercase them\n",
    "        rel_terms = [x.strip() if x.isupper() else x.lower().strip() for x in related_terms.split(\", \")]\n",
    "        for rel_term in rel_terms:\n",
    "            if not rel_term:\n",
    "                continue\n",
    "            # add related terms as a span (as well)\n",
    "            related_uid = ua.assign_UID(rel_term, SPANS)\n",
    "            irec_graph = add_tuples(irec_graph, irec_span(related_uid, rel_term))\n",
    "            irec_graph = add_tuples(irec_graph, skos_in_scheme(related_uid, 'schemeUID', SPANS, SPANS))\n",
    "#             irec_graph = add_tuples(irec_graph, provenance(related_uid, merged_approved_documents_IRI, SPANS))\n",
    "            all_index_terms.append(rel_term)\n",
    "            \n",
    "            if rel_term.isupper():\n",
    "                # there are acronyms among the related labels as well\n",
    "                irec_graph = add_tuples(irec_graph, irec_has_acronym(span_uid, related_uid))\n",
    "            else:\n",
    "                irec_graph = add_tuples(irec_graph, irec_related(span_uid, related_uid)) \n",
    "    \n",
    "    if broader_term:\n",
    "        # We do not expect that the broader term is necessarily a concept.\n",
    "        # Currently, it is simply a feature for future reference.\n",
    "        # We expect 1 broader term at most, assuming the final conceptualisation would\n",
    "        # be structured like a tree (Directed Acyclic Graph with 1 parent at most).\n",
    "        b_term = broader_term.strip().lower() if not broader_term.isupper() else broader_term.strip()\n",
    "        # also broader term as a span\n",
    "        b_term_uid = ua.assign_UID(b_term, SPANS)\n",
    "        irec_graph = add_tuples(irec_graph, irec_span(b_term_uid, b_term)) \n",
    "        irec_graph = add_tuples(irec_graph, skos_in_scheme(b_term_uid, 'schemeUID', SPANS, SPANS))\n",
    "#         irec_graph = add_tuples(irec_graph, provenance(b_term_uid, merged_approved_documents_IRI, SPANS))\n",
    "        all_index_terms.append(broader_term)\n",
    "        \n",
    "        ### Should I use skos broader between spans? maybe create an IREC broader?\n",
    "        irec_graph = add_tuples(irec_graph, skos_broader(span_uid, b_term_uid, SPANS, SPANS)) \n",
    "\n",
    "print(\"Total index terms found in spreadsheet: \", len(list(set(all_index_terms))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3916d3",
   "metadata": {},
   "source": [
    "* Check overlap between SPaR.txt output and the concepts found in the Merged Approved documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dd251711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of concepts also found by SPaR.txt: 225 / 349 (64.47%)\n"
     ]
    }
   ],
   "source": [
    "concepts = [t.lower() for t in list(set(concepts))]\n",
    "overlapping = [t for t in domain_terms if t.lower() in concepts]\n",
    "print(\"Number of concepts also found by SPaR.txt: {} / {} ({:.2f}%)\".format(len(overlapping), len(concepts), \n",
    "                                                                            len(overlapping)/len(concepts) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73491dc7",
   "metadata": {},
   "source": [
    "* Check overlap between SPaR.txt output, concepts and index terms \n",
    "  * Note, not all of the index terms are taken from the merged approved documents! For example, some are from our manually created KG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8169e3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of index terms also found by SPaR.txt: 525 / 1396 (37.61%)\n"
     ]
    }
   ],
   "source": [
    "index_terms = [t.lower() for t in list(set(all_index_terms))]\n",
    "overlapping = [t for t in domain_terms if t.lower() in index_terms]\n",
    "print(\"Number of index terms also found by SPaR.txt: {} / {} ({:.2f}%)\".format(len(overlapping), len(index_terms), \n",
    "                                                                            len(overlapping)/len(index_terms) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9e538a",
   "metadata": {},
   "source": [
    "* Save the graph so far; which contains terms from the Merged Approved documents only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "40952822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Graph identifier=Ne8b1118113f2479e8db1a9575cc50a79 (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irec_graph.serialize(destination=graph_output_fp.joinpath(\"approved_doc_terms_only.ttl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fbf590",
   "metadata": {},
   "source": [
    "### Print some insight in the graph so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "14daa5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes in 'https://spans.irec.org/': 6099\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6099"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ua.count_nodes_in_namespace(SPANS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "204cfd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes in 'https://concepts.irec.org/': 296\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "296"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ua.count_nodes_in_namespace(CONCEPTS) # extra node for the scheme UID (may want to change this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bdfeb278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233 ; type ; Concept\n",
      "233 ; prefLabel ; sanitary accommodation\n",
      "233 ; inScheme ; schemeUID\n",
      "233 ; hadPrimarySource ; https://www.gov.uk/government/collections/approved-documents\n",
      "233 ; exactMatch ; sanitary%20accommodation\n",
      "233 ; definition ; A space containing one or more water closets or urinals, whether or not it also contains other sanitary appliances. Sanitary accommodation containing one or  more cubicles counts as a single space if there is free circulation of air throughout the space.\n"
     ]
    }
   ],
   "source": [
    "ua.print_node_by_text(irec_graph, 'sanitary accommodation', CONCEPTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e6ad1b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sanitary%20accommodation ; type ; CharacterSpan\n",
      "sanitary%20accommodation ; label ; sanitary accommodation\n",
      "sanitary%20accommodation ; inScheme ; schemeUID\n",
      "sanitary%20accommodation ; hadPrimarySource ; https://www.gov.uk/government/collections/approved-documents\n",
      "sanitary%20accommodation ; wasAttributedTo ; http://dx.doi.org/10.18653/v1/2021.nllp-1.14\n",
      "sanitary%20accommodation ; related ; sanitary%20appliance\n"
     ]
    }
   ],
   "source": [
    "ua.print_node_by_text(irec_graph, 'sanitary accommodation', SPANS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4172c596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wet%20room ; type ; CharacterSpan\n",
      "wet%20room ; label ; wet room\n",
      "wet%20room ; inScheme ; schemeUID\n",
      "wet%20room ; hadPrimarySource ; https://www.gov.uk/government/collections/approved-documents\n",
      "wet%20room ; wasAttributedTo ; http://dx.doi.org/10.18653/v1/2021.nllp-1.14\n"
     ]
    }
   ],
   "source": [
    "ua.print_node_by_id(irec_graph, urllib.parse.quote('wet room'), SPANS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1ddcb895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "290 ; type ; Concept\n",
      "290 ; prefLabel ; wet room\n",
      "290 ; inScheme ; schemeUID\n",
      "290 ; hadPrimarySource ; https://www.gov.uk/government/collections/approved-documents\n",
      "290 ; exactMatch ; wet%20room\n",
      "290 ; definition ; A room used for domestic activities (such as cooking, clothes washing and bathing) which give rise to significant production of airborne moisture, e.g. a kitchen, utility room or bathroom.\n",
      "290 ; definition ; WC or bathroom compartment with tanking and drainage laid to fall to a connected gulley capable of draining the floor area when used as a shower.\n",
      "290 ; note ; For the purposes of Part F, sanitary accommodation is also regarded as a wet room.\n"
     ]
    }
   ],
   "source": [
    "ua.print_node_by_text(irec_graph, 'wet room', CONCEPTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1989fb60",
   "metadata": {},
   "source": [
    "As you can see in the examples above, the concept `wet room` and the span `sanitary accommodation` should be related:\n",
    "* The concept `wet room` is provided with a note in the merged approved document; indicating that for part F of the approved documents, `sanitary accommodation` is regarded as a `wet room`. \n",
    "\n",
    "Based on the above, we'd like to link the span `sanitary accomodation` to the concept `wet room`. While we could parse the note in more detail, and identify that a `skos:altLabel` relation should be added, we'll use a more generic approach:\n",
    "* Any span that is found inside a definition or note of a concept will be linked through `irec:definitionRelation`\n",
    "* Based on the definitions above, potential spans related to the `wet room` concept then become: `sanitary accomdoation`, `airborn moisture`, `kitchen`, `utility room`, `bathroom`, `WC`, `tanking`, `drainage`, `gulley`, `shower`.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "We believe that the types of relations described above can be valuable and would like to provide more definitions for more terms, to help interrelate more spans and concepts. To this end, we first try to find WikiData definitions for all concepts and spans. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f99770a",
   "metadata": {},
   "source": [
    "### We add the Uniclass terms that we found in the text to the graph\n",
    "* Before adding WikiData definitions, we'll add the Uniclass spans we found in the Merged Approved Documents. This will add some additional spans and possibly definitions to the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "82fe7391",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(graph_data_fp.joinpath(\"uniclass_terms_in_text.pkl\"), 'rb') as f:\n",
    "    uniclass_terms_in_text = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1076ed68",
   "metadata": {},
   "outputs": [],
   "source": [
    "for uniclass_uid, definition_dict in uniclass_terms_in_text.items():\n",
    "    # Add the Uniclass node to our graph\n",
    "    uniclass_term = definition_dict['pref_label']\n",
    "    # keep track of uid that is added to the graph\n",
    "    _ = ua.keep_track_of_existing_UID(uniclass_term, uniclass_uid, UNICLASS)\n",
    "    \n",
    "    # add the concept to the graph, in UNICLASS namespace\n",
    "    irec_graph = add_tuples(irec_graph, skos_node(uniclass_uid, uniclass_term, UNICLASS))\n",
    "    # irec_graph = add_tuples(irec_graph, skos_in_scheme(uniclass_uid, 'schemeUID', UNICLASS, UNICLASS))\n",
    "    irec_graph = add_tuples(irec_graph, provenance(uniclass_uid, uniclass_IRI, UNICLASS))\n",
    "    \n",
    "    # Determine or create the corresponding term_uid in SPANS and add a # skos:exactMatch?\n",
    "    if ua.retrieve_uid_by_text(uniclass_term): # First as is (no lowercasing, despite Uniclass casing)\n",
    "        # Add an exact match between the Uniclass node and the corresponding span\n",
    "        span_uid = ua.retrieve_uid_by_text(uniclass_term)\n",
    "        irec_graph = add_tuples(irec_graph, skos_exact_match(span_uid, uniclass_uid, SPANS, UNICLASS))\n",
    "    elif ua.retrieve_uid_by_text(uniclass_term.lower()):\n",
    "        # Add an exact match between the wiki node and the corresponding lowercased version in SPANS\n",
    "        span_uid = ua.retrieve_uid_by_text(uniclass_term.lower())\n",
    "        irec_graph = add_tuples(irec_graph, skos_exact_match(span_uid, uniclass_uid, SPANS, UNICLASS))\n",
    "    else:\n",
    "        # Although the uniclass term was found in the text, no exact matching span was extracted by SPaR.txt\n",
    "        # add the term (defined concept) as a span \n",
    "        term_uid = ua.assign_UID(uniclass_term, SPANS)\n",
    "        irec_graph = add_tuples(irec_graph, irec_span(term_uid, uniclass_term))\n",
    "        irec_graph = add_tuples(irec_graph, skos_in_scheme(term_uid, 'schemeUID', SPANS, SPANS))\n",
    "        irec_graph = add_tuples(irec_graph, provenance(term_uid, uniclass_IRI, SPANS))\n",
    "\n",
    "        irec_graph = add_tuples(irec_graph, skos_exact_match(term_uid, uniclass_uid, SPANS, UNICLASS))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407358e6",
   "metadata": {},
   "source": [
    "### Grab wikipedia definitions for Concept nodes, and store locally for re-use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05db9542",
   "metadata": {},
   "source": [
    "* First, we try to grab all wiki definitions for all spans and concepts that are in the graph (so far)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f2e41964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the SPARQL endpoint for wikidata\n",
    "sparql_wrapper = SPARQLWrapper(\"https://query.wikidata.org/sparql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e9e280",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wiki_matches(graph_sparql_endpoint: SPARQLWrapper,\n",
    "                     jargon_term_and_uids: List):\n",
    "\n",
    "    all_wiki_definitions = {}\n",
    "    # we want to grab the term (subject), any definition (subjectDescription) and the class (subjectClass)\n",
    "    sparql_q = \"\"\"\n",
    "               SELECT DISTINCT ?subject ?subjectDescription ?classUID ?className WHERE {\n",
    "                  ?subject rdfs:label \"QUERY\"@en.\n",
    "                  ?subject wdt:P31|wdt:P279 ?classUID.\n",
    "                  SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\".}\n",
    "                  ?classUID  rdfs:label ?className  FILTER(LANG(?className) = \"en\").\n",
    "                }\n",
    "               \"\"\"\n",
    "    \n",
    "    for term, uid in tqdm(jargon_term_and_uids):\n",
    "        # make the call to \n",
    "        temp_q = sparql_q.replace(\"QUERY\", term)\n",
    "        graph_sparql_endpoint.setQuery(temp_q)\n",
    "        graph_sparql_endpoint.setReturnFormat(JSON)\n",
    "        try:\n",
    "            json_output = graph_sparql_endpoint.query().convert()\n",
    "        except:\n",
    "            # If no result, wait a few seconds; One client is allowed 30 error queries per minute\n",
    "            print(f\"Error for query, may want to check what's wrong with the term: {term}\")\n",
    "            time.sleep(3)\n",
    "            continue\n",
    "            \n",
    "        # sometimes multiple Wiki UIDs for a single term, we grab them all here\n",
    "        bindings = [v for v in json_output['results']['bindings']]\n",
    "            \n",
    "\n",
    "        for v in bindings:\n",
    "            class_uid = v['classUID']['value'] if 'classUID' in v else \"\"\n",
    "            class_label = v['className']['value'] if 'className' in v else \"\"\n",
    "            \n",
    "            if 'subjectDescription' in v:\n",
    "                if uid not in all_wiki_definitions:\n",
    "                    all_wiki_definitions[uid] = [{'prefLabel': term,\n",
    "                                                  'class_uid': class_uid,\n",
    "                                                  'class_label': class_label,\n",
    "                                                  'WikiUID': v['subject']['value'],\n",
    "                                                  'WikiDefinition': v['subjectDescription']['value']}]\n",
    "                else:\n",
    "                    all_wiki_definitions[uid].append({'prefLabel': term,\n",
    "                                                      'class_uid': class_uid,\n",
    "                                                      'class_label': class_label,\n",
    "                                                      'WikiUID': v['subject']['value'],\n",
    "                                                      'WikiDefinition': v['subjectDescription']['value']})\n",
    "            elif 'subject' in v:\n",
    "                # no description found, simply adding wiki UID if that exists\n",
    "                if uid not in all_wiki_definitions:\n",
    "                    all_wiki_definitions[uid] = [{'prefLabel': term,\n",
    "                                                  'class_uid': class_uid,\n",
    "                                                  'class_label': class_label,\n",
    "                                                  'WikiUID': v['subject']['value']}]\n",
    "                else:\n",
    "                    all_wiki_definitions[uid].append({'prefLabel': term,\n",
    "                                                      'class_uid': class_uid,\n",
    "                                                      'class_label': class_label,\n",
    "                                                      'WikiUID': v['subject']['value']})\n",
    "    return all_wiki_definitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6b57bf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "concepts_and_uids = [(k, v) for k, v in ua.UIDs[CONCEPTS.placeholder.defrag().__reduce__()[1][0]].items()]\n",
    "spans_and_uids = [(k, v) for k, v in ua.UIDs[SPANS.placeholder.defrag().__reduce__()[1][0]].items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3f9b3c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 296/296 [01:04<00:00,  4.61it/s]\n"
     ]
    }
   ],
   "source": [
    "# First run for the Concepts\n",
    "concept_wiki_dict_fp = graph_output_fp.joinpath(\"concept_wiki_dict.json\")\n",
    "if not concept_wiki_dict_fp.exists():\n",
    "    concept_wiki_dict = get_wiki_matches(sparql_wrapper, concepts_and_uids)\n",
    "    with open(concept_wiki_dict_fp, 'w') as f:\n",
    "        json.dump(concept_wiki_dict, f, indent=2)\n",
    "else:\n",
    "    with open(concept_wiki_dict_fp, 'r') as f:\n",
    "        concept_wiki_dict = json.load(f)\n",
    "    \n",
    "missing_terms = [(k, uid) for k, uid in spans_and_uids if uid not in concept_wiki_dict]\n",
    "if missing_terms:\n",
    "    additional_terms_dict = get_wiki_matches(sparql_wrapper, missing_terms)\n",
    "    concept_wiki_dict.update(additional_terms_dict)\n",
    "    with open(concept_wiki_dict_fp, 'w') as f:\n",
    "        json.dump(concept_wiki_dict, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4f70afcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|█████████████████████████████████████████████████████▌                                                                                                           | 1548/4650 [05:09<09:53,  5.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for query, may want to check what's wrong with the term: litres / sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|██████████████████████████████████████████████████████                                                                                                           | 1562/4650 [05:14<10:11,  5.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for query, may want to check what's wrong with the term: cable routes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|██████████████████████████████████████████████████████▌                                                                                                          | 1576/4650 [05:18<10:09,  5.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for query, may want to check what's wrong with the term: fi e spread\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|██████████████████████████████████████████████████████▋                                                                                                          | 1579/4650 [05:21<24:46,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for query, may want to check what's wrong with the term: hot smoke\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|██████████████████████████████████████████████████████▉                                                                                                          | 1585/4650 [05:24<16:23,  3.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for query, may want to check what's wrong with the term: th January 2013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4650/4650 [15:50<00:00,  4.89it/s]\n"
     ]
    }
   ],
   "source": [
    "# Now run for the spans (those in the graph at this moment ~ before parsing definitions)\n",
    "span_wiki_dict_fp = graph_output_fp.joinpath(\"span_wiki_dict.json\")\n",
    "if not span_wiki_dict_fp.exists():\n",
    "    span_wiki_dict = get_wiki_matches(sparql_wrapper, spans_and_uids)\n",
    "    with open(span_wiki_dict_fp, 'w') as f:\n",
    "        json.dump(span_wiki_dict, f, indent=2)\n",
    "else:\n",
    "    with open(span_wiki_dict_fp, 'r') as f:\n",
    "        span_wiki_dict = json.load(f)\n",
    "\n",
    "# We may have run into some sparql endpoint errors, which we'd like to try and resolve.\n",
    "missing_terms = [(k, uid) for k, uid in spans_and_uids if uid not in span_wiki_dict]\n",
    "if missing_terms:\n",
    "    additional_terms_dict = get_wiki_matches(sparql_wrapper, missing_terms)\n",
    "    span_wiki_dict.update(additional_terms_dict)\n",
    "    with open(span_wiki_dict_fp, 'w') as f:\n",
    "        json.dump(span_wiki_dict, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "10f67b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes in 'https://concepts.irec.org/': 296\n",
      "Number of concepts with WikiData definitions: 75 (25.34%)\n",
      "Number of nodes in 'https://spans.irec.org/': 6500\n",
      "Number of spans with WikiData definitions: 1857 (28.57%)\n"
     ]
    }
   ],
   "source": [
    "# expected; 93 out of 350 concepts and 1398 out of 3917 spans\n",
    "print(\"Number of concepts with WikiData definitions: {} ({:.2f}%)\".format(len(concept_wiki_dict), len(concept_wiki_dict)/ua.count_nodes_in_namespace(CONCEPTS)*100))\n",
    "print(\"Number of spans with WikiData definitions: {} ({:.2f}%)\".format(len(span_wiki_dict), len(span_wiki_dict)/ua.count_nodes_in_namespace(SPANS)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699c6acb",
   "metadata": {},
   "source": [
    "### Only keep WikiData definitions that belong to classes that we've annotated\n",
    "* We have previously annotated the relevance of all WikiData classes returned for the defined terms and index terms in the Approved Documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c5aa0969",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_wikidata_classes_df = pd.read_csv(graph_data_fp.joinpath(\"wiki_classes_annotated.csv\"), index_col=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c66f2893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WikiData class</th>\n",
       "      <th>Annotation</th>\n",
       "      <th>Example spans</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WikiData UIDs</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>['Q107715']</th>\n",
       "      <td>physical quantity</td>\n",
       "      <td>y</td>\n",
       "      <td>['sound pressure level', 'density', 'area', 's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>['Q82799']</th>\n",
       "      <td>name</td>\n",
       "      <td>n</td>\n",
       "      <td>['access point']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>['Q180160']</th>\n",
       "      <td>metadata</td>\n",
       "      <td>n</td>\n",
       "      <td>['access point']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  WikiData class Annotation  \\\n",
       "WikiData UIDs                                 \n",
       "['Q107715']    physical quantity          y   \n",
       "['Q82799']                  name          n   \n",
       "['Q180160']             metadata          n   \n",
       "\n",
       "                                                   Example spans  \n",
       "WikiData UIDs                                                     \n",
       "['Q107715']    ['sound pressure level', 'density', 'area', 's...  \n",
       "['Q82799']                                      ['access point']  \n",
       "['Q180160']                                     ['access point']  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotated_wikidata_classes_df[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "075f88f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes annotated as relevant: 558/1220(45.74%)\n"
     ]
    }
   ],
   "source": [
    "num_relevant = len([a for a in annotated_wikidata_classes_df.Annotation if a == 'y'])\n",
    "total = len(annotated_wikidata_classes_df.Annotation)\n",
    "print(\"Number of classes annotated as relevant: {}/{}({:.2f}%)\".format(num_relevant, total, num_relevant/total*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1f587141",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikiclass_dict = {}\n",
    "for row in annotated_wikidata_classes_df.iterrows():\n",
    "    uid_list_string, class_annotations_examples = row\n",
    "    uid_list = uid_list_string[2:-2].split(',')\n",
    "    for uid in uid_list:\n",
    "        wikiclass_dict[uid] = {\n",
    "            'Class': class_annotations_examples['WikiData class'],\n",
    "            'Annotation': class_annotations_examples['Annotation'],\n",
    "            'Example spans': class_annotations_examples['Example spans']\n",
    "        }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "dde9e8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_wikidata_classes(wiki_class_dict, term_dict):\n",
    "    new_term_dict = {}\n",
    "    removed_definitions = []\n",
    "    for uid, definition_dict_list in term_dict.items():\n",
    "        for definition_dict in definition_dict_list:\n",
    "            class_uid = definition_dict['class_uid'].rsplit(\"/\", 1)[1]\n",
    "            if class_uid in wikiclass_dict:\n",
    "                class_name = wikiclass_dict[class_uid][\"Class\"]\n",
    "                if wikiclass_dict[class_uid][\"Annotation\"] == 'y':\n",
    "                    if uid not in new_term_dict:\n",
    "                        new_term_dict[uid] = [definition_dict]\n",
    "                    else:\n",
    "                        new_term_dict[uid].append(definition_dict)\n",
    "                else:\n",
    "                    removed_definitions.append(definition_dict)\n",
    "                        \n",
    "    return new_term_dict, removed_definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e49d20d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_wiki_dict, removed_definitions = filter_wikidata_classes(wikiclass_dict, concept_wiki_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "78aefda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "span_wiki_dict, removed_definitions = filter_wikidata_classes(wikiclass_dict, span_wiki_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "54afca17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 14057 definitions, as the corresponding classifications seemed to be out of domain.\n",
      "Number of nodes in 'https://spans.irec.org/': 6500\n",
      "825 (12.69%) spans with definitions/labels left.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Removed {len(removed_definitions)} definitions, as the corresponding classifications seemed to be out of domain.\")\n",
    "print(\"{} ({:.2f}%) spans with definitions/labels left.\".format(len(span_wiki_dict), len(span_wiki_dict)/ua.count_nodes_in_namespace(SPANS)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fae49f",
   "metadata": {},
   "source": [
    "### Parse all definitions (including WikiData) to identify additional spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bfdd2f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_spar_labels(input_dict: Dict[str, str], term_extractor: TermExtractor):\n",
    "    \"\"\"\n",
    "    Identify which spans occur in the definitions\n",
    "    \"\"\"\n",
    "    number_of_definitions = 0\n",
    "    for uid, definition_dict_list in tqdm(input_dict.items()):\n",
    "        for idx, definition_dict in enumerate(definition_dict_list):\n",
    "#             if 'Spans in definitions and notes' in definition_dict:\n",
    "#                 # spans already computed for this definition_dict, continuing to check next\n",
    "#                 continue\n",
    "            \n",
    "            spartxt_objects = []\n",
    "            for k, v in definition_dict.items():\n",
    "                if k in ['WikiDefinition', 'definition', 'note'] and v != '':\n",
    "                    to_be_parsed = definition_dict[k]\n",
    "                    number_of_definitions += 1\n",
    "                    sentences = term_extractor.split_into_sentences(to_be_parsed)\n",
    "                    # cleaning spans as well;\n",
    "                    sentences = [remove_unicode_chars(s).encode(\"ascii\", \"ignore\").decode() for s in sentences]\n",
    "                    spartxt_objects += custom_cleaning_rules(term_extractor.process_sentences(sentences))\n",
    "                    \n",
    "            input_dict[uid][idx]['Spans in definitions and notes'] = spartxt_objects\n",
    "    print(f\"Processed {number_of_definitions} definitions\")\n",
    "    return input_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdb4013",
   "metadata": {},
   "source": [
    "* Parse the definitions of concepts manually taken from the Approved Documents (not all concepts are defined, e.g., altLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "46bf46cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing SPaR.txt objects for concepts_definitions_dict\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 295/295 [00:38<00:00,  7.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 334 definitions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "concepts_definitions_dict_fp = graph_output_fp.joinpath(\"concepts_definitions_dict.json\")\n",
    "termextractor = None\n",
    "if not concepts_definitions_dict_fp.exists():\n",
    "    if not termextractor:\n",
    "        # instantiate a TermExtractor\n",
    "        termextractor = TermExtractor(max_num_cpu_threads=4)\n",
    "    \n",
    "    print(\"Computing SPaR.txt objects for concepts_definitions_dict\")\n",
    "    concepts_definitions_dict = add_spar_labels(concepts_definitions_dict, termextractor)\n",
    "    with open(concepts_definitions_dict_fp, 'w') as f:\n",
    "        json.dump(concepts_definitions_dict, f, indent=2)\n",
    "else:\n",
    "    print(\"Loading previously computed concepts_definitions_dict with SPaR.txt objects from file\")\n",
    "    with open(concepts_definitions_dict_fp, 'r') as f:\n",
    "        concepts_definitions_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bdd48f",
   "metadata": {},
   "source": [
    "* Parse the definitions of concepts that were found in WikiData (the file already exists, need to check if spar_labels exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6746b48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 62/62 [00:09<00:00,  6.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 126 definitions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "parsed_concept_wiki_dict_fp = graph_output_fp.joinpath(\"parsed_concept_wiki_dict.json\")\n",
    "if not parsed_concept_wiki_dict_fp.exists():\n",
    "    if not termextractor:\n",
    "        # instantiate a TermExtractor obj ~ actually running into a threading issue with the tokenizers being reused\n",
    "        # so setting to 1\n",
    "        termextractor = TermExtractor(max_num_cpu_threads=1)\n",
    "    concept_wiki_dict = add_spar_labels(concept_wiki_dict, termextractor)\n",
    "    # Save the updated concept_wiki_dict, will be loaded in previous cells anyway\n",
    "    with open(parsed_concept_wiki_dict_fp, 'w') as f:\n",
    "        json.dump(concept_wiki_dict, f, indent=2)\n",
    "else:\n",
    "    print(\"Loading previously computed wikidata definitions with SPaR.txt objects from file\")\n",
    "    with open(parsed_concept_wiki_dict_fp, 'r') as f:\n",
    "        concept_wiki_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ae7de0d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 825/825 [02:15<00:00,  6.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1741 definitions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "parsed_spans_wiki_dict_fp = graph_output_fp.joinpath(\"parsed_spans_wiki_dict.json\")\n",
    "if not parsed_spans_wiki_dict_fp.exists():\n",
    "    if not termextractor:\n",
    "        # instantiate a TermExtractor obj ~ actually running into a threading issue with the tokenizers being reused\n",
    "        # so setting to 1 worker\n",
    "        termextractor = TermExtractor(max_num_cpu_threads=1)\n",
    "    span_wiki_dict = add_spar_labels(span_wiki_dict, termextractor)\n",
    "    # Save the updated span_wiki_dict, which will be loaded in previous cells anyway\n",
    "    with open(parsed_spans_wiki_dict_fp, 'w') as f:\n",
    "        json.dump(span_wiki_dict, f, indent=2)\n",
    "else:\n",
    "    print(\"Loading previously computed wikidata definitions with SPaR.txt objects from file\")\n",
    "    with open(parsed_spans_wiki_dict_fp, 'r') as f:\n",
    "        span_wiki_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4dec3a",
   "metadata": {},
   "source": [
    "* Some examples of/insight in definitions from different sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0dfa4268",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'prefLabel': 'absorption coefficient',\n",
       "  'definition': 'A quantity characterising the effectiveness of a sound absorbing surface. The proportion of sound energy absorbed is given as a number between zero (for a fully reflective surface) and one (for a fully absorptive surface). Note that sound absorption coefficients determined from laboratory measurements may have values slightly larger than one.',\n",
       "  'note': 'See BS EN 20354:1993.',\n",
       "  'Spans in definitions and notes': ['A quantity',\n",
       "   'the effectiveness',\n",
       "   'a sound absorbing surface',\n",
       "   'The proportion',\n",
       "   'sound energy',\n",
       "   'a number',\n",
       "   'zero',\n",
       "   'a fully reflective surface',\n",
       "   'surface',\n",
       "   'sound absorption coefficients',\n",
       "   'laboratory',\n",
       "   'measurements',\n",
       "   'values',\n",
       "   'BS EN 20354 : 1993']}]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of definitions from approved documents\n",
    "concepts_definitions_dict['1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e746463c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'prefLabel': 'absorption coefficient',\n",
       "  'class_uid': 'http://www.wikidata.org/entity/Q107715',\n",
       "  'class_label': 'physical quantity',\n",
       "  'WikiUID': 'http://www.wikidata.org/entity/Q97368968',\n",
       "  'WikiDefinition': 'measure for the exponential reduction of a quantity along a path due to absorption',\n",
       "  'Spans in definitions and notes': ['measure',\n",
       "   'the exponential reduction',\n",
       "   'a quantity',\n",
       "   'absorption']}]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of definitions for the same concept, from WikiData\n",
    "concept_wiki_dict['1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2971f4f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['absorbent', 'absorption', 'absorption%20coefficient']\n"
     ]
    }
   ],
   "source": [
    "# List of definitions for a related span, from WikiData\n",
    "print([k for k in span_wiki_dict.keys() if 'absor' in k])\n",
    "# span_wiki_dict['absorbent']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d77b070",
   "metadata": {},
   "source": [
    "### Add spans from the definitions to the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb8c45a",
   "metadata": {},
   "source": [
    "* First, we'd like to get some insight in the number of new spans identified through parsing the definitions. We will count:\n",
    "  * the total number of defined terms\n",
    "  * total number of definitions (a defined term may have multiple definitions)\n",
    "  * the number of spans found in all of these definitions (total, and new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ee7f6d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_spans(some_dict: Dict[str, str], primary_source: URIRef):\n",
    "    primary_source_str = primary_source.__reduce__()[1][0]\n",
    "    spartxt_objects_in_dict = {primary_source_str: {}}\n",
    "    \n",
    "    definition_cntr = Counter()\n",
    "    total_num_definitions = 0\n",
    "    for definition_dict_list in some_dict.values():\n",
    "        defined_term = definition_dict_list[0]['prefLabel']\n",
    "        \n",
    "        # count definitions (unique)\n",
    "        defs = list(set([dv for d in definition_dict_list for dk, dv in d.items() if (dk in ['WikiDefinition', 'definition', 'note'] and dv != '')]))\n",
    "        total_num_definitions += len(defs)\n",
    "        definition_cntr[defined_term] += len(defs)\n",
    "        \n",
    "        # collect spans (not unique, but cleaned)\n",
    "        span_lists = [dv for d in definition_dict_list for dk, dv in d.items() if (dk == 'Spans in definitions and notes' and dv != [])]\n",
    "        spans = [remove_determiners(x) for x in custom_cleaning_rules([s for sl in span_lists for s in sl])] \n",
    "        \n",
    "        if defined_term not in spartxt_objects_in_dict[primary_source_str]:\n",
    "            spartxt_objects_in_dict[primary_source_str][defined_term] = spans\n",
    "        else:\n",
    "            spartxt_objects_in_dict[primary_source_str][defined_term] += spans\n",
    "                \n",
    "    return spartxt_objects_in_dict, definition_cntr, total_num_definitions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0ff7e865",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_spartxt_objects = {}\n",
    "total_num_definitions = 0\n",
    "per_term_definition_counts = Counter()\n",
    "for some_dict, source in zip([concepts_definitions_dict, concept_wiki_dict, span_wiki_dict],\n",
    "                             [merged_approved_documents_IRI, wikidata_IRI, wikidata_IRI]):\n",
    "    spans_found_in_definitions, definition_counter, nr_definitions = count_spans(some_dict, source)\n",
    "    all_spartxt_objects.update(spans_found_in_definitions)\n",
    "    per_term_definition_counts.update(definition_counter)\n",
    "    total_num_definitions += nr_definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5a462180",
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_or_span_with_definition = []\n",
    "spans_found_in_definitions = []\n",
    "for source, defined_term_dict in all_spartxt_objects.items():\n",
    "    concept_or_span_with_definition += [k for k in defined_term_dict.keys()]\n",
    "    spans_found_in_definitions += [s for v in defined_term_dict.values() for s in v]\n",
    "        \n",
    "unique_new_spans = [x for x in list(set(spans_found_in_definitions)) if x not in domain_terms]\n",
    "terms_with_multiple_defs = len([k for k, v in per_term_definition_counts.items() if v > 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4f39750a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of defined terms:  1120\n",
      "Number of defined terms with more than 1 definition: 249 (22.23%) \n",
      "Number of definitions/notes found: 1592\n",
      "Top 10 defined terms with most definitions:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('house', 59),\n",
       " ('Woodlands', 57),\n",
       " ('residential building', 38),\n",
       " ('building', 9),\n",
       " ('frequency', 9),\n",
       " ('gallery', 9),\n",
       " ('Chimneys', 8),\n",
       " ('landing', 7),\n",
       " ('pier', 7),\n",
       " ('span', 7)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of defined terms: \", len(concept_or_span_with_definition))\n",
    "print(\"Number of defined terms with more than 1 definition: {} ({:.2f}%) \".format(terms_with_multiple_defs, terms_with_multiple_defs/len(concept_or_span_with_definition) * 100))\n",
    "print(\"Number of definitions/notes found:\", total_num_definitions)\n",
    "print(\"Top 10 defined terms with most definitions:\")\n",
    "per_term_definition_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d97d0915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total count of terms found in definitions/notes: 7894\n",
      "Number of new spans (unique):  2741\n",
      "Random sample of unseen spans, found in the definitions:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['case',\n",
       " 'single person',\n",
       " 'fastener',\n",
       " 'street passageway',\n",
       " 'circle',\n",
       " 'contexts',\n",
       " 'cold feed pipe',\n",
       " 'virtual',\n",
       " 'smooth',\n",
       " 'Room exposed']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Total count of terms found in definitions/notes:\", len(spans_found_in_definitions))\n",
    "print(\"Number of new spans (unique): \", len(unique_new_spans))\n",
    "print(\"Random sample of unseen spans, found in the definitions:\")\n",
    "random.sample(list(set(unique_new_spans)), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53825d2d",
   "metadata": {},
   "source": [
    "Quite a few new spans from the defined terms!\n",
    "* Add any new spans to the graph, with prov:hasPrimarySource *WikiData* and prov:wasAttributedTo *SPaR.txt*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "558b5f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a related label between the defined concept/span, and the span found in a definition\n",
    "for i, (source, term_and_spans_dict) in enumerate(all_spartxt_objects.items()):\n",
    "    for term, related_spans in term_and_spans_dict.items():\n",
    "\n",
    "        # add the (concept or span) term as a span if it didn't exist yet as a span\n",
    "        term_uid = ua.assign_UID(term, SPANS)\n",
    "        irec_graph = add_tuples(irec_graph, irec_span(term_uid, term))\n",
    "        irec_graph = add_tuples(irec_graph, skos_in_scheme(term_uid, 'schemeUID', SPANS, SPANS))\n",
    "        irec_graph = add_tuples(irec_graph, provenance(term_uid, URIRef(source), SPANS))\n",
    "\n",
    "        rel_spans = [remove_determiners(r) for r in custom_cleaning_rules(related_spans)]\n",
    "        for rel_term in rel_spans :\n",
    "            # Add the spans that were extracted from the definitions, assign a new UID if needed\n",
    "            related_uid = ua.assign_UID(rel_term, SPANS)\n",
    "            irec_graph = add_tuples(irec_graph, irec_span(related_uid, rel_term))\n",
    "            irec_graph = add_tuples(irec_graph, skos_in_scheme(related_uid, 'schemeUID', SPANS, SPANS))\n",
    "            irec_graph = add_tuples(irec_graph, provenance(related_uid, URIRef(source), SPANS))\n",
    "            \n",
    "            # add agent for spans generated by SPaR.txt\n",
    "            irec_graph = add_tuples(irec_graph, prov_agent(related_uid, spart_txt_IRI, SPANS))\n",
    "\n",
    "            # Add relation between the defined span and the span from its definition\n",
    "            irec_graph = add_tuples(irec_graph, irec_definition_related(term_uid, related_uid)) \n",
    "\n",
    "#             # deprecated; Add relation between concept and span\n",
    "#             if concept_uid:\n",
    "#                 irec_graph = add_tuples(irec_graph, irec_definition_related(concept_uid, related_uid, CONCEPTS, SPANS)) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec817464",
   "metadata": {},
   "source": [
    "### Add WikiData definitions to graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c270684a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_wiki_definitions(irec_graph: Graph, wiki_dict: Dict[str, str], dict_namespace: Namespace):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    for i, (_, definition_dict_list) in enumerate(wiki_dict.items()):\n",
    "        for definition_dict in definition_dict_list:\n",
    "            wiki_term = definition_dict['prefLabel']\n",
    "            wiki_class_label = definition_dict['class_label'] \n",
    "            wiki_class_uid = definition_dict['class_uid'] \n",
    "            wiki_uid = definition_dict['WikiUID'].rsplit('/', 1)[1]\n",
    "\n",
    "            # keep track of uid in the Unique ID assigner obj as well\n",
    "            _ = ua.keep_track_of_existing_UID(wiki_term, wiki_uid, WIKI)\n",
    "            \n",
    "            # add the WikiData concept to the graph, in WIKI namespace\n",
    "            irec_graph = add_tuples(irec_graph, skos_node(wiki_uid, wiki_term, WIKI))\n",
    "            # irec_graph = add_tuples(irec_graph, skos_in_scheme(wiki_uid, 'schemeUID', WIKI, WIKI))\n",
    "            irec_graph = add_tuples(irec_graph, provenance(wiki_uid, wikidata_IRI, WIKI))\n",
    "            \n",
    "            # Add an exact match between the wiki node and our concept from the Merged Approved Documents\n",
    "            term_uid = ua.retrieve_uid_by_text(wiki_term, CONCEPTS)\n",
    "            if term_uid:\n",
    "                irec_graph = add_tuples(irec_graph, skos_exact_match(term_uid, wiki_uid, CONCEPTS, WIKI))\n",
    "            \n",
    "            #### We will link the WikiData concept to a span, rather than a concept, as well as its definitions\n",
    "            # and class labels\n",
    "            # 1) Add a span and a link to the wiki concept\n",
    "            span_uid = ua.retrieve_uid_by_text(wiki_term, SPANS)\n",
    "            if not span_uid:\n",
    "                raise Exception(f\"Cannot find the span: {wiki_term}!\")\n",
    "            \n",
    "            # 2) add wiki class label as a span in SPANS namespace, with provenance linking to original WikiData UID\n",
    "            wiki_class_span_uid = ua.assign_UID(wiki_class_label, SPANS)\n",
    "            irec_graph = add_tuples(irec_graph, irec_span(wiki_class_span_uid, wiki_class_label))\n",
    "            irec_graph = add_tuples(irec_graph, provenance(wiki_class_span_uid, WIKI[wiki_class_uid], SPANS))\n",
    "            # relate the class label span to the defined span, using RDF.type\n",
    "            irec_graph = add_tuples(irec_graph, rdf_type(span_uid, wiki_class_span_uid, SPANS, SPANS))\n",
    "\n",
    "            # 3)  Add the WIKI definition to the node if it exists, in SPANS namespace\n",
    "            if 'WikiDefinition' in definition_dict:            \n",
    "                definition = definition_dict['WikiDefinition']\n",
    "                irec_graph = add_tuples(irec_graph, irec_wikidef(span_uid, definition, SPANS))\n",
    "                \n",
    "            # 4) Add an exact match between the span and wikidata class as well\n",
    "            irec_graph = add_tuples(irec_graph, skos_exact_match(span_uid, wiki_uid, SPANS, WIKI))\n",
    "\n",
    "    return irec_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "43cf5dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "irec_graph = add_wiki_definitions(irec_graph, concept_wiki_dict, CONCEPTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "25128df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "irec_graph = add_wiki_definitions(irec_graph, span_wiki_dict, SPANS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549f2a32",
   "metadata": {},
   "source": [
    "* Save graph again at this stage for comparison, before computing the features between spans. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ae56c35e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Graph identifier=Ne8b1118113f2479e8db1a9575cc50a79 (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irec_graph.serialize(destination=graph_output_fp.joinpath(\"unfeatured_graph.ttl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e978d7",
   "metadata": {},
   "source": [
    "## Compute properties for and between spans \n",
    "<div class=\"alert alert-block alert-info\">\n",
    "Note that computing the span-span features will take some time!\n",
    "7K-ish spans, so 46 Million-ish combinations!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fceef5",
   "metadata": {},
   "source": [
    "###  Grab base antonyms\n",
    "* We may want to get a sense of which spans are antonyms\n",
    "* For this we'll use NLTK's version of WordNet, which mainly captures antonyms for adjectives and adverbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b7fe6de0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cold']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet_antonyms = {}\n",
    "for i in wn.all_synsets():\n",
    "    if i.pos() in ['a', 's']:    # If synset is adj or satelite-adj.\n",
    "        for j in i.lemmas():     # Iterating through lemmas for each synset.\n",
    "            if j.antonyms():     # If adj has antonym.\n",
    "                wordnet_antonyms[str(j.name()).strip()] = [x.name() for x in j.antonyms()]\n",
    "\n",
    "# Example of a useful antonym for us\n",
    "wordnet_antonyms['hot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "de3d0ae4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hot']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet_antonyms['cold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0a9692dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('acidic', ['alkaline', 'amphoteric']),\n",
       " ('alkaline', ['amphoteric', 'acidic']),\n",
       " ('amphoteric', ['acidic', 'alkaline']),\n",
       " ('air-to-surface', ['air-to-air', 'surface-to-air']),\n",
       " ('air-to-air', ['surface-to-air', 'air-to-surface']),\n",
       " ('surface-to-air', ['air-to-surface', 'air-to-air']),\n",
       " ('anadromous', ['catadromous', 'diadromous']),\n",
       " ('catadromous', ['diadromous', 'anadromous']),\n",
       " ('diadromous', ['anadromous', 'catadromous']),\n",
       " ('aquatic', ['terrestrial', 'amphibious']),\n",
       " ('terrestrial', ['amphibious', 'aquatic']),\n",
       " ('amphibious', ['aquatic', 'terrestrial']),\n",
       " ('prenatal', ['perinatal', 'postnatal']),\n",
       " ('perinatal', ['postnatal', 'prenatal']),\n",
       " ('postnatal', ['prenatal', 'perinatal']),\n",
       " ('sonic', ['subsonic', 'supersonic']),\n",
       " ('subsonic', ['supersonic', 'sonic']),\n",
       " ('supersonic', ['sonic', 'subsonic']),\n",
       " ('binucleate', ['trinucleate', 'mononuclear']),\n",
       " ('mononuclear', ['binucleate', 'trinucleate']),\n",
       " ('trinucleate', ['mononuclear', 'binucleate']),\n",
       " ('lower-class', ['middle-class', 'upper-class']),\n",
       " ('middle-class', ['upper-class', 'lower-class']),\n",
       " ('upper-class', ['lower-class', 'middle-class']),\n",
       " ('carnivorous', ['herbivorous', 'omnivorous', 'insectivorous']),\n",
       " ('herbivorous', ['omnivorous', 'insectivorous', 'carnivorous']),\n",
       " ('omnivorous', ['insectivorous', 'carnivorous', 'herbivorous']),\n",
       " ('insectivorous', ['carnivorous', 'herbivorous', 'omnivorous']),\n",
       " ('acatalectic', ['catalectic', 'hypercatalectic']),\n",
       " ('catalectic', ['hypercatalectic', 'acatalectic']),\n",
       " ('hypercatalectic', ['acatalectic', 'catalectic']),\n",
       " ('isotonic', ['hypertonic', 'hypotonic']),\n",
       " ('unconventional', ['conventional', 'conventional']),\n",
       " ('cubic', ['linear', 'planar']),\n",
       " ('planar', ['cubic', 'linear']),\n",
       " ('annual', ['biennial', 'perennial']),\n",
       " ('biennial', ['perennial', 'annual']),\n",
       " ('perennial', ['annual', 'biennial']),\n",
       " ('sharp', ['flat', 'natural']),\n",
       " ('early', ['middle', 'late']),\n",
       " ('middle', ['late', 'early']),\n",
       " ('late', ['early', 'middle']),\n",
       " ('ectomorphic', ['endomorphic', 'mesomorphic']),\n",
       " ('endomorphic', ['mesomorphic', 'ectomorphic']),\n",
       " ('mesomorphic', ['ectomorphic', 'endomorphic']),\n",
       " ('endogamous', ['exogamous', 'autogamous']),\n",
       " ('exogamous', ['autogamous', 'endogamous']),\n",
       " ('autogamous', ['endogamous', 'exogamous']),\n",
       " ('flat', ['natural', 'sharp']),\n",
       " ('nonspecific', ['specific', 'specific']),\n",
       " ('endemic', ['epidemic', 'ecdemic']),\n",
       " ('haploid', ['diploid', 'polyploid']),\n",
       " ('diploid', ['polyploid', 'haploid']),\n",
       " ('polyploid', ['haploid', 'diploid']),\n",
       " ('heterosexual', ['homosexual', 'bisexual']),\n",
       " ('homosexual', ['bisexual', 'heterosexual']),\n",
       " ('bisexual', ['heterosexual', 'homosexual']),\n",
       " ('homologous', ['analogous', 'heterologous']),\n",
       " ('heterologous', ['analogous', 'homologous']),\n",
       " ('autologous', ['homologous', 'heterologous']),\n",
       " ('analogous', ['homologous', 'heterologous']),\n",
       " ('horizontal', ['vertical', 'inclined']),\n",
       " ('vertical', ['inclined', 'horizontal']),\n",
       " ('vernal', ['summery', 'autumnal', 'wintry']),\n",
       " ('summery', ['autumnal', 'wintry', 'vernal']),\n",
       " ('autumnal', ['wintry', 'vernal', 'summery']),\n",
       " ('wintry', ['vernal', 'summery', 'autumnal']),\n",
       " ('introversive', ['extroversive', 'ambiversive']),\n",
       " ('extroversive', ['ambiversive', 'introversive']),\n",
       " ('ambiversive', ['introversive', 'extroversive']),\n",
       " ('leptorrhine', ['catarrhine', 'platyrrhine']),\n",
       " ('catarrhine', ['leptorrhine', 'platyrrhine']),\n",
       " ('platyrrhine', ['catarrhine', 'leptorrhine']),\n",
       " ('epidemic', ['endemic', 'ecdemic']),\n",
       " ('ecdemic', ['endemic', 'epidemic']),\n",
       " ('mini', ['midi', 'maxi']),\n",
       " ('midi', ['maxi', 'mini']),\n",
       " ('maxi', ['mini', 'midi']),\n",
       " ('male', ['female', 'androgynous']),\n",
       " ('female', ['androgynous', 'male']),\n",
       " ('androgynous', ['male', 'female']),\n",
       " ('masculine', ['feminine', 'neuter']),\n",
       " ('feminine', ['neuter', 'masculine']),\n",
       " ('neuter', ['masculine', 'feminine']),\n",
       " ('univalent', ['bivalent', 'multivalent']),\n",
       " ('bivalent', ['multivalent', 'univalent']),\n",
       " ('multivalent', ['univalent', 'bivalent']),\n",
       " ('natural', ['sharp', 'flat']),\n",
       " ('hypertensive', ['hypotensive', 'normotensive']),\n",
       " ('hypotensive', ['normotensive', 'hypertensive']),\n",
       " ('normotensive', ['hypertensive', 'hypotensive']),\n",
       " ('one-piece', ['two-piece', 'three-piece']),\n",
       " ('two-piece', ['three-piece', 'one-piece']),\n",
       " ('three-piece', ['one-piece', 'two-piece']),\n",
       " ('parallel', ['perpendicular', 'oblique']),\n",
       " ('oblique', ['parallel', 'perpendicular']),\n",
       " ('perpendicular', ['oblique', 'parallel']),\n",
       " ('past', ['present', 'future']),\n",
       " ('future', ['past', 'present']),\n",
       " ('neutral', ['positive', 'negative']),\n",
       " ('quantitative', ['syllabic', 'accentual']),\n",
       " ('right-handed', ['left-handed', 'ambidextrous']),\n",
       " ('left-handed', ['ambidextrous', 'right-handed']),\n",
       " ('ambidextrous', ['right-handed', 'left-handed']),\n",
       " ('center', ['right', 'left']),\n",
       " ('liquid', ['gaseous', 'solid']),\n",
       " ('gaseous', ['solid', 'liquid']),\n",
       " ('some', ['no', 'all']),\n",
       " ('no', ['all', 'some']),\n",
       " ('all', ['some', 'no']),\n",
       " ('syllabic', ['accentual', 'quantitative']),\n",
       " ('accentual', ['quantitative', 'syllabic']),\n",
       " ('superscript', ['subscript', 'adscript']),\n",
       " ('subscript', ['adscript', 'superscript']),\n",
       " ('adscript', ['superscript', 'subscript']),\n",
       " ('top', ['bottom', 'side']),\n",
       " ('bottom', ['side', 'top']),\n",
       " ('side', ['top', 'bottom']),\n",
       " ('surface', ['subsurface', 'overhead']),\n",
       " ('subsurface', ['overhead', 'surface']),\n",
       " ('overhead', ['surface', 'subsurface']),\n",
       " ('viviparous', ['oviparous', 'ovoviviparous']),\n",
       " ('oviparous', ['ovoviviparous', 'viviparous']),\n",
       " ('ovoviviparous', ['viviparous', 'oviparous']),\n",
       " ('xeric', ['hydric', 'mesic']),\n",
       " ('hydric', ['mesic', 'xeric']),\n",
       " ('mesic', ['xeric', 'hydric'])]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# there are cases of multiple antonyms:\n",
    "[(k, wordnet_antonyms[k]) for k, v in wordnet_antonyms.items() if len(v) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5b1aabbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3381\n"
     ]
    }
   ],
   "source": [
    "# total number of antonyms\n",
    "print(len(list(set([s for k, v in wordnet_antonyms.items() for s in [k] + v]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "afd035fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEPRECATED > we don't add the plain antonyms to the graph, since they are mostly irrelevant\n",
    "# wordnet_uid = URIRef(\"https://www.wikidata.org/wiki/Q533822\")\n",
    "# for span in wordnet_antonyms.keys():\n",
    "#     span_uid = ua.assign_UID(span, SPANS)\n",
    "    \n",
    "#     irec_graph = add_tuples(irec_graph, irec_span(span_uid, span))\n",
    "#     irec_graph = add_tuples(irec_graph, skos_in_scheme(span_uid, 'schemeUID', SPANS, SPANS))\n",
    "#     irec_graph = add_tuples(irec_graph, provenance(span_uid, wordnet_uid, SPANS))\n",
    "\n",
    "#     antonyms = wordnet_antonyms[span]\n",
    "#     for antonym in antonyms:\n",
    "#         antonym_uid = ua.assign_UID(antonym, SPANS)\n",
    "\n",
    "#         irec_graph = add_tuples(irec_graph, irec_span(antonym_uid, antonym))\n",
    "#         irec_graph = add_tuples(irec_graph, skos_in_scheme(antonym_uid, 'schemeUID', SPANS, SPANS))\n",
    "#         irec_graph = add_tuples(irec_graph, provenance(antonym_uid, wordnet_uid, SPANS))\n",
    "\n",
    "#         # add the antonym relation\n",
    "#         irec_graph = add_tuples(irec_graph, irec_antonym(span_uid, antonym_uid))\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca06c37f",
   "metadata": {},
   "source": [
    "### Domain prediction and Semantic Similarity between spans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8aed2a",
   "metadata": {},
   "source": [
    "**Embedding the new spans to determine distributed similarity and classify domain-specificity**\n",
    "* Note that we have embeddings for the domain terms, but not for all terms found in the definitions of spans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e46c83a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_model_name = \"bert-base-cased\"\n",
    "embedding_output_fp = Path.cwd().joinpath(\"data\", \"term_embedding\")\n",
    "IDF_path = embedding_output_fp.joinpath(\"IDF_weights.json\")\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "bert_model = BertModel.from_pretrained(bert_model_name, output_hidden_states=True)\n",
    "embedder = Embedder(tokenizer, bert_model, \n",
    "                      IDF_dict=json.load(open(IDF_path)), \n",
    "                      embedding_fp=embedding_output_fp,\n",
    "                      layers_to_use = [12],         # we'll use the output of the last layer\n",
    "                      layer_combination = \"avg\",    # how to combine layers if multiple are used\n",
    "                      idf_threshold = 1.5,          # minimum IDF value for a token to contribute\n",
    "                      idf_weight_factor = 1.0,      # modify how strong the influence of IDF weighting is\n",
    "                      not_found_idf_value = 0.5)    # IDF value for tokens that weren't seen during IDF computation (doesn't apply here)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4742e3c9",
   "metadata": {},
   "source": [
    "* First we'll need to compute the embeddings for the unique new spans\n",
    "  * Same process as before, EXCEPT that we now normalise the spans directly as well.\n",
    "  * This seems to break sometimes when I run it, and I don't know why yet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "547fbc64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2741"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_new_spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4f06a659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embeddings for 2741 spans, in groups of: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:44<00:00, 14.96s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### TODO: move this into a utility function\n",
    "# Compute the embeddings, this is split into subsets so we don't overload your memory (adjust these values if needed)\n",
    "max_num_cpu_threads = 4\n",
    "subset_size = 1000\n",
    "\n",
    "# Checks which of the embeddings for the clustering cluster_data already exist, so they can be re-used\n",
    "term_subsets = split_list(unique_new_spans, subset_size)\n",
    "embedding_files = [f for f in embedder.embedding_fp.glob('def_term_standardised_embeddings*.pkl')]\n",
    "span_and_embedding_pairs = []\n",
    "if len(embedding_files) == len(term_subsets):\n",
    "    for e in embedding_files:\n",
    "        span_and_embedding_pairs += pickle.load(open(e, 'rb'))\n",
    "else:\n",
    "    print(f\"Preparing embeddings for {len(unique_new_spans)} spans, in groups of: {subset_size}\")\n",
    "    subset_idx = 0            # iterator index outside of tqdm \n",
    "    for subset in tqdm(term_subsets):\n",
    "        subset_embeddings = []\n",
    "        subset_file_name = embedder.embedding_fp.joinpath(\"def_term_standardised_embeddings_part_{}.pkl\".format(subset_idx))\n",
    "        subset_idx += 1\n",
    "        if subset_file_name.exists():\n",
    "            # already computed previously\n",
    "            continue\n",
    "        \n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=max_num_cpu_threads) as executor:\n",
    "            futures = [executor.submit(embedder.embed_and_normalise_span, subset[idx]) for idx in range(len(subset))]\n",
    "\n",
    "        subset_embeddings += [f.result() for f in futures if f.result()]\n",
    "\n",
    "        with open(subset_file_name, 'wb') as f:\n",
    "            pickle.dump(subset_embeddings, f)\n",
    "\n",
    "    # Once all embeddings are created; combine them in span_and_embedding_pairs\n",
    "    embedding_files = [f for f in embedder.embedding_fp.glob('def_term_standardised_embeddings*.pkl')]\n",
    "    for e in embedding_files:\n",
    "        span_and_embedding_pairs += pickle.load(open(e, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2c64704c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the old span_and_embedding_pairs as well\n",
    "old_embedding_files = [f for f in embedder.embedding_fp.glob('embeddings*.pkl')]\n",
    "old_span_and_embedding_pairs = []\n",
    "for e in old_embedding_files:\n",
    "    old_span_and_embedding_pairs += pickle.load(open(e, 'rb'))\n",
    "\n",
    "domain_span_and_embedding_pairs = [(s, e) for (s, e) in old_span_and_embedding_pairs if s in domain_terms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "87890294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the old and new span and embeding pairs\n",
    "unique_spans = [s for (s, e) in (domain_span_and_embedding_pairs + span_and_embedding_pairs)]\n",
    "standardised_clustering_data = np.stack([np.mean(e, axis=0) if len(e.shape) > 1 else e for (s, e) in (domain_span_and_embedding_pairs + span_and_embedding_pairs)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b3cbbf",
   "metadata": {},
   "source": [
    "**Predict the domain of spans based on kNN classification**\n",
    "* Now that we have embeddings for all spans, we will re-use the knn graph and TFIDF based classification of the term extraction notebook\n",
    "* The domain options are either \"AEC domain\" or \"Out of domain\", which we'll add as a property to the span node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "160b6304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data to prepare the kNN based domain classifier\n",
    "knn_X = pickle.load(open(embedding_output_fp.joinpath(\"unique_embeddings.pkl\"), 'rb'))\n",
    "\n",
    "knn_spans = pickle.load(open(embedding_output_fp.joinpath(\"unique_spans.pkl\"), 'rb'))\n",
    "with open(embedding_output_fp.joinpath(\"span_domain_ood_dict.json\"), 'r') as f:\n",
    "    span_df_dict = json.load(f)  \n",
    "    \n",
    "knn_y = [span_df_dict[span]['domain'] for span in knn_spans]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1e85409e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the classifier\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=50, \n",
    "                                      weights='distance',\n",
    "                                      leaf_size=100, \n",
    "                                      metric='euclidean', \n",
    "                                      n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "dc05b930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(leaf_size=100, metric='euclidean', n_jobs=4,\n",
       "                     n_neighbors=50, weights='distance')"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the classifier\n",
    "knn_classifier.fit(knn_X, knn_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "09fb9e4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'y'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict on a single span\n",
    "test_span = 'waterproofing membrane'\n",
    "knn_classifier.predict(embedder.embed_and_normalise(test_span).reshape(1, -1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "342b11cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = knn_classifier.predict(standardised_clustering_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d52bba6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# depending on how long this takes, we'll simply predict the label for each of the spans (even the ones we already know the answer for)\n",
    "tuples_to_add = []\n",
    "for span, prediction in zip(unique_spans, predictions):\n",
    "    try:\n",
    "        span_uid = ua.UIDs[SPANS.placeholder.defrag().__reduce__()[1][0]][span.strip()]\n",
    "    except KeyError:\n",
    "        continue\n",
    "#         print(f\"Span does not exist in the graph: {span}\")\n",
    "        \n",
    "    if prediction == \"y\":\n",
    "        tuples_to_add += irec_domain(span_uid, \"AEC domain\")\n",
    "    elif prediction == \"n\":\n",
    "        tuples_to_add += irec_domain(span_uid)\n",
    "    else:\n",
    "        print(\"this should not happen!\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "cc4d732b",
   "metadata": {},
   "outputs": [],
   "source": [
    "irec_graph = add_tuples(irec_graph, tuples_to_add)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de86670d",
   "metadata": {},
   "source": [
    "**Add semantic similarity relation to the 5 Nearest Neighbours (NN)**\n",
    "* Compute the kNN graph for ALL spans now (old + new) in order to determine the 5 NNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "809890b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = 5 # the number of neighbours we compute for each term\n",
    "knn_graph = kneighbors_graph(standardised_clustering_data, \n",
    "                             n_neighbors,    \n",
    "                             metric=\"cosine\", # <- note we're using cosine sim\n",
    "                             n_jobs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "19302366",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_sim_dict = {}\n",
    "for span_idx, span in enumerate(unique_spans):\n",
    "    knn_sim_dict[span] = [unique_spans[neighbour_idx] for neighbour_idx in knn_graph[span_idx].indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "8b078319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add similarity to the irec_graph\n",
    "tuples_to_add = []\n",
    "missing_spans = []\n",
    "for span_one, neighbour_spans in knn_sim_dict.items():\n",
    "    try:\n",
    "        span_one_uid = ua.UIDs[SPANS.placeholder.defrag().__reduce__()[1][0]][span_one]\n",
    "    except:\n",
    "        missing_spans.append(span_one)\n",
    "        continue    \n",
    "    for span_two in neighbour_spans:\n",
    "        try:\n",
    "            span_two_uid = ua.UIDs[SPANS.placeholder.defrag().__reduce__()[1][0]][span_two]\n",
    "            tuples_to_add += irec_sem_sim(span_one_uid, span_two_uid)\n",
    "        except:\n",
    "            missing_spans.append(span_two)\n",
    "            \n",
    "irec_graph = add_tuples(irec_graph, tuples_to_add)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4276a04a",
   "metadata": {},
   "source": [
    "## Constitutes relations and Morphological Similarity\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "We will create a CharacterSpan object for each span to temporarily store the computed properties/relations, see below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b17cc54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterSpan:\n",
    "    def __init__(self, span:str, span_uid: str):\n",
    "        \n",
    "        if not span:\n",
    "            raise Exception(\"Input is an empty string!\")\n",
    "        \n",
    "        self.text = span\n",
    "        self.uid = span_uid\n",
    "        self.blob = TextBlob(span)\n",
    "        self.words = [w for w in self.blob.words]\n",
    "        self.stems = [w.stem() for w in self.words]\n",
    "        \n",
    "        self.morphologically_similar_uids = {}\n",
    "        self.semantically_similar_uids = {}\n",
    "        self.constitutes_uids = {}\n",
    "        self.contains_antonym_uids = {}       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cabc8fd",
   "metadata": {},
   "source": [
    "* Methods to compute features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e4b50ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def span_constitutes_span(span_one: CharacterSpan, span_two: CharacterSpan):\n",
    "    \"\"\" True if in a span, you can find all of the words comprising span_two (order doesn't matter) \"\"\"\n",
    "    word_overlap = list(set(span_one.words) & set(span_two.words))\n",
    "    if len(word_overlap) in [len(span_one.words), len(span_two.words)]:\n",
    "        # the word overlap is as long as one of the two spans, so the entire span overlaps with part of the other \n",
    "        return True\n",
    "    return False\n",
    "    \n",
    "def morphologically_similar(span_one: CharacterSpan, span_two: CharacterSpan):\n",
    "    \"\"\" True if this span has either a small Levenshtein distance, or many overlapping words/stems with another span \"\"\"\n",
    "    wl_one = len(span_one.words)\n",
    "    wl_two = len(span_two.words)\n",
    "    longest = max(wl_one, wl_two)\n",
    "    \n",
    "    if (wl_one < wl_two - 1) or (wl_one > wl_two + 1):\n",
    "        # max 1 word difference in length\n",
    "        return False\n",
    "    \n",
    "    if (wl_one == 1) and (wl_two == 1):\n",
    "        if span_one.stems == span_two.stems:\n",
    "            # if the stem is the same, then we'll assume that the words are morphologically similar\n",
    "            return True\n",
    "        \n",
    "        if levenshtein(span_one.text, span_two.text):\n",
    "            # small edit distance (levenshtein), works for single words only\n",
    "            return True\n",
    "    else:\n",
    "        stem_overlap = list(set(span_one.stems) & set(span_two.stems))\n",
    "        if (len(stem_overlap) >= (2 * longest // 3)):\n",
    "            # at least 2 out of 3 words or stems are overlapping \n",
    "            unique_w_indices_one = [idx for idx, s in enumerate(span_one.stems) if s not in stem_overlap]\n",
    "            unique_w_indices_two = [idx for idx, s in enumerate(span_two.stems) if s not in stem_overlap]\n",
    "            if not unique_w_indices_one or not unique_w_indices_two:\n",
    "                # This means span one constitutes span two, but with at least 2 words \n",
    "                # (so we give it an extra bump, which I'm not sure if we need it or care...)\n",
    "                return True\n",
    "            for w_one_idx, w_two_idx in product(unique_w_indices_one, unique_w_indices_two):\n",
    "                if not levenshtein(span_one.words[w_one_idx], span_two.words[w_two_idx]):\n",
    "                    # if any of the non-overlapping words have a large edit distance, the the whole thing is not \n",
    "                    # morphologically similar\n",
    "                    return False\n",
    "            \n",
    "            # now, many of the words are pretty much the same, and the edit disance for remainder of words is small\n",
    "            return True\n",
    "        \n",
    "    return False\n",
    "\n",
    "def span_with_antonym(span_one: CharacterSpan, span_two: CharacterSpan, wordnet_antonyms: Dict[str, str]=wordnet_antonyms):\n",
    "    antonyms_one = [w for w in span_one.words if w in wordnet_antonyms.keys()]\n",
    "    remainder_one = ' '.join([w for w in span_one.words if w not in antonyms_one])\n",
    "    for a in antonyms_one:\n",
    "        antonyms_to_find = wordnet_antonyms[a]\n",
    "        if any([x for x in span_two.words if x in antonyms_to_find]):\n",
    "            # antonym present, but any overlap in the rest of the spans?\n",
    "            antonyms_two = [w for w in span_two.words if w in wordnet_antonyms.keys()]\n",
    "            remainder_two = ' '.join([w for w in span_two.words if w not in antonyms_two])\n",
    "            if remainder_one and remainder_two:\n",
    "                cs_one= CharacterSpan(remainder_one, '')\n",
    "                cs_two = CharacterSpan(remainder_two, '')\n",
    "                if morphologically_similar(cs_one, cs_two):\n",
    "                    return True\n",
    "    return False\n",
    "\n",
    "def semantically_similar_check(span_one: CharacterSpan, span_two: CharacterSpan, sim_dict: Dict[str, str] = knn_sim_dict):\n",
    "    \"\"\"\n",
    "    This function only exists to check the outputs of our semantic similarity relations\n",
    "    \"\"\"\n",
    "    if span_one.text in sim_dict:\n",
    "        if span_two.text in sim_dict[span_one.text]:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a792482",
   "metadata": {},
   "source": [
    "* Computing some features for example spans, to test/show the behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "5a61f002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "span_1 has span_2 as NN (semantic similarity):  True\n",
      "span_1 constitutes span_2:  False\n",
      "morphologically similar:  False\n",
      "antonym present:  True\n"
     ]
    }
   ],
   "source": [
    "# Example; semantically similar following cosine similarity\n",
    "test_1 = \"hot water\"\n",
    "test_2 = \"cold water\"\n",
    "cs1 = CharacterSpan(test_1, '1')\n",
    "cs2= CharacterSpan(test_2, '2')\n",
    "print(\"span_1 has span_2 as NN (semantic similarity): \", semantically_similar_check(cs1, cs2))\n",
    "print(\"span_1 constitutes span_2: \", span_constitutes_span(cs1, cs2))\n",
    "print(\"morphologically similar: \", morphologically_similar(cs1, cs2))\n",
    "print(\"antonym present: \", span_with_antonym(cs1, cs2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e207d87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "span_1 has span_2 as NN (semantic similarity):  False\n",
      "span_1 constitutes span_2:  True\n",
      "morphologically similar:  True\n",
      "antonym present:  False\n"
     ]
    }
   ],
   "source": [
    "# Example; MWE constitutes, we allow morphological similarity to be annotated here as well\n",
    "test_1 = \"damp proof\"\n",
    "test_2 = \"damp proof membrane\"\n",
    "cs1 = CharacterSpan(test_1, '1')\n",
    "cs2= CharacterSpan(test_2, '2')\n",
    "print(\"span_1 has span_2 as NN (semantic similarity): \", semantically_similar_check(cs1, cs2))\n",
    "print(\"span_1 constitutes span_2: \", span_constitutes_span(cs1, cs2))\n",
    "print(\"morphologically similar: \", morphologically_similar(cs1, cs2))\n",
    "print(\"antonym present: \", span_with_antonym(cs1, cs2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "2ac1d190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "span_1 has span_2 as NN (semantic similarity):  False\n",
      "span_1 constitutes span_2:  False\n",
      "morphologically similar:  False\n",
      "antonym present:  True\n"
     ]
    }
   ],
   "source": [
    "# Example; antonym exists, and the terms may be subclasses of the same superclass\n",
    "test_1 = \"hot water storage\"\n",
    "test_2 = \"cold water storage\"\n",
    "cs1 = CharacterSpan(test_1, '1')\n",
    "cs2= CharacterSpan(test_2, '2')\n",
    "print(\"span_1 has span_2 as NN (semantic similarity): \", semantically_similar_check(cs1, cs2))\n",
    "print(\"span_1 constitutes span_2: \", span_constitutes_span(cs1, cs2))\n",
    "print(\"morphologically similar: \", morphologically_similar(cs1, cs2))\n",
    "print(\"antonym present: \", span_with_antonym(cs1, cs2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "95204ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "span_1 has span_2 as NN (semantic similarity):  False\n",
      "span_1 constitutes span_2:  False\n",
      "morphologically similar:  False\n",
      "antonym present:  False\n"
     ]
    }
   ],
   "source": [
    "# Example; antonym exists, but the rest of the terms is too different\n",
    "test_1 = \"hot water storage system\"\n",
    "test_2 = \"cold press\"  \n",
    "cs1 = CharacterSpan(test_1, '1')\n",
    "cs2= CharacterSpan(test_2, '2')\n",
    "print(\"span_1 has span_2 as NN (semantic similarity): \", semantically_similar_check(cs1, cs2))\n",
    "print(\"span_1 constitutes span_2: \", span_constitutes_span(cs1, cs2))\n",
    "print(\"morphologically similar: \", morphologically_similar(cs1, cs2))\n",
    "print(\"antonym present: \", span_with_antonym(cs1, cs2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491b93c5",
   "metadata": {},
   "source": [
    "**Compute the features**\n",
    "* method to compute all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "5b875cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features(argument_list: List[CharacterSpan]):\n",
    "    span_one, span_two = argument_list\n",
    "    feature_tuples = []\n",
    "    \n",
    "    if span_one == span_two:\n",
    "        return feature_tuples\n",
    "    \n",
    "    if span_constitutes_span(span_one, span_two):\n",
    "        feature_tuples.append(irec_constitutes(span_one.uid, span_two.uid))\n",
    "        \n",
    "    if morphologically_similar(span_one, span_two):\n",
    "        feature_tuples.append(irec_morp_sim(span_one.uid, span_two.uid))\n",
    "        \n",
    "    return feature_tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816910b0",
   "metadata": {},
   "source": [
    "* Convert spans to CharacterSpan objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "29daf2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "spans_namespace_uid = SPANS.placeholder.defrag().__reduce__()[1][0]\n",
    "spans = [k for k in ua.UIDs[spans_namespace_uid].keys() if ua.UIDs[spans_namespace_uid][k] != 'schemeUID']\n",
    "spans_c = [CharacterSpan(span, ua.UIDs[spans_namespace_uid][span]) for span in spans]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e206314e",
   "metadata": {},
   "source": [
    "* First, we check for antonyms, as we can optimise this to some extent:\n",
    "  * limit to spans that contain a potential antonym in the first place, could do more flexible matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "76941b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of spans that contain an antonym:  1596\n"
     ]
    }
   ],
   "source": [
    "spans_with_antonyms = [s for s in spans_c if (s.text not in wordnet_antonyms and any([w for w in s.words if w in wordnet_antonyms.keys()]))]\n",
    "print(\"Total number of spans that contain an antonym: \", len(spans_with_antonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "28647576",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2547216it [00:07, 359344.24it/s]\n"
     ]
    }
   ],
   "source": [
    "antonym_examples = []\n",
    "antonym_tuples = []\n",
    "for span_one, span_two in tqdm(product(spans_with_antonyms, spans_with_antonyms)):\n",
    "    if span_with_antonym(span_one, span_two):\n",
    "        antonym_examples.append([span_one.text, span_two.text])\n",
    "        antonym_tuples += irec_antonym(span_one.uid, span_two.uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "d0f2b85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "irec_graph = add_tuples(irec_graph, antonym_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "8de94f93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['manual controls', 'automatic control'],\n",
       " ['manual automatic control', 'automatic controls'],\n",
       " ['manual automatic control', 'manual automatic control'],\n",
       " ['inner leaf', 'outer leaf'],\n",
       " ['automatic control', 'manual automatic control'],\n",
       " ['low level', 'high level'],\n",
       " ['external wall', 'internal masonry walls'],\n",
       " ['open hot water storage', 'cold water storage cistern'],\n",
       " ['vertical fixing', 'horizontal fixing'],\n",
       " ['cold water', 'hot water']]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(antonym_examples, 10) # currently contains both A -> B and B <- A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8de5684",
   "metadata": {},
   "source": [
    "* **Second, we check for the other relations, morphological similarity and irec:constitutes**\n",
    "     * TODO: consider reducing the the amount of features to compute, e.g., only certain distance (maxhops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "fbbcba7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will compute 44504.895 subsets of features (44504895 span combinations)\n"
     ]
    }
   ],
   "source": [
    "total_combinations = sum(1 for ignore in combinations(spans, 2))\n",
    "total_subsets = total_combinations/subset_size\n",
    "print(f\"Will compute {total_subsets} subsets of features ({total_combinations} span combinations)\") \n",
    "# print(f\"Will compute features for {len(spans)} x {len(spans)} = {len(spans) * len(spans)} combinations\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "4b80c289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_list too memory intensive for the amount of combinations, need to yield         \n",
    "def batcher(iterable, batch_size):\n",
    "    iterator = iter(iterable)\n",
    "    while batch := list(islice(iterator, batch_size)):\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "aa299ab1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing feature triples for 44504895 span pairs, in groups of: 5000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                               | 0/9 [00:00<?, ?it/s]\n",
      " 11%|██████████████████▍                                                                                                                                                   | 1/9 [02:39<21:17, 159.72s/it]\n",
      " 22%|████████████████████████████████████▉                                                                                                                                 | 2/9 [07:37<28:07, 241.04s/it]\n",
      " 33%|███████████████████████████████████████████████████████▎                                                                                                              | 3/9 [12:36<26:44, 267.49s/it]\n",
      " 44%|█████████████████████████████████████████████████████████████████████████▊                                                                                            | 4/9 [18:43<25:33, 306.69s/it]\n",
      " 56%|████████████████████████████████████████████████████████████████████████████████████████████▏                                                                         | 5/9 [24:43<21:43, 325.88s/it]\n",
      " 67%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                       | 6/9 [30:26<16:35, 331.68s/it]\n",
      " 78%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                     | 7/9 [36:20<11:17, 338.89s/it]\n",
      " 89%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                  | 8/9 [44:06<06:19, 379.54s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [49:33<00:00, 363.19s/it]\n",
      "9it [49:33, 330.44s/it]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [49:33<00:00, 330.44s/it]\n"
     ]
    }
   ],
   "source": [
    "# Split the processing into multiple parts again and save intermediate states for re-use\n",
    "max_num_cpu_threads = 1024\n",
    "subset_size = 5000000 # 5M\n",
    "\n",
    "feature_files = [f for f in graph_output_fp.glob('features*.pkl')]\n",
    "feature_tuples = []\n",
    "#  (len(spans_c)*len(spans_c)/subset_size) IS NOT CORRECT ANYOMRE\n",
    "if len(feature_files) == math.ceil(total_combinations/subset_size):\n",
    "    for ff in feature_files:\n",
    "        feature_tuples += pickle.load(open(ff, 'rb'))\n",
    "else:\n",
    "    print(f\"Preparing feature triples for {total_combinations} span pairs, in groups of: {subset_size}\")\n",
    "    subset_idx = 0 \n",
    "    with tqdm(total=(total_combinations//subset_size)+1) as pbar:\n",
    "        for subset in tqdm(batcher(combinations(spans_c, 2), subset_size)):\n",
    "            subset_features = []\n",
    "            subset_file_name = graph_output_fp.joinpath(\"features_part_{}.pkl\".format(subset_idx))\n",
    "            subset_idx += 1\n",
    "            if subset_file_name.exists():\n",
    "                print(f\"Already computed '{subset_file_name.stem}' previously, skipping\") \n",
    "                pbar.update(1)\n",
    "                continue\n",
    "\n",
    "            with concurrent.futures.ThreadPoolExecutor(max_workers=max_num_cpu_threads) as executor: # ThreadPoolExecutor\n",
    "                futures = [executor.submit(compute_features, pair) for pair in subset]\n",
    "\n",
    "            subset_features += [f.result() for f in futures if f.result()]\n",
    "            with open(subset_file_name, 'wb') as f:\n",
    "                pickle.dump(subset_features, f)\n",
    "                \n",
    "            pbar.update(1)\n",
    "            \n",
    "    # Once all features are computed; combine them in a single list of tuples to add to the graph\n",
    "    feature_files = [f for f in graph_output_fp.glob('features*.pkl')]\n",
    "    for ff in feature_files:\n",
    "        feature_tuples += pickle.load(open(ff, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "5f5ced48",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuples_to_add = [t[0] for t_list in feature_tuples for t in t_list]\n",
    "irec_graph = add_tuples(irec_graph, tuples_to_add)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf754ed1",
   "metadata": {},
   "source": [
    "#### Save final graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "83372d92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Graph identifier=Ne8b1118113f2479e8db1a9575cc50a79 (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irec_graph.serialize(destination=graph_output_fp.joinpath(\"featured_graph.ttl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca68151",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f857428b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd603c45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
