{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00aa64c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import pickle\n",
    "import glob, os\n",
    "import subprocess\n",
    "import json, random\n",
    "import requests, urllib\n",
    "import concurrent.futures\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from tqdm import tqdm\n",
    "from typing import List, Any, List, Dict, Tuple\n",
    "from pathlib import Path\n",
    "from textblob import TextBlob\n",
    "from threading import current_thread\n",
    "from itertools import islice, product\n",
    "from collections import Counter\n",
    "from nltk.corpus import wordnet as wn\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "from rdflib import URIRef, BNode, Literal, Namespace, Graph\n",
    "from rdflib.namespace import XSD, RDF, RDFS, SKOS, NamespaceManager\n",
    "\n",
    "from utils.spar_utils import TermExtractor\n",
    "from utils.cluster_utils import levenshtein\n",
    "from utils.embedding_utils import Embedder\n",
    "from utils.cleaning_utils import split_list, custom_cleaning_rules, remove_unicode_chars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11663171",
   "metadata": {},
   "source": [
    "We would like to express the following features/relations:\n",
    "* Dictionary definition terms, which are always concepts\n",
    "  * We'll use the source as namespace, and corresponding concept identifier if it exists\n",
    "  * SKOS is used to establish a mapping (e.g., skos:exactMatch) and add the definition (skos:definition)\n",
    "* Special properties that we want to capture between words, which may help identify concepts:\n",
    "  * Word is part of MWE\n",
    "  * Morphologically similar words; stemming & Levenshtein distance\n",
    "  * Semantically similar words; distributed similarity (NNs)\n",
    "  * Acronyms\n",
    "  * Related, this is a generic relation, e.g., a `ampere` is related to `electric current`\n",
    "  * Domain-specificity; foreground or background term following our filtering procedure\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7038359e",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_output_fp = Path.cwd().joinpath(\"data\", \"graph_output\")\n",
    "graph_output_fp.mkdir(parents=True, exist_ok=True) # create directory if it doesn't exist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979b0616",
   "metadata": {},
   "source": [
    "### Prepare namespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2b60e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = Namespace(\"https://example.org/top_concept_for_visulisation/#\")\n",
    "WIKI = Namespace(\"https://www.wikidata.org/wiki/#\")\n",
    "# Note: that UNICLASS is not a namespace (yet) only has identifiers \n",
    "UNICLASS = Namespace(\"https://www.example.org/uniclass/#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4081e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROV = Namespace(\"http://www.w3.org/ns/prov#\")\n",
    "DCT = Namespace(\"http://purl.org/dc/terms/#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "788665e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://www.w3.org/ns/prov'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROV.placeholder.defrag().__reduce__()[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "098e92aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example/placeholder URLs for the IReC project \n",
    "IREC_ontology_URL = \"https://schema.irec.org/#\"\n",
    "IREC_spans_URL = \"https://spans.irec.org/#\"\n",
    "IREC_concepts_URL = \"https://concepts.irec.org/#\"\n",
    "\n",
    "# create our custom namespace for the schema to store spans\n",
    "IREC = Namespace(IREC_ontology_URL)\n",
    "\n",
    "# create a custom namespace to store spans and concepts\n",
    "SPANS = Namespace(IREC_spans_URL)\n",
    "CONCEPTS = Namespace(IREC_concepts_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcef6e56",
   "metadata": {},
   "source": [
    "### graph creation utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2e8ce74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UID_assigner:\n",
    "    def __init__(self):\n",
    "        self.UIDs = {}\n",
    "        self.UID = 0\n",
    "        self.scheme_uids = {}\n",
    "        \n",
    "    def get_scheme_UID(self, namespace: Namespace):\n",
    "        \"\"\"\n",
    "        Determines which type of UID to assign, based on the namespace.\n",
    "        \"\"\"\n",
    "        return [x for x in self.UIDs[namespace._.defrag().__reduce__()[1][0]].values() if x == \"schemeUID\"][0]\n",
    "        \n",
    "    def assign_UID(self, text, namespace: Namespace):\n",
    "        \"\"\"\n",
    "        Determines which type of UID to assign, based on the namespace.\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            raise Exception(\"Not text label provided to assign a UID.\")\n",
    "        if namespace == SPANS:\n",
    "            return self.span_UID(text)\n",
    "        elif namespace == CONCEPTS:\n",
    "            return self.concept_UID(text)\n",
    "        else:\n",
    "            print(\"UID assignment not set up for this namespace, maybe use UID_assigner.keep_track_of_existing_UID()\")\n",
    "            \n",
    "    \n",
    "    def span_UID(self, text):\n",
    "        \"\"\"\n",
    "        NOTE: each text span is a unique identifier in and of itself. We'll simply convert the text span to \n",
    "        a URL friendly representation.\n",
    "        \"\"\"\n",
    "        n_space = SPANS.placeholder.defrag().__reduce__()[1][0]\n",
    "        if n_space not in self.UIDs:\n",
    "            self.UIDs[n_space] = {}\n",
    "        \n",
    "        urltext = urllib.parse.quote(text)\n",
    "        if text not in self.UIDs[n_space]:\n",
    "            self.UIDs[n_space][text] = urltext\n",
    "            \n",
    "        return self.UIDs[n_space][text]\n",
    "        \n",
    "    def concept_UID(self, text):\n",
    "        \"\"\"\n",
    "        For now I'll create my own dumb interger-based UIDs for nodes as a simple shortcut, split per namespace\n",
    "        \"\"\"\n",
    "        n_space = CONCEPTS.placeholder.defrag().__reduce__()[1][0]\n",
    "        \n",
    "        if n_space not in self.UIDs:\n",
    "            self.UIDs[n_space] = {}\n",
    "        \n",
    "        if text not in self.UIDs[n_space]:\n",
    "            self.UID += 1\n",
    "            self.UIDs[n_space][text] = str(self.UID)\n",
    "        return self.UIDs[n_space][text]\n",
    "        \n",
    "    def keep_track_of_existing_UID(self, text:str, existing_uid: str, namespace:Namespace):\n",
    "        \"\"\"\n",
    "        Simply keep track of UIDs that exist in the provided namespace.\n",
    "        \"\"\"\n",
    "        n_space = namespace.placeholder.defrag().__reduce__()[1][0]\n",
    "        if n_space not in self.UIDs:\n",
    "            self.UIDs[n_space] = {}\n",
    "            \n",
    "        if text not in self.UIDs[n_space]:\n",
    "            # already seen by this UID assigner\n",
    "            self.UIDs[n_space][text] = existing_uid\n",
    "            \n",
    "        return existing_uid\n",
    "    \n",
    "    def retrieve_uid_by_text(self, node_text, namespace: Namespace = SPANS):\n",
    "        n_space = namespace.placeholder.defrag().__reduce__()[1][0]\n",
    "        if node_text in self.UIDs[n_space]:\n",
    "            return self.UIDs[n_space][node_text]\n",
    "        else:\n",
    "            return None \n",
    "        \n",
    "    def count_nodes_in_namespace(self, namespace: Namespace = SPANS):\n",
    "        n_space = namespace.placeholder.defrag().__reduce__()[1][0]\n",
    "        print(f\"Number of nodes in '{n_space}': {len(self.UIDs[n_space])}\")\n",
    "        return len(self.UIDs[n_space])\n",
    "        \n",
    "    def print_node_by_id(self, graph, node_id, namespace: Namespace = SPANS):\n",
    "        for s, p, o in graph.triples((namespace[str(node_id)],  None, None)):\n",
    "            print(f\"{s.split('#')[-1]} ; {p.split('#')[-1]} ; {o.split('#')[-1]}\")\n",
    "        \n",
    "    def print_node_by_text(self, graph, node_text, namespace: Namespace = SPANS):\n",
    "        n_space = namespace.placeholder.defrag().__reduce__()[1][0]\n",
    "        node_id = self.UIDs[n_space][node_text]\n",
    "        # find all triples with subject\n",
    "        for s, p, o in graph.triples((namespace[node_id],  None, None)):\n",
    "            print(f\"{s.split('#')[-1]} ; {p.split('#')[-1]} ; {o.split('#')[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "772b6692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These wrappers only exist to help me consistently add nodes to the graph\n",
    "\n",
    "def dct_title(node_uid: str, title: str, namespace: Namespace) -> List[Tuple]:\n",
    "    return [(namespace[node_uid], DCT.title,  Literal(title, lang='en'))]\n",
    "\n",
    "def provenance(node_uid: str, source: URIRef, namespace: Namespace) -> List[Tuple]:\n",
    "    \"\"\" temp source attribution \"\"\"\n",
    "    return [(namespace[node_uid], PROV.hadPrimarySource, source)]\n",
    "\n",
    "def prov_agent(node_uid: str, agent: URIRef, namespace: Namespace) -> List[Tuple]:\n",
    "    \"\"\" temp agent attribution (for Spans generated by SPaR.txt) \"\"\"\n",
    "    return [(namespace[node_uid], PROV.wasAttributedTo, agent)]\n",
    "\n",
    "\n",
    "# SKOS \n",
    "def skos_scheme(node_uid, namespace: Namespace=CONCEPTS) -> List[Tuple]:\n",
    "    \"\"\" Node that identifies the concep scheme with a URI, expecting/using as scheme root \"\"\"\n",
    "    return [(namespace[node_uid], RDF.type, SKOS.ConceptScheme)]\n",
    "\n",
    "def skos_top_concept(node_uid, top_concept_uid, \n",
    "                    namespace: Namespace=CONCEPTS, top_concept_namespace: Namespace=CONCEPTS) -> List[Tuple]:\n",
    "    \"\"\" Currently, we mainly use the top-concept for visualisation. \"\"\"\n",
    "    return [(namespace[node_uid], SKOS.hasTopConcept, top_concept_namespace[top_concept_uid]),\n",
    "            (top_concept_namespace[top_concept_uid], SKOS.topConceptOf, namespace[node_uid])]\n",
    "\n",
    "def skos_in_scheme(node_uid, scheme_uid, namespace: Namespace=CONCEPTS, scheme_namespace: Namespace=CONCEPTS) -> List[Tuple]:\n",
    "    \"\"\" Keep track of the scheme/source of a node. \"\"\"\n",
    "    return [(namespace[node_uid], SKOS.inScheme, scheme_namespace[scheme_uid])]\n",
    "\n",
    "def skos_node(node_uid, text, namespace: Namespace=CONCEPTS) -> List[Tuple]:\n",
    "    \"\"\" Add a concept with prefLabel to the graph in the CONCEPTS namespace, of type SKOS.Concept \"\"\"\n",
    "    return [(namespace[node_uid], RDF.type, SKOS.Concept), \n",
    "            (namespace[node_uid], SKOS.prefLabel, Literal(text, lang='en'))]\n",
    "\n",
    "def skos_prefLabel(node_uid, text, namespace: Namespace=CONCEPTS) -> List[Tuple]:\n",
    "    \"\"\" Add the text label for a node \"\"\"\n",
    "    return [(namespace[node_uid], SKOS.prefLabel, Literal(text, lang='en'))]\n",
    "\n",
    "def skos_altLabel(node_uid, alt_label_uid, namespace: Namespace=CONCEPTS)-> List[Tuple]:\n",
    "    \"\"\" Add an alternative text label for a concept node \"\"\"\n",
    "    return [(namespace[node_uid], SKOS.altLabel, namespace[alt_label_uid]), \n",
    "            (namespace[alt_label_uid], SKOS.altLabel, namespace[node_uid])]\n",
    "\n",
    "def skos_exact_match(subject_node_uid, object_node_uid,\n",
    "                subject_namespace: Namespace=SPANS, object_namespace: Namespace=CONCEPTS) -> List[Tuple]:\n",
    "    \"\"\" Denotes an exact match between two nodes, would expect the nodes to be in different vocabularies \"\"\"\n",
    "    return [(subject_namespace[subject_node_uid], SKOS.exactMatch, object_namespace[object_node_uid])]\n",
    "\n",
    "def skos_related(subject_node_uid, object_node_uid,\n",
    "                subject_namespace: Namespace=SPANS, object_namespace: Namespace=CONCEPTS) -> List[Tuple]:\n",
    "    \"\"\" Denotes a relation between two nodes, would expect the nodes to be in different vocabularies \"\"\"\n",
    "    return [(subject_namespace[subject_node_uid], SKOS.related, object_namespace[object_node_uid])]\n",
    "    \n",
    "def skos_broader(narrower_node_uid, broader_node_uid, \n",
    "                 narrower_namespace: Namespace=CONCEPTS, broader_namespace: Namespace=CONCEPTS) -> List[Tuple]:\n",
    "    \"\"\" Assuming narrower/broader is always reflexive, would expect the nodes to be in different vocabularies \"\"\"\n",
    "    return [(broader_namespace[narrower_node_uid], SKOS.narrower, narrower_namespace[broader_node_uid]),\n",
    "            (narrower_namespace[broader_node_uid], SKOS.broader, broader_namespace[narrower_node_uid])]\n",
    "    \n",
    "def skos_definition(node_uid, definition_text, namespace: Namespace=CONCEPTS) -> List[Tuple]:\n",
    "    \"\"\" Add a definition, if provided in the Merged Approved Documents \"\"\"\n",
    "    return [(namespace[node_uid], SKOS.definition, Literal(definition_text, lang='en'))]\n",
    "\n",
    "def skos_note(node_uid, note_text, namespace: Namespace=CONCEPTS) -> List[Tuple]:\n",
    "    \"\"\" Some notes exist in the approved docs at least, containing useful information \"\"\"\n",
    "    return [(namespace[node_uid], SKOS.note, Literal(note_text, lang='en'))]\n",
    "\n",
    "\n",
    "\n",
    "# IREC functions and REFERENCE\n",
    "IREC.CharacterSpan # A span is a sequence of characters that occurs verbatim in a text, either contiguous or discontiguos as extracted by SPaR.txt (Kruiper et al., 2021).   \n",
    "IREC.constitutes  # Indicates that a span constitutes another span, e.g., the Multi-Word Expression (MWE) Span `hot water storage system` the Span `storage`.\n",
    "IREC.isMorphologicallySimilarTo # Indicates that a Span is morphologically similar to another Span, e.g., they may have the same stem or a small Levenshtein distance.\n",
    "IREC.isSemanticallySimilarTo # Indicates that a Span is semantically similar to another Span, following a cosine similarity between their  embeddings.\n",
    "IREC.related # General way to indicate some relation between two spans, e.g., `ampere` is related to `electric current`\n",
    "IREC.hasAcronym # A Span can have an acronym, e.g., `British Standards Institute` has the acronym `BSI`.\n",
    "IREC.isAcronymOf # A Span can have an acronym, e.g., `BSI` is the acronym for `British Standards Institute`.\n",
    "IREC.hasAntonym # Property that relates a Span to another Span, each being each other's antonyms.\n",
    "IREC.wikiDefinition # One of potentially multiple WikiData definitions for the irec:CharacterSpan node.\n",
    "IREC.wikiClassLabel # One of potentially multiple WikiData class labels for the irec:CharacterSpan node.\n",
    "\n",
    "def irec_span(node_uid, text, namespace: Namespace=SPANS) -> List[Tuple]:\n",
    "    \"\"\" Add a span node in the SPANS namespace, of type IREC.CharacterSpan and the span text set as its RDF.label \"\"\"\n",
    "    # is preflabel a property? I would assume so\n",
    "    return [(namespace[node_uid], RDF.type, IREC.CharacterSpan), \n",
    "            (namespace[node_uid], RDFS.label,  Literal(text, lang='en'))]\n",
    "\n",
    "def irec_constitutes(subject_node_uid, object_node_uid,\n",
    "                     subject_namespace: Namespace=SPANS, object_namespace: Namespace=SPANS) -> List[Tuple]:\n",
    "    \"\"\" Indicates that somewhere in the label of the first SPAN node, you can find the second span's label \"\"\"\n",
    "    return [(subject_namespace[subject_node_uid], IREC.constitutes, object_namespace[object_node_uid])]\n",
    "\n",
    "def irec_morp_sim(subject_node_uid, object_node_uid,\n",
    "                  subject_namespace: Namespace=SPANS, object_namespace: Namespace=SPANS) -> List[Tuple]:\n",
    "    \"\"\" Indicates that the labels of two SPAN nodes are morphologically similar \"\"\"\n",
    "    return [(subject_namespace[subject_node_uid], IREC.isMorphologicallySimilarTo, object_namespace[object_node_uid])]\n",
    "\n",
    "def irec_sem_sim(subject_node_uid, object_node_uid,\n",
    "                 subject_namespace: Namespace=SPANS, object_namespace: Namespace=SPANS) -> List[Tuple]:\n",
    "    \"\"\" Indicates that the labels of two SPAN nodes are semantically similar, following the distributed semantics hypothesis \"\"\"\n",
    "    return [(subject_namespace[subject_node_uid], IREC.isSemanticallySimilarTo, object_namespace[object_node_uid])]\n",
    "\n",
    "def irec_related(subject_node_uid, object_node_uid,\n",
    "                 subject_namespace: Namespace=SPANS, object_namespace: Namespace=SPANS) -> List[Tuple]:\n",
    "    \"\"\" Indicates that a Span is related in SOME way to another Span. \"\"\"\n",
    "    return [(subject_namespace[subject_node_uid], IREC.related, object_namespace[object_node_uid])]\n",
    "\n",
    "def irec_has_acronym(subject_node_uid, object_node_uid,\n",
    "                     subject_namespace: Namespace=SPANS, object_namespace: Namespace=SPANS) -> List[Tuple]:\n",
    "    \"\"\" Indicates that the label of the subject node has an acronym, ergo the label of the object node  \"\"\"\n",
    "    return [(subject_namespace[subject_node_uid], IREC.hasAcronym, object_namespace[object_node_uid]),\n",
    "            (object_namespace[object_node_uid], IREC.isAcronymOf, subject_namespace[subject_node_uid])]\n",
    "\n",
    "def irec_antonym(subject_node_uid, object_node_uid,\n",
    "                 subject_namespace: Namespace=SPANS, object_namespace: Namespace=SPANS) -> List[Tuple]:\n",
    "    \"\"\" Indicates that the label of the subject node is an antonym of the label of the object node  \"\"\"\n",
    "    return [(subject_namespace[subject_node_uid], IREC.hasAntonym, object_namespace[object_node_uid])]\n",
    "\n",
    "def irec_wikidef(node_uid, definition_text, namespace: Namespace=SPANS) -> List[Tuple]:\n",
    "    \"\"\" Add a candidate definition, as retrieved from WikiData for the Span \"\"\"\n",
    "    return [(namespace[node_uid], IREC.wikiDefinition, Literal(definition_text, lang='en'))]\n",
    "\n",
    "def irec_wikiclass(node_uid, definition_text, namespace: Namespace=SPANS) -> List[Tuple]:\n",
    "    \"\"\" Add a candidate class label, as retrieved from WikiData for the Span \"\"\"\n",
    "    return [(namespace[node_uid], IREC.wikiDefinition, Literal(definition_text, lang='en'))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08060b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_tuples(graph, tuples):\n",
    "    \"\"\"\n",
    "    We'll never add the same tuple twice to a graph\n",
    "    \"\"\"\n",
    "    for t in tuples:\n",
    "        assert len(t) == 3\n",
    "    [graph.add(t) for t in tuples if t not in graph]\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fdc557",
   "metadata": {},
   "source": [
    "### Prepare graph\n",
    "* Currently creating a single graph to hold all information. \n",
    "* Relevant information gathered from external resources is added; primarily class labels and definitions from WikiData."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54a1e1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "irec_graph = Graph()\n",
    "\n",
    "irec_graph.bind(\"root\", ROOT)\n",
    "irec_graph.bind(\"wikipedia\", WIKI)\n",
    "irec_graph.bind(\"uniclass\", UNICLASS)\n",
    "irec_graph.bind(\"dct\", DCT)\n",
    "irec_graph.bind(\"prov\", PROV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56acbeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bind our vocabulary of classes/relations\n",
    "graph_data_fp = Path.cwd().joinpath(\"data\", \"graph_data\")\n",
    "irec_graph.parse(graph_data_fp.joinpath(\"IREC.rdf\"))\n",
    "irec_graph.bind(\"spans\", SPANS)\n",
    "irec_graph.bind(\"concepts\", CONCEPTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ccd4a8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# primary sources and agents\n",
    "irec_IRI = URIRef(\"https://github.com/rubenkruiper/irec\")\n",
    "\n",
    "merged_approved_documents_IRI = URIRef(\"https://www.gov.uk/government/collections/approved-documents\")\n",
    "wikidata_IRI = URIRef(\"https://www.wikidata.org/\")\n",
    "uniclass_IRI = URIRef(\"https://en.wikipedia.org/wiki/Uniclass\")\n",
    "spart_txt_IRI = URIRef(\"http://dx.doi.org/10.18653/v1/2021.nllp-1.14\")\n",
    "\n",
    "irec_graph = add_tuples(irec_graph, \n",
    "                        [\n",
    "                            (irec_IRI, PROV.type, PROV.PrimarySource),\n",
    "                            (merged_approved_documents_IRI, PROV.type, PROV.PrimarySource),\n",
    "                            (wikidata_IRI, PROV.type, PROV.PrimarySource),\n",
    "                            (uniclass_IRI, PROV.type, PROV.PrimarySource),\n",
    "                            (spart_txt_IRI, PROV.type, PROV.SoftwareAgent)       \n",
    "                        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67b6fd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_scheme_uid(graph: Graph, primary_source: URIRef, scheme_name: str, scheme_uid_label:str, namespace: Namespace) -> Graph:\n",
    "    # We'll set the UID ourselves\n",
    "    scheme_uid = ua.keep_track_of_existing_UID(scheme_name, scheme_uid_label, namespace)\n",
    "    # add title\n",
    "    graph = add_tuples(graph, dct_title(scheme_uid, scheme_name, namespace)) \n",
    "    # add source note  \n",
    "    graph = add_tuples(graph, provenance(scheme_uid, primary_source, namespace)) \n",
    "    # is of type skos:ConceptScheme\n",
    "    graph = add_tuples(graph, skos_scheme(scheme_uid, SPANS))\n",
    "    # self-reference being in scheme\n",
    "    graph = add_tuples(graph, skos_in_scheme(scheme_uid, scheme_uid, namespace, namespace))\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbb5024e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ua = UID_assigner()\n",
    "\n",
    "# global UIDs for the schemes we'll be using\n",
    "irec_graph = add_scheme_uid(irec_graph, irec_IRI, \"IREC spans\", \"schemeUID\", SPANS)\n",
    "irec_graph = add_scheme_uid(irec_graph, irec_IRI, \"IREC concepts\", \"schemeUID\", CONCEPTS)\n",
    "\n",
    "# irec_graph = add_scheme_uid(irec_graph, \"IREC WikiData concepts\", \"schemeUID\", WIKI) # NON EXISTENT NODE\n",
    "# irec_graph = add_scheme_uid(irec_graph, \"IREC Uniclass concepts\", \"schemeUID\", UNICLASS)  # NON EXISTENT NODE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fceef5",
   "metadata": {},
   "source": [
    "### Add base antonyms\n",
    "* We may want to get a sense of which spans are antonyms\n",
    "* For this we'll use NLTK's version of WordNet, which mainly captures antonyms for adjectives and adverbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7fe6de0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cold']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet_antonyms = {}\n",
    "for i in wn.all_synsets():\n",
    "    if i.pos() in ['a', 's']:    # If synset is adj or satelite-adj.\n",
    "        for j in i.lemmas():     # Iterating through lemmas for each synset.\n",
    "            if j.antonyms():     # If adj has antonym.\n",
    "                wordnet_antonyms[str(j.name()).strip()] = [x.name() for x in j.antonyms()]\n",
    "\n",
    "# Example of a useful antonym for us\n",
    "wordnet_antonyms['hot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de3d0ae4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hot']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet_antonyms['cold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a9692dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('acidic', ['alkaline', 'amphoteric']),\n",
       " ('alkaline', ['amphoteric', 'acidic']),\n",
       " ('amphoteric', ['acidic', 'alkaline']),\n",
       " ('air-to-surface', ['air-to-air', 'surface-to-air']),\n",
       " ('air-to-air', ['surface-to-air', 'air-to-surface']),\n",
       " ('surface-to-air', ['air-to-surface', 'air-to-air']),\n",
       " ('anadromous', ['catadromous', 'diadromous']),\n",
       " ('catadromous', ['diadromous', 'anadromous']),\n",
       " ('diadromous', ['anadromous', 'catadromous']),\n",
       " ('aquatic', ['terrestrial', 'amphibious']),\n",
       " ('terrestrial', ['amphibious', 'aquatic']),\n",
       " ('amphibious', ['aquatic', 'terrestrial']),\n",
       " ('prenatal', ['perinatal', 'postnatal']),\n",
       " ('perinatal', ['postnatal', 'prenatal']),\n",
       " ('postnatal', ['prenatal', 'perinatal']),\n",
       " ('sonic', ['subsonic', 'supersonic']),\n",
       " ('subsonic', ['supersonic', 'sonic']),\n",
       " ('supersonic', ['sonic', 'subsonic']),\n",
       " ('binucleate', ['trinucleate', 'mononuclear']),\n",
       " ('mononuclear', ['binucleate', 'trinucleate']),\n",
       " ('trinucleate', ['mononuclear', 'binucleate']),\n",
       " ('lower-class', ['middle-class', 'upper-class']),\n",
       " ('middle-class', ['upper-class', 'lower-class']),\n",
       " ('upper-class', ['lower-class', 'middle-class']),\n",
       " ('carnivorous', ['herbivorous', 'omnivorous', 'insectivorous']),\n",
       " ('herbivorous', ['omnivorous', 'insectivorous', 'carnivorous']),\n",
       " ('omnivorous', ['insectivorous', 'carnivorous', 'herbivorous']),\n",
       " ('insectivorous', ['carnivorous', 'herbivorous', 'omnivorous']),\n",
       " ('acatalectic', ['catalectic', 'hypercatalectic']),\n",
       " ('catalectic', ['hypercatalectic', 'acatalectic']),\n",
       " ('hypercatalectic', ['acatalectic', 'catalectic']),\n",
       " ('isotonic', ['hypertonic', 'hypotonic']),\n",
       " ('unconventional', ['conventional', 'conventional']),\n",
       " ('cubic', ['linear', 'planar']),\n",
       " ('planar', ['cubic', 'linear']),\n",
       " ('annual', ['biennial', 'perennial']),\n",
       " ('biennial', ['perennial', 'annual']),\n",
       " ('perennial', ['annual', 'biennial']),\n",
       " ('sharp', ['flat', 'natural']),\n",
       " ('early', ['middle', 'late']),\n",
       " ('middle', ['late', 'early']),\n",
       " ('late', ['early', 'middle']),\n",
       " ('ectomorphic', ['endomorphic', 'mesomorphic']),\n",
       " ('endomorphic', ['mesomorphic', 'ectomorphic']),\n",
       " ('mesomorphic', ['ectomorphic', 'endomorphic']),\n",
       " ('endogamous', ['exogamous', 'autogamous']),\n",
       " ('exogamous', ['autogamous', 'endogamous']),\n",
       " ('autogamous', ['endogamous', 'exogamous']),\n",
       " ('flat', ['natural', 'sharp']),\n",
       " ('nonspecific', ['specific', 'specific']),\n",
       " ('endemic', ['epidemic', 'ecdemic']),\n",
       " ('haploid', ['diploid', 'polyploid']),\n",
       " ('diploid', ['polyploid', 'haploid']),\n",
       " ('polyploid', ['haploid', 'diploid']),\n",
       " ('heterosexual', ['homosexual', 'bisexual']),\n",
       " ('homosexual', ['bisexual', 'heterosexual']),\n",
       " ('bisexual', ['heterosexual', 'homosexual']),\n",
       " ('homologous', ['analogous', 'heterologous']),\n",
       " ('heterologous', ['analogous', 'homologous']),\n",
       " ('autologous', ['homologous', 'heterologous']),\n",
       " ('analogous', ['homologous', 'heterologous']),\n",
       " ('horizontal', ['vertical', 'inclined']),\n",
       " ('vertical', ['inclined', 'horizontal']),\n",
       " ('vernal', ['summery', 'autumnal', 'wintry']),\n",
       " ('summery', ['autumnal', 'wintry', 'vernal']),\n",
       " ('autumnal', ['wintry', 'vernal', 'summery']),\n",
       " ('wintry', ['vernal', 'summery', 'autumnal']),\n",
       " ('introversive', ['extroversive', 'ambiversive']),\n",
       " ('extroversive', ['ambiversive', 'introversive']),\n",
       " ('ambiversive', ['introversive', 'extroversive']),\n",
       " ('leptorrhine', ['catarrhine', 'platyrrhine']),\n",
       " ('catarrhine', ['leptorrhine', 'platyrrhine']),\n",
       " ('platyrrhine', ['catarrhine', 'leptorrhine']),\n",
       " ('epidemic', ['endemic', 'ecdemic']),\n",
       " ('ecdemic', ['endemic', 'epidemic']),\n",
       " ('mini', ['midi', 'maxi']),\n",
       " ('midi', ['maxi', 'mini']),\n",
       " ('maxi', ['mini', 'midi']),\n",
       " ('male', ['female', 'androgynous']),\n",
       " ('female', ['androgynous', 'male']),\n",
       " ('androgynous', ['male', 'female']),\n",
       " ('masculine', ['feminine', 'neuter']),\n",
       " ('feminine', ['neuter', 'masculine']),\n",
       " ('neuter', ['masculine', 'feminine']),\n",
       " ('univalent', ['bivalent', 'multivalent']),\n",
       " ('bivalent', ['multivalent', 'univalent']),\n",
       " ('multivalent', ['univalent', 'bivalent']),\n",
       " ('natural', ['sharp', 'flat']),\n",
       " ('hypertensive', ['hypotensive', 'normotensive']),\n",
       " ('hypotensive', ['normotensive', 'hypertensive']),\n",
       " ('normotensive', ['hypertensive', 'hypotensive']),\n",
       " ('one-piece', ['two-piece', 'three-piece']),\n",
       " ('two-piece', ['three-piece', 'one-piece']),\n",
       " ('three-piece', ['one-piece', 'two-piece']),\n",
       " ('parallel', ['perpendicular', 'oblique']),\n",
       " ('oblique', ['parallel', 'perpendicular']),\n",
       " ('perpendicular', ['oblique', 'parallel']),\n",
       " ('past', ['present', 'future']),\n",
       " ('future', ['past', 'present']),\n",
       " ('neutral', ['positive', 'negative']),\n",
       " ('quantitative', ['syllabic', 'accentual']),\n",
       " ('right-handed', ['left-handed', 'ambidextrous']),\n",
       " ('left-handed', ['ambidextrous', 'right-handed']),\n",
       " ('ambidextrous', ['right-handed', 'left-handed']),\n",
       " ('center', ['right', 'left']),\n",
       " ('liquid', ['gaseous', 'solid']),\n",
       " ('gaseous', ['solid', 'liquid']),\n",
       " ('some', ['no', 'all']),\n",
       " ('no', ['all', 'some']),\n",
       " ('all', ['some', 'no']),\n",
       " ('syllabic', ['accentual', 'quantitative']),\n",
       " ('accentual', ['quantitative', 'syllabic']),\n",
       " ('superscript', ['subscript', 'adscript']),\n",
       " ('subscript', ['adscript', 'superscript']),\n",
       " ('adscript', ['superscript', 'subscript']),\n",
       " ('top', ['bottom', 'side']),\n",
       " ('bottom', ['side', 'top']),\n",
       " ('side', ['top', 'bottom']),\n",
       " ('surface', ['subsurface', 'overhead']),\n",
       " ('subsurface', ['overhead', 'surface']),\n",
       " ('overhead', ['surface', 'subsurface']),\n",
       " ('viviparous', ['oviparous', 'ovoviviparous']),\n",
       " ('oviparous', ['ovoviviparous', 'viviparous']),\n",
       " ('ovoviviparous', ['viviparous', 'oviparous']),\n",
       " ('xeric', ['hydric', 'mesic']),\n",
       " ('hydric', ['mesic', 'xeric']),\n",
       " ('mesic', ['xeric', 'hydric'])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# there are cases of multiple antonyms:\n",
    "[(k, wordnet_antonyms[k]) for k, v in wordnet_antonyms.items() if len(v) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "afd035fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_uid = URIRef(\"https://www.wikidata.org/wiki/Q533822\")\n",
    "for span in wordnet_antonyms.keys():\n",
    "    span_uid = ua.assign_UID(span, SPANS)\n",
    "    \n",
    "    irec_graph = add_tuples(irec_graph, irec_span(span_uid, span))\n",
    "    irec_graph = add_tuples(irec_graph, skos_in_scheme(span_uid, 'schemeUID', SPANS, SPANS))\n",
    "    irec_graph = add_tuples(irec_graph, provenance(span_uid, wordnet_uid, SPANS))\n",
    "\n",
    "    antonyms = wordnet_antonyms[span]\n",
    "    for antonym in antonyms:\n",
    "        antonym_uid = ua.assign_UID(antonym, SPANS)\n",
    "\n",
    "        irec_graph = add_tuples(irec_graph, irec_span(antonym_uid, antonym))\n",
    "        irec_graph = add_tuples(irec_graph, skos_in_scheme(antonym_uid, 'schemeUID', SPANS, SPANS))\n",
    "        irec_graph = add_tuples(irec_graph, provenance(antonym_uid, wordnet_uid, SPANS))\n",
    "\n",
    "        # add the antonym relation\n",
    "        irec_graph = add_tuples(irec_graph, irec_antonym(span_uid, antonym_uid))\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41f63e5",
   "metadata": {},
   "source": [
    "### Add domain terms extracted from the Approved documents as Spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8178ad1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_terms = pickle.load(open(graph_data_fp.joinpath('domain_terms.pkl'), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07f4b0f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['turning facilities',\n",
       " 'energy meters',\n",
       " 'The output report',\n",
       " 'the smaller',\n",
       " 'concrete floor slabs',\n",
       " 'the supporting structure',\n",
       " 'Continuous',\n",
       " 'fixed system',\n",
       " 'an employer',\n",
       " 'Timber']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(domain_terms, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c506907d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simply adding the extracted spans\n",
    "domain_terms = custom_cleaning_rules(domain_terms)\n",
    "for span in domain_terms:\n",
    "    span_uid = ua.assign_UID(span, SPANS)\n",
    "    \n",
    "    irec_graph = add_tuples(irec_graph, irec_span(span_uid, span))\n",
    "    irec_graph = add_tuples(irec_graph, skos_in_scheme(span_uid, 'schemeUID', SPANS, SPANS))\n",
    "    irec_graph = add_tuples(irec_graph, provenance(span_uid, merged_approved_documents_IRI, SPANS))\n",
    "    # add agent for spans generated by SPaR.txt\n",
    "    irec_graph = add_tuples(irec_graph, prov_agent(span_uid, spart_txt_IRI, SPANS))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8c4c91",
   "metadata": {},
   "source": [
    "### Add Acronyms that were grabbed from the text\n",
    "\n",
    "These can help:\n",
    "* remove terms where the boundary detection is off\n",
    "* avoid suggesting similar acronyms, e.g., suggest that EPC and EPS are similar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "434b8738",
   "metadata": {},
   "outputs": [],
   "source": [
    "acronyms = {'PAS': ['ecification', 'Specification'],  'GSIUR': ['Regulations 1998'],  'HSE': ['Regulations 2000',   'water systems',   'Safety Executive',   'Health and Safety Executive'],  'PE': ['Polyethylene', 'polyethylene'],  'DN': ['pipe'],  'DCLG': ['land', 'Local Government', 'England', 'ment'],  'PP': ['Polypropylene'],  'BCB': ['Control Body',   'the building control body',   'Building control body',   'building control body',   'Building Control Body'],  'SRHRV': ['ventilator',   'single room heat recovery ventilator',   'a single room heat recovery ventilator'],  'MVHR': ['blocks', 'heat recovery'],  'WC': ['sets'],  'TFA': ['the total floor area'],  'LRV': ['Light reflectance value'],  'BER': ['Building CO2 Emission Rate', 'CO2 Emission Rate'],  'TER': ['CO2 Emission Rate',   'the Target CO2 Emission Rate',   'Target CO2 Emission Rate'],  'DER': ['CO2 Emission Rate', 'the Dwelling CO2 Emission Rate'],  'EPC': ['energy performance certificate'],  'TFEE': ['Target Fabric Energy Efficiency',   'Fixed building services',   'Energy Efficiency'],  'DHF': ['the Door and Hardware Federation', 'Door and Hardware Federation'],  'REI': ['fire resistance', 'bility'],  'PHE': ['horizontal evacuation'],  'W': ['the final exit', 'final exit'],  'DWELLINGS': ['RESIDENTIAL'],  'OTHER': ['RESIDENTIAL'],  'TSO': ['Office', 'The Stationery Office'],  'FPA': ['the Fire Protection Association', 'Association'],  'A': ['absorption area'],  'AT': ['absorption area'],  'DECC': ['Climate Change'],  'NCM': ['the National Calculation Methodology'],  'ADCAS': ['Allied Services'],  'DFEE': ['Energy Efficiency'],  'LPA': ['the local planning authority', 'planning authority'],  'UKAS': ['the United Kingdom Accreditation Service'],  'BSI': ['the British Standards Institution'],  'EA': ['Accreditation'],  'BGS': ['British Geological Survey'],  'HBN': ['Notes'],  'GGF': ['Glazing Federation'],  'E': ['terms of integrity'],  'TRADA': ['the Timber Research and Development Association', 'Association'],  'ACOP': ['Code of Practice'],  'ATTMA': ['Association'],  'RVA': ['Association', 'the Residential Ventilation Association'],  'TEHVA': ['Association'],  'DSA': ['Association'],  'CIRIA': ['Association'],  'MCRMA': ['Association'],  'DSMA': ['Association'],  'OFTEC': ['Association'],  'WHO': ['Organisation'],  'GAI': ['Architectural Ironmongers'],  'MEV': ['mechanical extract', 'extract ventilation'],  'VST': ['Vicat softening temperature'],  'SCI': ['Guild Steel Construction Institute'],  'FBE': ['the Built Environment', 'ment'],  'DSER': ['Rating'],  'WER': ['Rating'],  'CIWM': ['ment', 'Wastes Management'],  'EOTA': ['ment'],  'GQRA': ['ment'],  'BRE': ['ment', 'the Building Research Establishment'],  'PPS': ['ment'],  'PSV': ['Passive stack ventilation'],  'EST': ['the Energy Saving Trust'],  'CIBSE': ['Ventilation hygiene toolkit', 'Building Services Engineers'],  'AGS': ['Geoenvironmental Specialists'],  'SPAB': ['Ancient Buildings'],  'UF': ['urea formaldehyde'],  'ODPM': ['the Deputy Prime Minister']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aaf33be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for acronym, spans in acronyms.items():\n",
    "    \n",
    "    acronym_uid = ua.assign_UID(acronym, SPANS)\n",
    "   \n",
    "    irec_graph = add_tuples(irec_graph, irec_span(acronym_uid, acronym))\n",
    "    irec_graph = add_tuples(irec_graph, skos_in_scheme(acronym_uid, 'schemeUID', SPANS, SPANS))\n",
    "    irec_graph = add_tuples(irec_graph, provenance(acronym_uid, merged_approved_documents_IRI, SPANS))\n",
    "    \n",
    "    for span in spans:\n",
    "        span_uid = ua.assign_UID(span, SPANS) \n",
    "        irec_graph = add_tuples(irec_graph, irec_span(span_uid, span))\n",
    "        irec_graph = add_tuples(irec_graph, skos_in_scheme(span_uid, 'schemeUID', SPANS, SPANS))\n",
    "        irec_graph = add_tuples(irec_graph, provenance(span_uid, merged_approved_documents_IRI, SPANS))\n",
    "\n",
    "        # These spans should have been \n",
    "        \n",
    "        # todo; \n",
    "        #  could do some filtering here of the clearly erroneous span-acronym combinations\n",
    "        #  or leave this until later, using the graph...\n",
    "    \n",
    "        irec_graph = add_tuples(irec_graph, irec_has_acronym(acronym_uid, span_uid))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953c8e57",
   "metadata": {},
   "source": [
    "### Add CONCEPTS: defined terms from the Approved Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1af2368f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from csv file\n",
    "definitions = pd.read_excel(graph_data_fp.joinpath(\"Approved Documents and derived terms.xlsx\"), sheet_name=\"Definitions\", keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "77530dc1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Term</th>\n",
       "      <th>Definition</th>\n",
       "      <th>Alternative labels</th>\n",
       "      <th>Note</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Absorption</td>\n",
       "      <td>Conversion of sound energy to heat, often by t...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Absorption coefficient</td>\n",
       "      <td>A quantity characterising the effectiveness of...</td>\n",
       "      <td></td>\n",
       "      <td>See BS EN 20354:1993.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Absorptive material</td>\n",
       "      <td>Material that absorbs sound energy.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Term                                         Definition  \\\n",
       "0              Absorption  Conversion of sound energy to heat, often by t...   \n",
       "1  Absorption coefficient  A quantity characterising the effectiveness of...   \n",
       "2     Absorptive material                Material that absorbs sound energy.   \n",
       "\n",
       "  Alternative labels                   Note  \n",
       "0                                            \n",
       "1                     See BS EN 20354:1993.  \n",
       "2                                            "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "definitions[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9d066368",
   "metadata": {},
   "outputs": [],
   "source": [
    "concepts_definitions_dict = {} # keep track of definitions for parsing later\n",
    "\n",
    "# create graph from definitions first\n",
    "for i, row in definitions.iloc[1:].iterrows():\n",
    "    # These terms all start with a capital; lowercase them\n",
    "    term = row['Term'] if row['Term'].isupper() else row['Term'].lower()\n",
    "    alternative_labels = row['Alternative labels']\n",
    "    definition = row['Definition']\n",
    "    note = row['Note']\n",
    "\n",
    "    # add the term as a CONCEPT and as a SPAN\n",
    "    concept_uid = ua.assign_UID(term, CONCEPTS)\n",
    "    irec_graph = add_tuples(irec_graph, skos_node(concept_uid, term))\n",
    "    irec_graph = add_tuples(irec_graph, skos_in_scheme(concept_uid, 'schemeUID', CONCEPTS, CONCEPTS))\n",
    "    irec_graph = add_tuples(irec_graph, provenance(concept_uid, merged_approved_documents_IRI, CONCEPTS))\n",
    "    \n",
    "    span_uid = ua.assign_UID(term, SPANS)\n",
    "    irec_graph = add_tuples(irec_graph, irec_span(span_uid, term))\n",
    "    irec_graph = add_tuples(irec_graph, skos_in_scheme(span_uid, 'schemeUID', SPANS, SPANS))\n",
    "    irec_graph = add_tuples(irec_graph, provenance(span_uid, merged_approved_documents_IRI, SPANS))\n",
    "    \n",
    "    # link the concept and the span # as a skos:exactMatch? or smt else?\n",
    "    irec_graph = add_tuples(irec_graph, skos_exact_match(concept_uid, span_uid, CONCEPTS, SPANS))\n",
    "    \n",
    "    # always expecting a definition\n",
    "    irec_graph = add_tuples(irec_graph, skos_definition(concept_uid, definition))\n",
    "    \n",
    "    if note: \n",
    "        irec_graph = add_tuples(irec_graph, skos_note(concept_uid, note))\n",
    "    \n",
    "    if alternative_labels:\n",
    "        # These terms all start with a capital; lowercase if not an acronym\n",
    "        alt_labels = [x.strip() if x.isupper() else x.lower().strip() for x in alternative_labels.split(\", \")]  \n",
    "        \n",
    "        for alt_label in alt_labels:\n",
    "            if not alt_label:\n",
    "                continue\n",
    "            # add the altlabel to the concept node\n",
    "            alt_label_concept_uid = ua.assign_UID(alt_label, CONCEPTS)\n",
    "            irec_graph = add_tuples(irec_graph, skos_node(alt_label_concept_uid, alt_label))\n",
    "            irec_graph = add_tuples(irec_graph, skos_in_scheme(alt_label_concept_uid, 'schemeUID', CONCEPTS, CONCEPTS))\n",
    "            irec_graph = add_tuples(irec_graph, provenance(alt_label_concept_uid, merged_approved_documents_IRI, CONCEPTS))\n",
    "\n",
    "            irec_graph = add_tuples(irec_graph, skos_altLabel(concept_uid, alt_label_concept_uid))\n",
    "            \n",
    "            # also add as a span\n",
    "            alt_label_span_uid = ua.assign_UID(alt_label, SPANS)\n",
    "            irec_graph = add_tuples(irec_graph, irec_span(alt_label_span_uid, alt_label))\n",
    "            irec_graph = add_tuples(irec_graph, skos_in_scheme(alt_label_span_uid, 'schemeUID', SPANS, SPANS))\n",
    "            irec_graph = add_tuples(irec_graph, provenance(alt_label_span_uid, merged_approved_documents_IRI, SPANS))\n",
    "            \n",
    "            # link the altlabel concept and the altlabel span\n",
    "            irec_graph = add_tuples(irec_graph, skos_exact_match(alt_label_concept_uid, alt_label_span_uid, CONCEPTS, SPANS))\n",
    "    \n",
    "    if concept_uid not in concepts_definitions_dict: \n",
    "        concepts_definitions_dict[concept_uid] = [{'prefLabel': term, 'definition': definition, 'note': note}]  \n",
    "    else:\n",
    "        concepts_definitions_dict[concept_uid].append({'prefLabel': term, 'definition': definition, 'note': note})  \n",
    "                                                      \n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86a2f6c",
   "metadata": {},
   "source": [
    "### Add SPANS: glossary/index terms from the Approved Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "92cbdde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_terms = pd.read_excel(graph_data_fp.joinpath(\"Approved Documents and derived terms.xlsx\"), sheet_name=\"Index terms\", keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "42ddb6c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Term</th>\n",
       "      <th>AltLabel(s)</th>\n",
       "      <th>Related terms</th>\n",
       "      <th>Broader term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abbreviated eaves</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>eaves</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Access floors</td>\n",
       "      <td>access floor</td>\n",
       "      <td>Platform floors</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Access for fire service</td>\n",
       "      <td>fire access</td>\n",
       "      <td></td>\n",
       "      <td>Fire service facilities</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Term   AltLabel(s)    Related terms  \\\n",
       "0        abbreviated eaves                                  \n",
       "1            Access floors  access floor  Platform floors   \n",
       "2  Access for fire service   fire access                    \n",
       "\n",
       "              Broader term  \n",
       "0                    eaves  \n",
       "1                           \n",
       "2  Fire service facilities  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_terms[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843da834",
   "metadata": {},
   "source": [
    "* add triples from index terms / glossaries; we will treat these terms as SPANS\n",
    "* some of these terms were added manually on top of the index terms found in the Mergeds Approved documents, so we'll avoid adding the provenance relation to these\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4f1358bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total index terms found in spreadsheet:  2363\n"
     ]
    }
   ],
   "source": [
    "total_num_index_terms = 0\n",
    "for i, row in index_terms.iloc[1:].iterrows():\n",
    "    # Many of these terms start with a capital; lowercase them\n",
    "    term = row['Term'].strip() if row['Term'].isupper() else row['Term'].lower().strip()\n",
    "    alternative_labels = row['AltLabel(s)']\n",
    "    related_terms = row['Related terms']\n",
    "    broader_term = row['Broader term']\n",
    "    \n",
    "    # add the term as a SPAN only\n",
    "    span_uid = ua.assign_UID(term, SPANS)\n",
    "    irec_graph = add_tuples(irec_graph, irec_span(span_uid, term))\n",
    "    irec_graph = add_tuples(irec_graph, skos_in_scheme(span_uid, 'schemeUID', SPANS, SPANS))\n",
    "#     irec_graph = add_tuples(irec_graph, provenance(span_uid, merged_approved_documents_IRI, SPANS))\n",
    "    total_num_index_terms += 1\n",
    "        \n",
    "    if alternative_labels:\n",
    "        # Many of these terms start with a capital; lowercase them\n",
    "        alt_labels = [x.strip() if x.isupper() else x.lower().strip() for x in alternative_labels.split(\", \")]  \n",
    "        \n",
    "        for alt_label in alt_labels:\n",
    "            if not alt_label:\n",
    "                continue\n",
    "            # add alt-label as a span only (as well)\n",
    "            alt_label_uid = ua.assign_UID(alt_label, SPANS)\n",
    "            irec_graph = add_tuples(irec_graph, irec_span(alt_label_uid, alt_label))\n",
    "            irec_graph = add_tuples(irec_graph, skos_in_scheme(alt_label_uid, 'schemeUID', SPANS, SPANS))\n",
    "#             irec_graph = add_tuples(irec_graph, provenance(alt_label_uid, merged_approved_documents_IRI, SPANS))\n",
    "            total_num_index_terms += 1\n",
    "            \n",
    "            if alt_label.isupper():\n",
    "                # there are acronyms among the alternative labels\n",
    "                irec_graph = add_tuples(irec_graph, irec_has_acronym(span_uid, alt_label_uid))\n",
    "            else:\n",
    "                ### Should I use skos altlabels between spans? maybe create IREC alternative label?\n",
    "                ### Should I use skos altlabels between spans? maybe create IREC alternative label?\n",
    "                ### Should I use skos altlabels between spans? maybe create IREC alternative label?\n",
    "                irec_graph = add_tuples(irec_graph, skos_altLabel(span_uid, alt_label_uid))\n",
    "                \n",
    "\n",
    "    if related_terms:\n",
    "        # Many of these terms start with a capital; lowercase them\n",
    "        rel_terms = [x.strip() if x.isupper() else x.lower().strip() for x in related_terms.split(\", \")]\n",
    "        for rel_term in rel_terms:\n",
    "            if not rel_term:\n",
    "                continue\n",
    "            # add related terms as a span (as well)\n",
    "            related_uid = ua.assign_UID(rel_term, SPANS)\n",
    "            irec_graph = add_tuples(irec_graph, irec_span(related_uid, rel_term))\n",
    "            irec_graph = add_tuples(irec_graph, skos_in_scheme(related_uid, 'schemeUID', SPANS, SPANS))\n",
    "#             irec_graph = add_tuples(irec_graph, provenance(related_uid, merged_approved_documents_IRI, SPANS))\n",
    "            total_num_index_terms += 1\n",
    "            \n",
    "            if rel_term.isupper():\n",
    "                # there are acronyms among the related labels as well\n",
    "                irec_graph = add_tuples(irec_graph, irec_has_acronym(span_uid, related_uid))\n",
    "            else:\n",
    "                irec_graph = add_tuples(irec_graph, irec_related(span_uid, related_uid)) \n",
    "    \n",
    "    if broader_term:\n",
    "        # We do not expect that the broader term is necessarily a concept.\n",
    "        # Currently, it is simply a feature for future reference.\n",
    "        # We expect 1 broader term at most, assuming the final conceptualisation would\n",
    "        # be structured like a tree (Directed Acyclic Graph with 1 parent at most).\n",
    "        b_term = broader_term.strip().lower() if not broader_term.isupper() else broader_term.strip()\n",
    "        # also broader term as a span\n",
    "        b_term_uid = ua.assign_UID(b_term, SPANS)\n",
    "        irec_graph = add_tuples(irec_graph, irec_span(b_term_uid, b_term)) \n",
    "        irec_graph = add_tuples(irec_graph, skos_in_scheme(b_term_uid, 'schemeUID', SPANS, SPANS))\n",
    "#         irec_graph = add_tuples(irec_graph, provenance(b_term_uid, merged_approved_documents_IRI, SPANS))\n",
    "        total_num_index_terms += 1\n",
    "        \n",
    "        ### Should I use skos broader between spans? maybe create an IREC broader?\n",
    "        ### Should I use skos broader between spans? maybe create an IREC broader?\n",
    "        irec_graph = add_tuples(irec_graph, skos_broader(span_uid, b_term_uid, SPANS, SPANS)) \n",
    "\n",
    "print(\"Total index terms found in spreadsheet: \", total_num_index_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "40952822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Graph identifier=N6b829faf098f4276bebec242edb83a35 (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irec_graph.serialize(destination=graph_output_fp.joinpath(\"approved_doc_terms_only.ttl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fbf590",
   "metadata": {},
   "source": [
    "### Print some insight in the graph so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "14daa5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes in 'https://spans.irec.org/': 12734\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12734"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ua.count_nodes_in_namespace(SPANS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "204cfd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes in 'https://concepts.irec.org/': 352\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "352"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ua.count_nodes_in_namespace(CONCEPTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bdfeb278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "273 ; type ; Concept\n",
      "273 ; prefLabel ; sanitary accommodation\n",
      "273 ; inScheme ; schemeUID\n",
      "273 ; hadPrimarySource ; https://www.gov.uk/government/collections/approved-documents\n",
      "273 ; exactMatch ; sanitary%20accommodation\n",
      "273 ; definition ; A space containing one or more water closets or urinals, whether or not it also contains other sanitary appliances. Sanitary accommodation containing one or  more cubicles counts as a single space if there is free circulation of air throughout the space.\n"
     ]
    }
   ],
   "source": [
    "ua.print_node_by_id(irec_graph, 273, CONCEPTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e6ad1b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sanitary%20accommodation ; type ; CharacterSpan\n",
      "sanitary%20accommodation ; label ; sanitary accommodation\n",
      "sanitary%20accommodation ; inScheme ; schemeUID\n",
      "sanitary%20accommodation ; hadPrimarySource ; https://www.gov.uk/government/collections/approved-documents\n",
      "sanitary%20accommodation ; wasAttributedTo ; http://dx.doi.org/10.18653/v1/2021.nllp-1.14\n",
      "sanitary%20accommodation ; related ; sanitary%20appliance\n"
     ]
    }
   ],
   "source": [
    "ua.print_node_by_id(irec_graph, urllib.parse.quote('sanitary accommodation'), SPANS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4172c596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wet%20room ; type ; CharacterSpan\n",
      "wet%20room ; label ; wet room\n",
      "wet%20room ; inScheme ; schemeUID\n",
      "wet%20room ; hadPrimarySource ; https://www.gov.uk/government/collections/approved-documents\n",
      "wet%20room ; wasAttributedTo ; http://dx.doi.org/10.18653/v1/2021.nllp-1.14\n"
     ]
    }
   ],
   "source": [
    "ua.print_node_by_id(irec_graph, urllib.parse.quote('wet room'), SPANS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1ddcb895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sanitary%20accommodation ; type ; CharacterSpan\n",
      "sanitary%20accommodation ; label ; sanitary accommodation\n",
      "sanitary%20accommodation ; inScheme ; schemeUID\n",
      "sanitary%20accommodation ; hadPrimarySource ; https://www.gov.uk/government/collections/approved-documents\n",
      "sanitary%20accommodation ; wasAttributedTo ; http://dx.doi.org/10.18653/v1/2021.nllp-1.14\n",
      "sanitary%20accommodation ; related ; sanitary%20appliance\n"
     ]
    }
   ],
   "source": [
    "ua.print_node_by_text(irec_graph, 'sanitary accommodation', SPANS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1989fb60",
   "metadata": {},
   "source": [
    "As you can see in the examples above, the concept `wet room` and the span `sanitary accomdodation` are related:\n",
    "* The concept `wet room` is provided with a note in the merged approved documents.\n",
    "* The text inside this node describes how, for part F of the approved documents, `sanitary accomodation` is regarded as a `wet room`. \n",
    "\n",
    "Based on the above, we'd like to link the span `sanitary accomodation` to the concept `wet room`. While we could parse the note in more detail, and identify that a `skos:altLabel` relation should be added, we'll use a more generic approach:\n",
    "* Any span that is found inside a definition or note of a concept will be linked through `irec:related`\n",
    "* Based on the definitions above, potential spans related to the `wet room` concept then become: `sanitary accomdoation`, `airborn moisture`, `kitchen`, `utility room`, `bathroom`, `WC`, `tanking`, `drainage`, `gulley`, `shower`.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "We believe that the types of relations described above can be valuable and would like to provide more definitions for more terms, to help interrelate more spans and concepts. To this end, we first try to find WikiData definitions for all concepts and spans. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407358e6",
   "metadata": {},
   "source": [
    "### Grab wikipedia definitions for Concept nodes, and store locally for re-use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05db9542",
   "metadata": {},
   "source": [
    "* First, we try to grab all wiki definitions for all spans and concepts that are in the graph (so far)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f2e41964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the SPARQL endpoint for wikidata\n",
    "sparql_wrapper = SPARQLWrapper(\"https://query.wikidata.org/sparql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "12e9e280",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wiki_matches(graph_sparql_endpoint: SPARQLWrapper,\n",
    "                     jargon_term_and_uids: List):\n",
    "\n",
    "    all_wiki_definitions = {}\n",
    "    # we want to grab the term (subject), any definition (subjectDescription) and the class (subjectClass)\n",
    "    sparql_q = \"\"\"\n",
    "               SELECT ?subject ?subjectDescription ?classUID ?className WHERE {\n",
    "                  ?subject rdfs:label \"$QUERY\"@en.\n",
    "                  ?subject wdt:P31|wdt:P279 ?classUID.\n",
    "                  SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\".}\n",
    "                  ?classUID  rdfs:label ?className  FILTER(LANG(?className) = \"en\").\n",
    "                }\n",
    "               \"\"\"\n",
    "    \n",
    "    for term, uid in tqdm(jargon_term_and_uids):\n",
    "        # make the call to \n",
    "        temp_q = sparql_q.replace(\"$QUERY\", term)\n",
    "        graph_sparql_endpoint.setQuery(temp_q)\n",
    "        graph_sparql_endpoint.setReturnFormat(JSON)\n",
    "        try:\n",
    "            json_output = graph_sparql_endpoint.query().convert()\n",
    "        except:\n",
    "            # If no result, wait 2s; One client is allowed 30 error queries per minute\n",
    "            print(f\"Error for query, you should what's wrong with the term: {term}\")\n",
    "            time.sleep(2)\n",
    "            continue\n",
    "            \n",
    "        # sometimes multiple Wiki UIDs for a single term, we grab them all here\n",
    "        bindings = [v for v in json_output['results']['bindings']]\n",
    "            \n",
    "\n",
    "        for v in bindings:\n",
    "            class_uid = v['classUID']['value'] if 'classUID' in v else \"\"\n",
    "            class_label = v['className']['value'] if 'className' in v else \"\"\n",
    "            \n",
    "            if 'subjectDescription' in v:\n",
    "                if uid not in all_wiki_definitions:\n",
    "                    all_wiki_definitions[uid] = [{'prefLabel': term,\n",
    "                                                  'class_uid': class_uid,\n",
    "                                                  'class_label': class_label,\n",
    "                                                  'WikiUID': v['subject']['value'],\n",
    "                                                  'WikiDefinition': v['subjectDescription']['value']}]\n",
    "                else:\n",
    "                    all_wiki_definitions[uid].append({'prefLabel': term,\n",
    "                                                      'class_uid': class_uid,\n",
    "                                                      'class_label': class_label,\n",
    "                                                      'WikiUID': v['subject']['value'],\n",
    "                                                      'WikiDefinition': v['subjectDescription']['value']})\n",
    "            elif 'subject' in v:\n",
    "                # no description found, simply adding wiki UID if that exists\n",
    "                if uid not in all_wiki_definitions:\n",
    "                    all_wiki_definitions[uid] = [{'prefLabel': term,\n",
    "                                                  'class_uid': class_uid,\n",
    "                                                  'class_label': class_label,\n",
    "                                                  'WikiUID': v['subject']['value']}]\n",
    "                else:\n",
    "                    all_wiki_definitions[uid].append({'prefLabel': term,\n",
    "                                                      'class_uid': class_uid,\n",
    "                                                      'class_label': class_label,\n",
    "                                                      'WikiUID': v['subject']['value']})\n",
    "    return all_wiki_definitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6b57bf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "concepts_and_uids = [(k, v) for k, v in ua.UIDs[CONCEPTS.placeholder.defrag().__reduce__()[1][0]].items()]\n",
    "spans_and_uids = [(k, v) for k, v in ua.UIDs[SPANS.placeholder.defrag().__reduce__()[1][0]].items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3f9b3c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First run for the Concepts\n",
    "concept_wiki_dict_fp = graph_output_fp.joinpath(\"concept_wiki_dict.json\")\n",
    "if not concept_wiki_dict_fp.exists():\n",
    "    concept_wiki_dict = get_wiki_matches(sparql_wrapper, concepts_and_uids)#{'test': 1, 'conductor':2})\n",
    "    with open(concept_wiki_dict_fp, 'w') as f:\n",
    "        json.dump(concept_wiki_dict, f, indent=2)\n",
    "else:\n",
    "    with open(concept_wiki_dict_fp, 'r') as f:\n",
    "        concept_wiki_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1b368865",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now run for the spans\n",
    "span_wiki_dict_fp = graph_output_fp.joinpath(\"span_wiki_dict.json\")\n",
    "if not span_wiki_dict_fp.exists():\n",
    "    span_wiki_dict = get_wiki_matches(sparql_wrapper, spans_and_uids)#{'test': 1, 'conductor':2})\n",
    "    with open(span_wiki_dict_fp, 'w') as f:\n",
    "        json.dump(span_wiki_dict, f, indent=2)\n",
    "else:\n",
    "    with open(span_wiki_dict_fp, 'r') as f:\n",
    "        span_wiki_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "10f67b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes in 'https://concepts.irec.org/': 352\n",
      "Number of concepts with WikiData definitions: 244 (69.32%)\n",
      "Number of nodes in 'https://spans.irec.org/': 12734\n",
      "Number of spans with WikiData definitions: 655 (5.14%)\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of concepts with WikiData definitions: {} ({:.2f}%)\".format(len(concept_wiki_dict), len(concept_wiki_dict)/ua.count_nodes_in_namespace(CONCEPTS)*100))\n",
    "print(\"Number of spans with WikiData definitions: {} ({:.2f}%)\".format(len(span_wiki_dict), len(span_wiki_dict)/ua.count_nodes_in_namespace(SPANS)*100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4dec3a",
   "metadata": {},
   "source": [
    "* Some examples of/insight in definitions from different sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0dfa4268",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'prefLabel': 'absorption coefficient',\n",
       "  'definition': 'A quantity characterising the effectiveness of a sound absorbing surface. The proportion of sound energy absorbed is given as a number between zero (for a fully reflective surface) and one (for a fully absorptive surface). Note that sound absorption coefficients determined from laboratory measurements may have values slightly larger than one.',\n",
       "  'note': 'See BS EN 20354:1993.'}]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of definitions from approved documents\n",
    "concepts_definitions_dict['1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e746463c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'prefLabel': 'absorption coefficient',\n",
       "  'class_uid': 'http://www.wikidata.org/entity/Q107715',\n",
       "  'class_label': 'physical quantity',\n",
       "  'WikiUID': 'http://www.wikidata.org/entity/Q97368968',\n",
       "  'WikiDefinition': 'measure for the exponential reduction of a quantity along a path due to absorption',\n",
       "  'Spans in definitions and notes': ['a path due',\n",
       "   'a quantity',\n",
       "   'the exponential reduction']}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of definitions for the same concept, from WikiData\n",
    "concept_wiki_dict['1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2971f4f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['absorbent']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'prefLabel': 'absorbent',\n",
       "  'class_uid': 'http://www.wikidata.org/entity/Q3505845',\n",
       "  'class_label': 'state',\n",
       "  'WikiUID': 'http://www.wikidata.org/entity/Q110147344',\n",
       "  'WikiDefinition': 'having the ability or tendency to absorb; able to soak up liquid easily; absorptive.',\n",
       "  'Spans in definitions and notes': ['liquid easily', 'the ability tendency']}]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of definitions for a related span, from WikiData\n",
    "print([k for k in span_wiki_dict.keys() if 'absor' in k])\n",
    "span_wiki_dict['absorbent']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699c6acb",
   "metadata": {},
   "source": [
    "### Only keep WikiData definitions that belong to classes that we've annotated\n",
    "* We have previously annotated the relevance of all WikiData classes returned for the defined terms and index terms in the Approved Documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c5aa0969",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_wikidata_classes_df = pd.read_csv(graph_data_fp.joinpath(\"wiki_classes_annotated.csv\"), index_col=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c66f2893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WikiData class</th>\n",
       "      <th>Annotation</th>\n",
       "      <th>Example spans</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WikiData UIDs</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>['Q107715']</th>\n",
       "      <td>physical quantity</td>\n",
       "      <td>y</td>\n",
       "      <td>['sound pressure level', 'density', 'area', 's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>['Q82799']</th>\n",
       "      <td>name</td>\n",
       "      <td>n</td>\n",
       "      <td>['access point']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>['Q180160']</th>\n",
       "      <td>metadata</td>\n",
       "      <td>n</td>\n",
       "      <td>['access point']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  WikiData class Annotation  \\\n",
       "WikiData UIDs                                 \n",
       "['Q107715']    physical quantity          y   \n",
       "['Q82799']                  name          n   \n",
       "['Q180160']             metadata          n   \n",
       "\n",
       "                                                   Example spans  \n",
       "WikiData UIDs                                                     \n",
       "['Q107715']    ['sound pressure level', 'density', 'area', 's...  \n",
       "['Q82799']                                      ['access point']  \n",
       "['Q180160']                                     ['access point']  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotated_wikidata_classes_df[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1f587141",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikiclass_dict = {}\n",
    "for row in annotated_wikidata_classes_df.iterrows():\n",
    "    uid_list_string, class_annotations_examples = row\n",
    "    uid_list = uid_list_string[2:-2].split(',')\n",
    "    for uid in uid_list:\n",
    "        wikiclass_dict[uid] = {\n",
    "            'Class': class_annotations_examples['WikiData class'],\n",
    "            'Annotation': class_annotations_examples['Annotation'],\n",
    "            'Example spans': class_annotations_examples['Example spans']\n",
    "        }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dde9e8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_wikidata_classes(wiki_class_dict, term_dict):\n",
    "    new_term_dict = {}\n",
    "    removed_definitions = []\n",
    "    for uid, definition_dict_list in term_dict.items():\n",
    "        for definition_dict in definition_dict_list:\n",
    "            class_uid = definition_dict['class_uid'].rsplit(\"/\", 1)[1]\n",
    "            if class_uid in wikiclass_dict:\n",
    "                class_name = wikiclass_dict[class_uid][\"Class\"]\n",
    "                if wikiclass_dict[class_uid][\"Annotation\"] == 'y':\n",
    "                    if uid not in new_term_dict:\n",
    "                        new_term_dict[uid] = [definition_dict]\n",
    "                    else:\n",
    "                        new_term_dict[uid].append(definition_dict)\n",
    "                else:\n",
    "                    removed_definitions.append(definition_dict)\n",
    "                        \n",
    "    return new_term_dict, removed_definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e49d20d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_wiki_dict, removed_definitions = filter_wikidata_classes(wikiclass_dict, concept_wiki_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "78aefda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "span_wiki_dict, removed_definitions = filter_wikidata_classes(wikiclass_dict, span_wiki_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fae49f",
   "metadata": {},
   "source": [
    "### Parse all definitions (including WikiData) to identify additional spans\n",
    "* Add the spar objects found in definitions to respective dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bfdd2f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_spar_labels(input_dict: Dict[str, str], term_extractor: TermExtractor):\n",
    "    number_of_definitions = 0\n",
    "    for uid, definition_dict_list in tqdm(input_dict.items()):\n",
    "        for idx, definition_dict in enumerate(definition_dict_list):\n",
    "            if 'Spans in definitions and notes' in definition_dict:\n",
    "                # spans already computed for this definition_dict, continuing to check next\n",
    "                continue\n",
    "            \n",
    "            spartxt_objects = []\n",
    "            for k in definition_dict.keys():\n",
    "                if k in ['WikiDefinition', 'definition', 'note']:\n",
    "                    to_be_parsed = definition_dict[k]\n",
    "                    number_of_definitions += 1\n",
    "                    sentences = term_extractor.split_into_sentences(to_be_parsed)\n",
    "                    # cleaning spans as well;\n",
    "                    sentences = [remove_unicode_chars(s).encode(\"ascii\", \"ignore\").decode() for s in sentences]\n",
    "                    spartxt_objects += custom_cleaning_rules(term_extractor.process_sentences(sentences))\n",
    "                    \n",
    "            input_dict[uid][idx]['Spans in definitions and notes'] = spartxt_objects\n",
    "    print(f\"Processed {number_of_definitions} definitions\")\n",
    "    return input_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "46bf46cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading previously computed concepts_definitions_dict with SPaR.txt objects from file\n"
     ]
    }
   ],
   "source": [
    "concept_definitions_dict_fp = graph_output_fp.joinpath(\"concepts_definitions_dict.json\")\n",
    "te = None\n",
    "if not concept_definitions_dict_fp.exists():\n",
    "    if not te:\n",
    "        # instantiate a TermExtractor obj, with max_num_cpu_threads instances of SPaR.txt predictors \n",
    "        te = TermExtractor(max_num_cpu_threads=4)\n",
    "    \n",
    "    print(\"Computing SPaR.txt objects for concepts_definitions_dict\")\n",
    "    concepts_definitions_dict = add_spar_labels(concepts_definitions_dict, te)\n",
    "    with open(concept_definitions_dict_fp, 'w') as f:\n",
    "        json.dump(concepts_definitions_dict, f, indent=2)\n",
    "else:\n",
    "    print(\"Loading previously computed concepts_definitions_dict with SPaR.txt objects from file\")\n",
    "    with open(concept_definitions_dict_fp, 'r') as f:\n",
    "        concepts_definitions_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6746b48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not concept_wiki_dict_fp.exists():\n",
    "    if not te:\n",
    "        # instantiate a TermExtractor obj, with max_num_cpu_threads instances of SPaR.txt predictors \n",
    "        te = TermExtractor(max_num_cpu_threads=4)\n",
    "    concept_wiki_dict = add_spar_labels(concept_wiki_dict, te)\n",
    "    # Save the updated concept_wiki_dict, will be loaded in previous cells anyway\n",
    "    with open(concept_wiki_dict_fp, 'w') as f:\n",
    "        json.dump(concept_wiki_dict, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ae7de0d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not span_wiki_dict_fp.exists():\n",
    "    if not te:\n",
    "        # instantiate a TermExtractor obj, with max_num_cpu_threads instances of SPaR.txt predictors \n",
    "        te = TermExtractor(max_num_cpu_threads=4)\n",
    "    span_wiki_dict = add_spar_labels(span_wiki_dict, te)\n",
    "    # Save the updated span_wiki_dict, which will be loaded in previous cells anyway\n",
    "    with open(span_wiki_dict_fp, 'w') as f:\n",
    "        json.dump(span_wiki_dict, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d77b070",
   "metadata": {},
   "source": [
    "### Add any new spans to the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb8c45a",
   "metadata": {},
   "source": [
    "* First, count the number of defined terms, separate definitions (a term may have multiple definitions), and the number of spans found in these definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ee7f6d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_spans(some_dict: Dict[str, str], primary_source: URIRef, per_term_definition_counts: Counter):\n",
    "    primary_source_str = primary_source.__reduce__()[1][0]\n",
    "    spartxt_objects_in_dict = {primary_source_str: {}}\n",
    "    total_num_definitions = 0\n",
    "    for definition_dict_list in some_dict.values():\n",
    "        for definition_dict in definition_dict_list:\n",
    "            per_term_definition_counts[definition_dict['prefLabel']] = len([d for d in definition_dict_list if any(d for d in d.keys() if d in ['WikiDefinition', 'definition', 'note'])])\n",
    "        \n",
    "            for k in definition_dict.keys():\n",
    "                if k in ['WikiDefinition', 'definition', 'note']:\n",
    "                    total_num_definitions += 1\n",
    "            \n",
    "            defined_term = definition_dict['prefLabel']\n",
    "            spar_objects_in_dict = custom_cleaning_rules(definition_dict['Spans in definitions and notes'])\n",
    "            \n",
    "            if defined_term not in spartxt_objects_in_dict[primary_source_str]:\n",
    "                spartxt_objects_in_dict[primary_source_str][defined_term] = spar_objects_in_dict\n",
    "            else:\n",
    "                spartxt_objects_in_dict[primary_source_str][defined_term] += spar_objects_in_dict\n",
    "                \n",
    "    return spartxt_objects_in_dict, per_term_definition_counts, total_num_definitions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0ff7e865",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_spartxt_objects = {}\n",
    "total_num_definitions = 0\n",
    "per_term_definition_counts = Counter()\n",
    "for some_dict, source in zip([concepts_definitions_dict, concept_wiki_dict, span_wiki_dict],\n",
    "                             [merged_approved_documents_IRI, wikidata_IRI, wikidata_IRI]):\n",
    "    spar_txt_objects_in_dict, term_def_counts, num_def = count_spans(some_dict, source, per_term_definition_counts)\n",
    "    all_spartxt_objects.update(spar_txt_objects_in_dict)\n",
    "    per_term_definition_counts.update(term_def_counts)\n",
    "    total_num_definitions += num_def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5a462180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of defined terms:  951\n",
      "Number of definitions/notes found: 2472\n",
      "Top 10 defined terms with most definitions:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('house', 192),\n",
       " ('frequency', 32),\n",
       " ('lead', 32),\n",
       " ('Hotel', 28),\n",
       " ('density', 24),\n",
       " ('pier', 24),\n",
       " ('accessibility', 24),\n",
       " ('volume', 24),\n",
       " ('risk assessment', 20),\n",
       " ('aluminium', 20)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "defined_terms = []\n",
    "spans_from_defined_terms = []\n",
    "for source, defined_term_dict in all_spartxt_objects.items():\n",
    "    defined_terms += [k for k in defined_term_dict.keys()]\n",
    "    for spans in defined_term_dict.values():\n",
    "        spans_from_defined_terms += spans\n",
    "        \n",
    "unique_new_spans = [x for x in custom_cleaning_rules([span for span in set(spans_from_defined_terms)]) if x not in domain_terms]\n",
    "print(\"Number of defined terms: \", len(defined_terms))\n",
    "print(\"Number of definitions/notes found:\", total_num_definitions)\n",
    "print(\"Top 10 defined terms with most definitions:\")\n",
    "per_term_definition_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d97d0915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total count of terms found in definitions/notes: 6167\n",
      "Number of new spans (unique):  2706\n",
      "Random sample of unseen spans, found in the definitions:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['wooden',\n",
       " 'an end date',\n",
       " 'conductors',\n",
       " 'seconds',\n",
       " 'composting toilet',\n",
       " 'part of',\n",
       " 'strands',\n",
       " 'firm',\n",
       " 'noise pollution',\n",
       " 'Waverley']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Total count of terms found in definitions/notes:\", len(spans_from_defined_terms))\n",
    "print(\"Number of new spans (unique): \", len(unique_new_spans))\n",
    "print(\"Random sample of unseen spans, found in the definitions:\")\n",
    "random.sample(list(set(unique_new_spans)), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53825d2d",
   "metadata": {},
   "source": [
    "* Add any new spans to the graph, with prov:hasPrimarySource *WikiData* and prov:wasAttributedTo *SPaR.txt*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "558b5f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a related label between the defined concept/span, and the span found in a definition\n",
    "non_missing_terms = []\n",
    "for i, (source, term_and_spans_dict) in enumerate(all_spartxt_objects.items()):\n",
    "    for term, related_spans in term_and_spans_dict.items():\n",
    "\n",
    "        # add the term (defined concept) as a span (if it didn't exist yet)\n",
    "        term_uid = ua.assign_UID(term, SPANS)\n",
    "        irec_graph = add_tuples(irec_graph, irec_span(term_uid, term))\n",
    "        irec_graph = add_tuples(irec_graph, skos_in_scheme(term_uid, 'schemeUID', SPANS, SPANS))\n",
    "        irec_graph = add_tuples(irec_graph, provenance(term_uid, URIRef(source), SPANS))\n",
    "\n",
    "        # Check if the term is actually already identified as a concept\n",
    "        concept_uid = None\n",
    "        if term in ua.UIDs[CONCEPTS._.defrag().__reduce__()[1][0]]:\n",
    "            concept_uid = ua.UIDs[CONCEPTS._.defrag().__reduce__()[1][0]][term]\n",
    "\n",
    "        rel_spans = custom_cleaning_rules(related_spans)\n",
    "        for rel_term in rel_spans :\n",
    "            non_missing_terms.append(rel_term)\n",
    "            # Add the spans that were extracted from the definitions, assign a new UID if needed\n",
    "            related_uid = ua.assign_UID(rel_term, SPANS)\n",
    "            irec_graph = add_tuples(irec_graph, irec_span(related_uid, rel_term))\n",
    "            irec_graph = add_tuples(irec_graph, skos_in_scheme(related_uid, 'schemeUID', SPANS, SPANS))\n",
    "            irec_graph = add_tuples(irec_graph, provenance(related_uid, URIRef(source), SPANS))\n",
    "            \n",
    "            # add agent for spans generated by SPaR.txt\n",
    "            irec_graph = add_tuples(irec_graph, prov_agent(related_uid, spart_txt_IRI, SPANS))\n",
    "\n",
    "            # Add relation between spans\n",
    "            irec_graph = add_tuples(irec_graph, irec_related(term_uid, related_uid)) \n",
    "\n",
    "            # Add relation between concept and span\n",
    "            if concept_uid:\n",
    "                irec_graph = add_tuples(irec_graph, irec_related(concept_uid, related_uid, CONCEPTS, SPANS)) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec817464",
   "metadata": {},
   "source": [
    "### Add WikiData definitions to graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c270684a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_wiki_definitions(irec_graph: Graph, wiki_dict: Dict[str, str], dict_namespace: Namespace):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    for i, (_, definition_dict_list) in enumerate(wiki_dict.items()):\n",
    "        for definition_dict in definition_dict_list:\n",
    "            wiki_term = definition_dict['prefLabel']\n",
    "            wiki_class_label = definition_dict['class_label'] \n",
    "            wiki_class_uid = definition_dict['class_uid'] \n",
    "            wiki_uid = definition_dict['WikiUID'].rsplit('/', 1)[1]\n",
    "\n",
    "            # keep track of uid in the Unique ID assigner obj as well\n",
    "            _ = ua.keep_track_of_existing_UID(wiki_term, wiki_uid, WIKI)\n",
    "            \n",
    "            # add the WikiData concept to the graph, in WIKI namespace\n",
    "            irec_graph = add_tuples(irec_graph, skos_node(wiki_uid, wiki_term, WIKI))\n",
    "            # irec_graph = add_tuples(irec_graph, skos_in_scheme(wiki_uid, 'schemeUID', WIKI, WIKI))\n",
    "            irec_graph = add_tuples(irec_graph, provenance(wiki_uid, wikidata_IRI, WIKI))\n",
    "            \n",
    "            # Add an exact match between the wiki node and our concept from the Merged Approved Documents\n",
    "            term_uid = ua.retrieve_uid_by_text(wiki_term, CONCEPTS)\n",
    "            if term_uid:\n",
    "                irec_graph = add_tuples(irec_graph, skos_exact_match(term_uid, wiki_uid, CONCEPTS, WIKI))\n",
    "            \n",
    "            #### We will link the WikiData concept to a span, rather than a concept, as well as its definitions\n",
    "            # and class labels\n",
    "            # 1) Add a span and a link to the wiki concept\n",
    "            span_uid = ua.retrieve_uid_by_text(wiki_term, SPANS)\n",
    "            if not span_uid:\n",
    "                raise Exception(f\"Cannot find the span: {wiki_term}!\")\n",
    "            \n",
    "            # 2) add wiki class label, as well as a str version of the UID for reference, in SPANS namespace\n",
    "            wiki_class_label_and_uid = f\"{wiki_class_label} [{wiki_class_uid}]\"\n",
    "            irec_graph = add_tuples(irec_graph, irec_wikidef(span_uid,  wiki_class_label_and_uid, SPANS))\n",
    "\n",
    "            # 3)  Add the WIKI definition to the node if it exists, in SPANS namespace\n",
    "            if 'WikiDefinition' in definition_dict:            \n",
    "                definition = definition_dict['WikiDefinition']\n",
    "                irec_graph = add_tuples(irec_graph, irec_wikidef(span_uid, definition, SPANS))\n",
    "                \n",
    "            # 4) Add an exact match between the span and wikidata concept as well\n",
    "            irec_graph = add_tuples(irec_graph, skos_exact_match(span_uid, wiki_uid, SPANS, WIKI))\n",
    "\n",
    "    return irec_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "43cf5dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "irec_graph = add_wiki_definitions(irec_graph, concept_wiki_dict, CONCEPTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "25128df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "irec_graph = add_wiki_definitions(irec_graph, span_wiki_dict, SPANS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f99770a",
   "metadata": {},
   "source": [
    "### We will also add the Uniclass terms that we found in the text to the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "82fe7391",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(graph_data_fp.joinpath(\"uniclass_terms_in_text.pkl\"), 'rb') as f:\n",
    "    uniclass_terms_in_text = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1076ed68",
   "metadata": {},
   "outputs": [],
   "source": [
    "for uniclass_uid, definition_dict in uniclass_terms_in_text.items():\n",
    "    # Add the Uniclass node to our graph\n",
    "    uniclass_term = definition_dict['pref_label']\n",
    "    # keep track of uid that is added to the graph\n",
    "    _ = ua.keep_track_of_existing_UID(uniclass_term, uniclass_uid, UNICLASS)\n",
    "    \n",
    "    # add the concept to the graph, in UNICLASS namespace\n",
    "    irec_graph = add_tuples(irec_graph, skos_node(uniclass_uid, uniclass_term, UNICLASS))\n",
    "    # irec_graph = add_tuples(irec_graph, skos_in_scheme(uniclass_uid, 'schemeUID', UNICLASS, UNICLASS))\n",
    "    irec_graph = add_tuples(irec_graph, provenance(uniclass_uid, uniclass_IRI, UNICLASS))\n",
    "    \n",
    "    # Determine or create the corresponding term_uid in SPANS and add a # skos:exactMatch?\n",
    "    if ua.retrieve_uid_by_text(uniclass_term): # First as is (no lowercasing, despite Uniclass casing)\n",
    "        # Add an exact match between the Uniclass node and the corresponding span\n",
    "        span_uid = ua.retrieve_uid_by_text(uniclass_term)\n",
    "        irec_graph = add_tuples(irec_graph, skos_exact_match(span_uid, uniclass_uid, SPANS, UNICLASS))\n",
    "    elif ua.retrieve_uid_by_text(uniclass_term.lower()):\n",
    "        # Add an exact match between the wiki node and the corresponding lowercased version in SPANS\n",
    "        span_uid = ua.retrieve_uid_by_text(uniclass_term.lower())\n",
    "        irec_graph = add_tuples(irec_graph, skos_exact_match(span_uid, uniclass_uid, SPANS, UNICLASS))\n",
    "    else:\n",
    "        # Although the uniclass term was found in the text, no exact matching span was extracted by SPaR.txt\n",
    "        # add the term (defined concept) as a span \n",
    "        term_uid = ua.assign_UID(uniclass_term, SPANS)\n",
    "        irec_graph = add_tuples(irec_graph, irec_span(term_uid, uniclass_term))\n",
    "        irec_graph = add_tuples(irec_graph, skos_in_scheme(term_uid, 'schemeUID', SPANS, SPANS))\n",
    "        irec_graph = add_tuples(irec_graph, provenance(term_uid, uniclass_IRI, SPANS))\n",
    "\n",
    "        irec_graph = add_tuples(irec_graph, skos_exact_match(term_uid, uniclass_uid, SPANS, UNICLASS))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ae56c35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "irec_graph.serialize(destination=graph_output_fp.joinpath(\"unfeatured_graph.ttl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e978d7",
   "metadata": {},
   "source": [
    "### Compute properties between spans [work in progress, 15K spans is 225 Million combinations... ]\n",
    "* link definitions to spans (if span occurs verbatim; linkwith irec:related)\n",
    "* link spans to spans (if a concept exists, they have the an equivalent span anyway)\n",
    "  * semantic similarity, x and y might be alternative labels or have the same superclass -> based on kNN\n",
    "  * constitutes; x occurs in y, thus y might be an extended phrase for x and perhaps a subclass, or x may be a material property, and so on\n",
    "  * morphological similarity, x may be an inflection of y or somehow related"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8aed2a",
   "metadata": {},
   "source": [
    "**Semantic similarity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e46c83a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_model_name = \"bert-base-cased\"\n",
    "embedding_output_fp = Path.cwd().joinpath(\"data\", \"term_embedding\")\n",
    "IDF_path = embedding_output_fp.joinpath(\"IDF_weights.json\")\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "bert_model = BertModel.from_pretrained(bert_model_name, output_hidden_states=True)\n",
    "embedder = Embedder(tokenizer, bert_model, \n",
    "                      IDF_dict=json.load(open(IDF_path)), \n",
    "                      embedding_fp=embedding_output_fp,\n",
    "                      layers_to_use = [12],         # we'll use the output of the last layer\n",
    "                      layer_combination = \"avg\",    # how to combine layers if multiple are used\n",
    "                      idf_threshold = 1.5,          # minimum IDF value for a token to contribute\n",
    "                      idf_weight_factor = 1.0,      # modify how strong the influence of IDF weighting is\n",
    "                      not_found_idf_value = 0.5)    # IDF value for tokens that weren't seen during IDF computation (doesn't apply here)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4742e3c9",
   "metadata": {},
   "source": [
    "* First we'll need to compute the embeddings for the unique new spans\n",
    "  * Same process as before, EXCEPT that we now normalise the spans directly as well.\n",
    "  * This seems to break sometimes when I run it, and I don't know why yet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4f06a659",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### TODO: move this into a utility function\n",
    "# Compute the embeddings, this is split into subsets so we don't overload your memory (adjust these values if needed)\n",
    "max_num_cpu_threads = 4\n",
    "subset_size = 1000\n",
    "\n",
    "# Checks which of the embeddings for the clustering cluster_data already exist, so they can be re-used\n",
    "term_subsets = split_list(unique_new_spans, subset_size)\n",
    "embedding_files = [f for f in embedder.embedding_fp.glob('def_term_standardised_embeddings*.pkl')]\n",
    "span_and_embedding_pairs = []\n",
    "if len(embedding_files) == len(term_subsets):\n",
    "    for e in embedding_files:\n",
    "        span_and_embedding_pairs += pickle.load(open(e, 'rb'))\n",
    "else:\n",
    "    print(f\"Preparing embeddings for {len(unique_new_spans)} spans, in groups of: {subset_size}\")\n",
    "    subset_idx = 0            # iterator index outside of tqdm \n",
    "    for subset in tqdm(term_subsets):\n",
    "        subset_embeddings = []\n",
    "        subset_file_name = embedder.embedding_fp.joinpath(\"def_term_standardised_embeddings_part_{}.pkl\".format(subset_idx))\n",
    "        subset_idx += 1\n",
    "        if subset_file_name.exists():\n",
    "            # already computed previously\n",
    "            continue\n",
    "        \n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=max_num_cpu_threads) as executor:\n",
    "            futures = [executor.submit(embedder.embed_and_normalise_span, subset[idx]) for idx in range(len(subset))]\n",
    "\n",
    "        subset_embeddings += [f.result() for f in futures if f.result()]\n",
    "\n",
    "        with open(subset_file_name, 'wb') as f:\n",
    "            pickle.dump(subset_embeddings, f)\n",
    "\n",
    "    # Once all embeddings are created; combine them in span_and_embedding_pairs\n",
    "    embedding_files = [f for f in embedder.embedding_fp.glob('def_term_standardised_embeddings*.pkl')]\n",
    "    for e in embedding_files:\n",
    "        span_and_embedding_pairs += pickle.load(open(e, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2c64704c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the old span_and_embedding_pairs as well\n",
    "old_embedding_files = [f for f in embedder.embedding_fp.glob('embeddings*.pkl')]\n",
    "old_span_and_embedding_pairs = []\n",
    "for e in old_embedding_files:\n",
    "    old_span_and_embedding_pairs += pickle.load(open(e, 'rb'))\n",
    "\n",
    "domain_span_and_embedding_pairs = [(s, e) for (s, e) in old_span_and_embedding_pairs if s in domain_terms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "87890294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the old and new span and embeding pairs\n",
    "unique_spans = [s for (s, e) in (domain_span_and_embedding_pairs + span_and_embedding_pairs)]\n",
    "standardised_clustering_data = np.stack([np.mean(e, axis=0) if len(e.shape) > 1 else e for (s, e) in (domain_span_and_embedding_pairs + span_and_embedding_pairs)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "809890b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the kNN graph for ALL spans now (old + new)\n",
    "n_neighbors = 5 # the number of neighbours we compute for each term\n",
    "knn_graph = kneighbors_graph(standardised_clustering_data, \n",
    "                             n_neighbors,    \n",
    "                             metric=\"cosine\", # <- note we're using cosine sim\n",
    "                             n_jobs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "19302366",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_sim_dict = {}\n",
    "for span_idx, span in enumerate(unique_spans):\n",
    "    knn_sim_dict[span] = [unique_spans[neighbour_idx] for neighbour_idx in knn_graph[span_idx].indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8b078319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add similarity to the irec_graph\n",
    "tuples_to_add = []\n",
    "missing_spans = []\n",
    "for span_one, neighbour_spans in knn_sim_dict.items():\n",
    "    try:\n",
    "        span_one_uid = ua.UIDs[SPANS.placeholder.defrag().__reduce__()[1][0]][span_one]\n",
    "    except:\n",
    "        missing_spans.append(span_one)\n",
    "        continue    \n",
    "    for span_two in neighbour_spans:\n",
    "        try:\n",
    "            span_two_uid = ua.UIDs[SPANS.placeholder.defrag().__reduce__()[1][0]][span_two]\n",
    "            tuples_to_add += irec_sem_sim(span_one_uid, span_two_uid)\n",
    "        except:\n",
    "            missing_spans.append(span_two)\n",
    "            \n",
    "irec_graph = add_tuples(irec_graph, tuples_to_add)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4276a04a",
   "metadata": {},
   "source": [
    "**Constitutes & morphological similarity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b17cc54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterSpan:\n",
    "    def __init__(self, span:str, span_uid: str):\n",
    "        \n",
    "        if not span:\n",
    "            raise Exception(\"Input is an empty string!\")\n",
    "        \n",
    "        self.text = span\n",
    "        self.uid = span_uid\n",
    "        self.blob = TextBlob(span)\n",
    "        self.words = [w for w in self.blob.words]\n",
    "        self.stems = [w.stem() for w in self.words]\n",
    "        \n",
    "        self.morphologically_similar_uids = {}\n",
    "        self.semantically_similar_uids = {}\n",
    "        self.constitutes_uids = {}\n",
    "        self.contains_antonym_uids = {}       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e4b50ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def span_constitutes_span(span_one: CharacterSpan, span_two: CharacterSpan):\n",
    "    \"\"\" True if in a span, you can find all of the words comprising span_two (order doesn't matter) \"\"\"\n",
    "    word_overlap = list(set(span_one.words) & set(span_two.words))\n",
    "    if len(word_overlap) in [len(span_one.words), len(span_two.words)]:\n",
    "#             span_one.constitutes_uids.add(span_two.uid)\n",
    "        return True\n",
    "    return False\n",
    "    \n",
    "def morphologically_similar(span_one: CharacterSpan, span_two: CharacterSpan):\n",
    "    \"\"\" True if this span has either a small Levenshtein distance, or many overlapping words/stems with another span \"\"\"\n",
    "    if levenshtein(span_one.text, span_two.text):\n",
    "#         self.morphologically_similar_uids.add(span_two.uid)\n",
    "        return True\n",
    "    elif len(span_one.words) > 1 and len(span_two.words) > 1: # maybe both?\n",
    "        word_overlap = list(set(span_one.words) & set(span_two.words))\n",
    "        stem_overlap = list(set(span_one.stems) & set(span_two.stems))\n",
    "\n",
    "        if len(word_overlap) >= ((len(span_one.words) + len(span_two.words) - 2) // 2) or \\\n",
    "            len(stem_overlap) >= ((len(span_one.stems) + len(span_two.stems) - 2) // 2):\n",
    "#             span_one.morphologically_similar_uids.add(span_two.uid)\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def span_with_antonym(span_one: CharacterSpan, span_two: CharacterSpan, wordnet_antonyms: Dict[str, str]=wordnet_antonyms):\n",
    "    antonyms_one = [w for w in span_one.words if w in wordnet_antonyms.keys()]\n",
    "    for a in antonyms_one:\n",
    "        antonyms_to_find = wordnet_antonyms[a]\n",
    "        if any([x for x in antonyms_to_find in span_two.words]):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5a61f002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "span_1 constitutes span_2:  False\n",
      "morphologically similar:  False\n",
      "antonym present:  False\n"
     ]
    }
   ],
   "source": [
    "test_1 = \"acoustic\"\n",
    "test_2 = \"thermal\"\n",
    "cs1 = CharacterSpan(test_1, '1')\n",
    "cs2= CharacterSpan(test_2, '2')\n",
    "print(\"span_1 constitutes span_2: \", span_constitutes_span(cs1, cs2))\n",
    "print(\"morphologically similar: \", morphologically_similar(cs1, cs2))\n",
    "print(\"antonym present: \", span_with_antonym(cs1, cs2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c5cdcc07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "span_1 constitutes span_2:  False\n",
      "morphologically similar:  False\n",
      "antonym present:  False\n"
     ]
    }
   ],
   "source": [
    "s1 = \"photo-voltaic cell\"\n",
    "s2 = \"photo-voltaic system\"\n",
    "cs1 = CharacterSpan(test_1, '1')\n",
    "cs2= CharacterSpan(test_2, '2')\n",
    "print(\"span_1 constitutes span_2: \", span_constitutes_span(cs1, cs2))\n",
    "print(\"morphologically similar: \", morphologically_similar(cs1, cs2))\n",
    "print(\"antonym present: \", span_with_antonym(cs1, cs2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e207d87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "span_1 constitutes span_2:  False\n",
      "morphologically similar:  True\n",
      "antonym present:  False\n"
     ]
    }
   ],
   "source": [
    "test_1 = \"damp proof course\"\n",
    "test_2 = \"damp proof membrane\"\n",
    "cs1 = CharacterSpan(test_1, '1')\n",
    "cs2= CharacterSpan(test_2, '2')\n",
    "print(\"span_1 constitutes span_2: \", span_constitutes_span(cs1, cs2))\n",
    "print(\"morphologically similar: \", morphologically_similar(cs1, cs2))\n",
    "print(\"antonym present: \", span_with_antonym(cs1, cs2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2ac1d190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "span_1 constitutes span_2:  False\n",
      "morphologically similar:  False\n",
      "antonym present:  True\n"
     ]
    }
   ],
   "source": [
    "test_1 = \"hot water storage\"\n",
    "test_2 = \"cold water system\"\n",
    "cs1 = CharacterSpan(test_1, '1')\n",
    "cs2= CharacterSpan(test_2, '2')\n",
    "print(\"span_1 constitutes span_2: \", span_constitutes_span(cs1, cs2))\n",
    "print(\"morphologically similar: \", morphologically_similar(cs1, cs2))\n",
    "print(\"antonym present: \", span_with_antonym(cs1, cs2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "95204ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "span_1 constitutes span_2:  True\n",
      "morphologically similar:  True\n",
      "antonym present:  False\n"
     ]
    }
   ],
   "source": [
    "test_1 = \"hot water storage system\"\n",
    "test_2 = \"hot water storage\"\n",
    "cs1 = CharacterSpan(test_1, '1')\n",
    "cs2= CharacterSpan(test_2, '2')\n",
    "print(\"span_1 constitutes span_2: \", span_constitutes_span(cs1, cs2))\n",
    "print(\"morphologically similar: \", morphologically_similar(cs1, cs2))\n",
    "print(\"antonym present: \", span_with_antonym(cs1, cs2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5b875cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features(argument_list: List[CharacterSpan]):\n",
    "    span_one, span_two = argument_list\n",
    "    feature_tuples = []\n",
    "    \n",
    "    if span_one == span_two:\n",
    "        return feature_tuples\n",
    "    \n",
    "    if span_constitutes_span(span_one, span_two):\n",
    "        feature_tuples.append(irec_constitutes(span_one.uid, span_two.uid))\n",
    "        \n",
    "    if morphologically_similar(span_one, span_two):\n",
    "        feature_tuples.append(irec_morp_sim(span_one.uid, span_two.uid))\n",
    "    \n",
    "    # this simply takes too long... need to find ways to speed up that function\n",
    "#     if span_with_antonym(span_one, span_two):\n",
    "#         feature_tuples.append(irec_related(span_one.uid, span_two.uid))\n",
    "        \n",
    "    return feature_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "29daf2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO; need to think of ways to speed up this process\n",
    "## - avoid occurence in definitions (could use this to filter, e.g. occurs in over 30% of definitions then remove)\n",
    "## - would I want to remove spans that occur very often in constitutes\n",
    "\n",
    "spans_namespace_uid = SPANS.placeholder.defrag().__reduce__()[1][0]\n",
    "spans = [k for k in ua.UIDs[spans_namespace_uid].keys() if ua.UIDs[spans_namespace_uid][k] != 'schemeUID']\n",
    "spans_c = [CharacterSpan(span, ua.UIDs[spans_namespace_uid][span]) for span in spans]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fbbcba7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will need to compute 15550 x 15550 = 241802500 combinations\n"
     ]
    }
   ],
   "source": [
    "print(f\"Will need to compute {len(spans)} x {len(spans)} = {len(spans) * len(spans)} combinations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4b80c289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_list too memory intensive for the amount of combinations, need to yield         \n",
    "def batcher(iterable, batch_size):\n",
    "    iterator = iter(iterable)\n",
    "    while batch := list(islice(iterator, batch_size)):\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "aa299ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing feature triples for 241802500 span pairs, in groups of: 1000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                             | 0/241 [00:00<?, ?it/s]\n",
      "  0%|                                                                                                                                                                  | 1/241 [00:40<2:41:01, 40.26s/it]\n",
      "  1%|                                                                                                                                                                 | 2/241 [01:16<2:30:44, 37.85s/it]\n",
      "  1%|                                                                                                                                                                 | 3/241 [01:54<2:30:14, 37.88s/it]\n",
      "  2%|                                                                                                                                                                | 4/241 [02:33<2:31:43, 38.41s/it]\n",
      "  2%|                                                                                                                                                               | 5/241 [03:10<2:29:24, 37.99s/it]\n",
      "  2%|                                                                                                                                                               | 6/241 [03:50<2:30:35, 38.45s/it]\n",
      "  3%|                                                                                                                                                              | 7/241 [04:27<2:28:13, 38.01s/it]\n",
      "  3%|                                                                                                                                                             | 8/241 [05:06<2:29:16, 38.44s/it]\n",
      "  4%|                                                                                                                                                             | 9/241 [05:43<2:27:12, 38.07s/it]\n",
      "  4%|                                                                                                                                                           | 10/241 [06:21<2:26:01, 37.93s/it]\n",
      "  5%|                                                                                                                                                          | 11/241 [07:02<2:29:26, 38.98s/it]\n",
      "  5%|                                                                                                                                                          | 12/241 [07:42<2:30:00, 39.30s/it]\n",
      "  5%|                                                                                                                                                         | 13/241 [08:21<2:28:18, 39.03s/it]\n",
      "  6%|                                                                                                                                                        | 14/241 [08:53<2:20:14, 37.07s/it]\n",
      "  6%|                                                                                                                                                        | 15/241 [09:36<2:26:17, 38.84s/it]\n",
      "  7%|                                                                                                                                                       | 16/241 [10:18<2:28:50, 39.69s/it]\n",
      "  7%|                                                                                                                                                      | 17/241 [10:57<2:27:21, 39.47s/it]\n",
      "  7%|                                                                                                                                                      | 18/241 [11:31<2:20:29, 37.80s/it]\n",
      "  8%|                                                                                                                                                     | 19/241 [12:12<2:24:11, 38.97s/it]\n",
      "  8%|                                                                                                                                                    | 20/241 [12:50<2:22:16, 38.63s/it]\n",
      "  9%|                                                                                                                                                    | 21/241 [13:32<2:25:11, 39.60s/it]\n",
      "  9%|                                                                                                                                                   | 22/241 [14:09<2:21:29, 38.77s/it]\n",
      " 10%|                                                                                                                                                  | 23/241 [14:48<2:21:28, 38.94s/it]\n",
      " 10%|                                                                                                                                                 | 24/241 [15:31<2:24:38, 39.99s/it]\n",
      " 10%|                                                                                                                                                 | 25/241 [16:13<2:26:27, 40.68s/it]\n",
      " 11%|                                                                                                                                                | 26/241 [16:53<2:24:44, 40.39s/it]\n",
      " 11%|                                                                                                                                               | 27/241 [17:33<2:23:53, 40.34s/it]\n",
      " 12%|                                                                                                                                               | 28/241 [18:10<2:19:19, 39.25s/it]\n",
      " 12%|                                                                                                                                              | 29/241 [18:50<2:19:35, 39.51s/it]\n",
      " 12%|                                                                                                                                             | 30/241 [19:33<2:23:13, 40.73s/it]\n",
      " 13%|                                                                                                                                             | 31/241 [20:17<2:25:20, 41.52s/it]\n",
      " 13%|                                                                                                                                            | 32/241 [20:58<2:24:11, 41.39s/it]\n",
      " 14%|                                                                                                                                           | 33/241 [21:38<2:22:12, 41.02s/it]\n",
      " 14%|                                                                                                                                           | 34/241 [22:18<2:20:37, 40.76s/it]\n",
      " 15%|                                                                                                                                          | 35/241 [23:00<2:20:40, 40.97s/it]\n",
      " 15%|                                                                                                                                         | 36/241 [23:39<2:18:34, 40.56s/it]\n",
      " 15%|                                                                                                                                         | 37/241 [24:21<2:19:23, 41.00s/it]\n",
      " 16%|                                                                                                                                        | 38/241 [25:05<2:21:17, 41.76s/it]\n",
      " 16%|                                                                                                                                       | 39/241 [25:44<2:18:24, 41.11s/it]\n",
      " 17%|                                                                                                                                       | 40/241 [26:25<2:16:56, 40.88s/it]\n",
      " 17%|                                                                                                                                      | 41/241 [27:06<2:16:21, 40.91s/it]\n",
      " 17%|                                                                                                                                     | 42/241 [27:46<2:14:48, 40.65s/it]\n",
      " 18%|                                                                                                                                     | 43/241 [28:28<2:15:44, 41.13s/it]\n",
      " 18%|                                                                                                                                    | 44/241 [29:10<2:15:58, 41.41s/it]\n",
      " 19%|                                                                                                                                   | 45/241 [29:53<2:16:45, 41.86s/it]\n",
      " 19%|                                                                                                                                   | 46/241 [30:33<2:14:29, 41.38s/it]\n",
      " 20%|                                                                                                                                  | 47/241 [31:12<2:11:17, 40.61s/it]\n",
      " 20%|                                                                                                                                 | 48/241 [31:53<2:10:57, 40.71s/it]\n",
      " 20%|                                                                                                                                 | 49/241 [32:32<2:09:00, 40.31s/it]\n",
      " 21%|                                                                                                                                | 50/241 [33:15<2:10:18, 40.94s/it]\n",
      " 21%|                                                                                                                               | 51/241 [33:50<2:04:31, 39.32s/it]\n",
      " 22%|                                                                                                                               | 52/241 [34:33<2:07:10, 40.37s/it]\n",
      " 22%|                                                                                                                              | 53/241 [35:11<2:03:59, 39.57s/it]\n",
      " 22%|                                                                                                                             | 54/241 [35:56<2:08:34, 41.26s/it]\n",
      " 23%|                                                                                                                             | 55/241 [36:38<2:08:49, 41.55s/it]\n",
      " 23%|                                                                                                                            | 56/241 [37:21<2:08:59, 41.83s/it]\n",
      " 24%|                                                                                                                           | 57/241 [38:01<2:07:15, 41.49s/it]\n",
      " 24%|                                                                                                                           | 58/241 [38:42<2:05:44, 41.23s/it]\n",
      " 24%|                                                                                                                          | 59/241 [39:26<2:07:48, 42.13s/it]\n",
      " 25%|                                                                                                                         | 60/241 [40:10<2:08:19, 42.54s/it]\n",
      " 25%|                                                                                                                         | 61/241 [40:49<2:04:38, 41.55s/it]\n",
      " 26%|                                                                                                                        | 62/241 [41:35<2:07:28, 42.73s/it]\n",
      " 26%|                                                                                                                       | 63/241 [42:19<2:08:39, 43.37s/it]\n",
      " 27%|                                                                                                                       | 64/241 [43:00<2:05:43, 42.62s/it]\n",
      " 27%|                                                                                                                      | 65/241 [43:46<2:07:59, 43.63s/it]\n",
      " 27%|                                                                                                                     | 66/241 [44:27<2:04:52, 42.81s/it]\n",
      " 28%|                                                                                                                     | 67/241 [45:12<2:05:53, 43.41s/it]\n",
      " 28%|                                                                                                                    | 68/241 [45:55<2:04:53, 43.31s/it]\n",
      " 29%|                                                                                                                   | 69/241 [46:38<2:03:39, 43.14s/it]\n",
      " 29%|                                                                                                                   | 70/241 [47:23<2:04:38, 43.73s/it]\n",
      " 29%|                                                                                                                  | 71/241 [48:03<2:01:13, 42.79s/it]\n",
      " 30%|                                                                                                                 | 72/241 [48:51<2:04:16, 44.12s/it]\n",
      " 30%|                                                                                                                 | 73/241 [49:31<2:00:11, 42.93s/it]\n",
      " 31%|                                                                                                                | 74/241 [50:13<1:58:24, 42.54s/it]\n",
      " 31%|                                                                                                               | 75/241 [51:00<2:01:44, 44.00s/it]\n",
      " 32%|                                                                                                               | 76/241 [51:40<1:57:38, 42.78s/it]\n",
      " 32%|                                                                                                              | 77/241 [52:19<1:54:04, 41.73s/it]\n",
      " 32%|                                                                                                             | 78/241 [53:02<1:54:37, 42.19s/it]\n",
      " 33%|                                                                                                             | 79/241 [53:49<1:57:10, 43.40s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|                                                                                                            | 80/241 [54:30<1:54:58, 42.85s/it]\n",
      " 34%|                                                                                                           | 81/241 [55:13<1:54:19, 42.87s/it]\n",
      " 34%|                                                                                                           | 82/241 [55:53<1:51:22, 42.03s/it]\n",
      " 34%|                                                                                                          | 83/241 [56:39<1:53:25, 43.08s/it]\n",
      " 35%|                                                                                                         | 84/241 [57:21<1:52:06, 42.84s/it]\n",
      " 35%|                                                                                                        | 85/241 [58:06<1:52:50, 43.40s/it]\n",
      " 36%|                                                                                                        | 86/241 [58:47<1:50:46, 42.88s/it]\n",
      " 36%|                                                                                                       | 87/241 [59:33<1:52:01, 43.64s/it]\n",
      " 37%|                                                                                                     | 88/241 [1:00:13<1:48:32, 42.56s/it]\n",
      " 37%|                                                                                                     | 89/241 [1:01:00<1:51:35, 44.05s/it]\n",
      " 37%|                                                                                                    | 90/241 [1:01:42<1:49:02, 43.33s/it]\n",
      " 38%|                                                                                                   | 91/241 [1:02:27<1:49:25, 43.77s/it]\n",
      " 38%|                                                                                                   | 92/241 [1:03:08<1:46:40, 42.96s/it]\n",
      " 39%|                                                                                                  | 93/241 [1:03:54<1:48:04, 43.82s/it]\n",
      " 39%|                                                                                                 | 94/241 [1:04:36<1:45:55, 43.24s/it]\n",
      " 39%|                                                                                                 | 95/241 [1:05:18<1:44:20, 42.88s/it]\n",
      " 40%|                                                                                                | 96/241 [1:06:04<1:46:11, 43.94s/it]\n",
      " 40%|                                                                                               | 97/241 [1:06:50<1:47:02, 44.60s/it]\n",
      " 41%|                                                                                               | 98/241 [1:07:31<1:43:52, 43.58s/it]\n",
      " 41%|                                                                                              | 99/241 [1:08:13<1:41:52, 43.05s/it]\n",
      " 41%|                                                                                             | 100/241 [1:09:00<1:43:48, 44.18s/it]\n",
      " 42%|                                                                                            | 101/241 [1:09:42<1:41:50, 43.65s/it]\n",
      " 42%|                                                                                           | 102/241 [1:10:25<1:40:37, 43.44s/it]\n",
      " 43%|                                                                                           | 103/241 [1:11:11<1:41:12, 44.00s/it]\n",
      " 43%|                                                                                          | 104/241 [1:11:57<1:41:59, 44.67s/it]\n",
      " 44%|                                                                                         | 105/241 [1:12:36<1:37:40, 43.09s/it]\n",
      " 44%|                                                                                         | 106/241 [1:13:22<1:38:38, 43.84s/it]\n",
      " 44%|                                                                                        | 107/241 [1:14:06<1:37:49, 43.80s/it]\n",
      " 45%|                                                                                       | 108/241 [1:14:46<1:35:06, 42.91s/it]\n",
      " 45%|                                                                                       | 109/241 [1:15:31<1:35:49, 43.56s/it]\n",
      " 46%|                                                                                      | 110/241 [1:16:17<1:36:41, 44.29s/it]\n",
      " 46%|                                                                                     | 111/241 [1:16:58<1:33:21, 43.09s/it]\n",
      " 46%|                                                                                     | 112/241 [1:17:42<1:33:16, 43.39s/it]\n",
      " 47%|                                                                                    | 113/241 [1:18:23<1:31:20, 42.82s/it]\n",
      " 47%|                                                                                   | 114/241 [1:19:03<1:28:39, 41.88s/it]\n",
      " 48%|                                                                                   | 115/241 [1:19:49<1:30:40, 43.18s/it]\n",
      " 48%|                                                                                  | 116/241 [1:20:34<1:30:42, 43.54s/it]\n",
      " 49%|                                                                                 | 117/241 [1:21:14<1:27:44, 42.46s/it]\n",
      " 49%|                                                                                 | 118/241 [1:21:56<1:27:00, 42.44s/it]\n",
      " 49%|                                                                                | 119/241 [1:22:36<1:24:43, 41.67s/it]\n",
      " 50%|                                                                               | 120/241 [1:23:15<1:22:13, 40.78s/it]\n",
      " 50%|                                                                               | 121/241 [1:23:57<1:22:23, 41.19s/it]\n",
      " 51%|                                                                              | 122/241 [1:24:39<1:22:22, 41.53s/it]\n",
      " 51%|                                                                             | 123/241 [1:25:21<1:21:59, 41.69s/it]\n",
      " 51%|                                                                             | 124/241 [1:26:00<1:19:48, 40.92s/it]\n",
      " 52%|                                                                            | 125/241 [1:26:43<1:20:07, 41.45s/it]\n",
      " 52%|                                                                           | 126/241 [1:27:22<1:18:14, 40.83s/it]\n",
      " 53%|                                                                           | 127/241 [1:28:05<1:18:24, 41.27s/it]\n",
      " 53%|                                                                          | 128/241 [1:28:48<1:18:41, 41.79s/it]\n",
      " 54%|                                                                          | 129/241 [1:29:26<1:16:13, 40.83s/it]\n",
      " 54%|                                                                         | 130/241 [1:30:09<1:16:24, 41.30s/it]\n",
      " 54%|                                                                        | 131/241 [1:30:50<1:15:53, 41.40s/it]\n",
      " 55%|                                                                        | 132/241 [1:31:34<1:16:36, 42.17s/it]\n",
      " 55%|                                                                       | 133/241 [1:32:13<1:14:17, 41.28s/it]\n",
      " 56%|                                                                      | 134/241 [1:32:52<1:12:00, 40.38s/it]\n",
      " 56%|                                                                      | 135/241 [1:33:34<1:12:27, 41.02s/it]\n",
      " 56%|                                                                     | 136/241 [1:34:18<1:13:07, 41.78s/it]\n",
      " 57%|                                                                    | 137/241 [1:35:00<1:12:38, 41.91s/it]\n",
      " 57%|                                                                    | 138/241 [1:35:39<1:10:30, 41.08s/it]\n",
      " 58%|                                                                   | 139/241 [1:36:24<1:11:45, 42.21s/it]\n",
      " 58%|                                                                  | 140/241 [1:37:06<1:11:00, 42.18s/it]\n",
      " 59%|                                                                  | 141/241 [1:37:45<1:08:40, 41.20s/it]\n",
      " 59%|                                                                 | 142/241 [1:38:28<1:08:50, 41.72s/it]\n",
      " 59%|                                                                | 143/241 [1:39:11<1:08:44, 42.09s/it]\n",
      " 60%|                                                                | 144/241 [1:39:54<1:08:30, 42.37s/it]\n",
      " 60%|                                                               | 145/241 [1:40:32<1:05:47, 41.12s/it]\n",
      " 61%|                                                              | 146/241 [1:41:15<1:06:04, 41.73s/it]\n",
      " 61%|                                                              | 147/241 [1:41:59<1:06:27, 42.42s/it]\n",
      " 61%|                                                             | 148/241 [1:42:38<1:04:05, 41.35s/it]\n",
      " 62%|                                                            | 149/241 [1:43:21<1:04:05, 41.80s/it]\n",
      " 62%|                                                            | 150/241 [1:43:59<1:01:46, 40.73s/it]\n",
      " 63%|                                                           | 151/241 [1:44:43<1:02:42, 41.80s/it]\n",
      " 63%|                                                          | 152/241 [1:45:24<1:01:39, 41.57s/it]\n",
      " 63%|                                                          | 153/241 [1:46:09<1:02:08, 42.37s/it]\n",
      " 64%|                                                          | 154/241 [1:46:45<58:54, 40.62s/it]\n",
      " 64%|                                                        | 155/241 [1:47:30<1:00:09, 41.97s/it]\n",
      " 65%|                                                        | 156/241 [1:48:09<58:11, 41.07s/it]\n",
      " 65%|                                                        | 157/241 [1:48:52<58:17, 41.64s/it]\n",
      " 66%|                                                       | 158/241 [1:49:32<56:35, 40.91s/it]\n",
      " 66%|                                                      | 159/241 [1:50:14<56:41, 41.48s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|                                                      | 160/241 [1:50:56<56:01, 41.50s/it]\n",
      " 67%|                                                     | 161/241 [1:51:36<54:37, 40.97s/it]\n",
      " 67%|                                                    | 162/241 [1:52:18<54:36, 41.47s/it]\n",
      " 68%|                                                    | 163/241 [1:52:58<53:04, 40.82s/it]\n",
      " 68%|                                                   | 164/241 [1:53:39<52:38, 41.01s/it]\n",
      " 68%|                                                  | 165/241 [1:54:17<50:43, 40.04s/it]\n",
      " 69%|                                                  | 166/241 [1:55:01<51:42, 41.37s/it]\n",
      " 69%|                                                 | 167/241 [1:55:40<49:54, 40.46s/it]\n",
      " 70%|                                                | 168/241 [1:56:24<50:38, 41.63s/it]\n",
      " 70%|                                                | 169/241 [1:57:03<48:53, 40.74s/it]\n",
      " 71%|                                               | 170/241 [1:57:44<48:32, 41.02s/it]\n",
      " 71%|                                              | 171/241 [1:58:28<48:39, 41.71s/it]\n",
      " 71%|                                              | 172/241 [1:59:09<47:47, 41.55s/it]\n",
      " 72%|                                             | 173/241 [1:59:52<47:29, 41.91s/it]\n",
      " 72%|                                            | 174/241 [2:00:32<46:12, 41.37s/it]\n",
      " 73%|                                            | 175/241 [2:01:14<45:45, 41.60s/it]\n",
      " 73%|                                           | 176/241 [2:01:58<45:54, 42.38s/it]\n",
      " 73%|                                          | 177/241 [2:02:39<44:49, 42.03s/it]\n",
      " 74%|                                          | 178/241 [2:03:16<42:32, 40.52s/it]\n",
      " 74%|                                         | 179/241 [2:03:58<42:20, 40.97s/it]\n",
      " 75%|                                        | 180/241 [2:04:43<42:44, 42.04s/it]\n",
      " 75%|                                        | 181/241 [2:05:26<42:30, 42.51s/it]\n",
      " 76%|                                       | 182/241 [2:06:06<41:04, 41.78s/it]\n",
      " 76%|                                      | 183/241 [2:06:49<40:34, 41.98s/it]\n",
      " 76%|                                      | 184/241 [2:07:28<39:01, 41.08s/it]\n",
      " 77%|                                     | 185/241 [2:08:09<38:15, 40.99s/it]\n",
      " 77%|                                    | 186/241 [2:08:52<38:12, 41.68s/it]\n",
      " 78%|                                    | 187/241 [2:09:35<37:47, 41.99s/it]\n",
      " 78%|                                   | 188/241 [2:10:14<36:24, 41.22s/it]\n",
      " 78%|                                  | 189/241 [2:10:59<36:37, 42.27s/it]\n",
      " 79%|                                  | 190/241 [2:11:37<34:59, 41.17s/it]\n",
      " 79%|                                 | 191/241 [2:12:15<33:30, 40.21s/it]\n",
      " 80%|                                | 192/241 [2:12:58<33:19, 40.81s/it]\n",
      " 80%|                                | 193/241 [2:13:41<33:09, 41.45s/it]\n",
      " 80%|                               | 194/241 [2:14:18<31:37, 40.36s/it]\n",
      " 81%|                              | 195/241 [2:15:01<31:25, 40.98s/it]\n",
      " 81%|                              | 196/241 [2:15:43<31:06, 41.48s/it]\n",
      " 82%|                             | 197/241 [2:16:23<29:54, 40.78s/it]\n",
      " 82%|                            | 198/241 [2:17:06<29:45, 41.53s/it]\n",
      " 83%|                            | 199/241 [2:17:44<28:26, 40.63s/it]\n",
      " 83%|                           | 200/241 [2:18:27<28:04, 41.09s/it]\n",
      " 83%|                          | 201/241 [2:19:10<27:49, 41.73s/it]\n",
      " 84%|                          | 202/241 [2:19:53<27:25, 42.20s/it]\n",
      " 84%|                         | 203/241 [2:20:31<25:59, 41.05s/it]\n",
      " 85%|                        | 204/241 [2:21:14<25:40, 41.62s/it]\n",
      " 85%|                        | 205/241 [2:21:58<25:18, 42.17s/it]\n",
      " 85%|                       | 206/241 [2:22:37<24:03, 41.24s/it]\n",
      " 86%|                      | 207/241 [2:23:18<23:22, 41.26s/it]\n",
      " 86%|                      | 208/241 [2:24:01<22:59, 41.81s/it]\n",
      " 87%|                     | 209/241 [2:24:46<22:44, 42.63s/it]\n",
      " 87%|                    | 210/241 [2:25:26<21:41, 41.97s/it]\n",
      " 88%|                    | 211/241 [2:26:05<20:30, 41.00s/it]\n",
      " 88%|                   | 212/241 [2:26:47<20:01, 41.42s/it]\n",
      " 88%|                  | 213/241 [2:27:31<19:37, 42.06s/it]\n",
      " 89%|                  | 214/241 [2:28:11<18:38, 41.43s/it]\n",
      " 89%|                 | 215/241 [2:28:53<18:01, 41.58s/it]\n",
      " 90%|                | 216/241 [2:29:37<17:38, 42.35s/it]\n",
      " 90%|                | 217/241 [2:30:16<16:32, 41.36s/it]\n",
      " 90%|               | 218/241 [2:31:00<16:09, 42.15s/it]\n",
      " 91%|              | 219/241 [2:31:45<15:42, 42.84s/it]\n",
      " 91%|              | 220/241 [2:32:23<14:34, 41.63s/it]\n",
      " 92%|             | 221/241 [2:33:07<14:03, 42.16s/it]\n",
      " 92%|            | 222/241 [2:33:45<13:00, 41.09s/it]\n",
      " 93%|            | 223/241 [2:34:29<12:33, 41.86s/it]\n",
      " 93%|           | 224/241 [2:35:07<11:33, 40.78s/it]\n",
      " 93%|          | 225/241 [2:35:52<11:10, 41.91s/it]\n",
      " 94%|          | 226/241 [2:36:31<10:16, 41.09s/it]\n",
      " 94%|         | 227/241 [2:37:14<09:42, 41.63s/it]\n",
      " 95%|        | 228/241 [2:37:56<09:02, 41.75s/it]\n",
      " 95%|        | 229/241 [2:38:39<08:27, 42.30s/it]\n",
      " 95%|       | 230/241 [2:39:18<07:32, 41.14s/it]\n",
      " 96%|      | 231/241 [2:40:02<07:01, 42.15s/it]\n",
      " 96%|      | 232/241 [2:40:45<06:21, 42.39s/it]\n",
      " 97%|     | 233/241 [2:41:25<05:32, 41.56s/it]\n",
      " 97%|    | 234/241 [2:42:09<04:56, 42.40s/it]\n",
      " 98%|    | 235/241 [2:42:47<04:06, 41.02s/it]\n",
      " 98%|   | 236/241 [2:43:32<03:30, 42.07s/it]\n",
      " 98%|  | 237/241 [2:44:11<02:45, 41.30s/it]\n",
      " 99%|  | 238/241 [2:44:48<01:59, 39.84s/it]\n",
      " 99%| | 239/241 [2:45:30<01:21, 40.52s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 240/241 [2:46:13<00:41, 41.50s/it]\n",
      "100%|| 241/241 [2:46:51<00:00, 40.30s/it]\n",
      "242it [2:47:24, 37.98s/it]                                                                                                                                                                                \n",
      "242it [2:47:24, 41.50s/it]\u001b[A\n",
      "242it [2:47:24, 41.50s/it]\n"
     ]
    }
   ],
   "source": [
    "# Split the processing into multiple parts again and save intermediate states for re-use\n",
    "max_num_cpu_threads = 1024\n",
    "subset_size = 1000000 # 1M\n",
    "\n",
    "feature_files = [f for f in graph_output_fp.glob('features*.pkl')]\n",
    "feature_tuples = []\n",
    "if len(feature_files) == (len(spans_c)*len(spans_c)/subset_size):\n",
    "    for ff in feature_files:\n",
    "        feature_tuples += pickle.load(open(ff, 'rb'))\n",
    "else:\n",
    "    print(f\"Preparing feature triples for {len(spans) * len(spans)} span pairs, in groups of: {subset_size}\")\n",
    "    subset_idx = 0 \n",
    "    with tqdm(total=(len(spans_c) * len(spans_c))//subset_size) as pbar:\n",
    "        for subset in tqdm(batcher(product(spans_c, spans_c), subset_size)):\n",
    "            subset_features = []\n",
    "            subset_file_name = graph_output_fp.joinpath(\"features_part_{}.pkl\".format(subset_idx))\n",
    "            subset_idx += 1\n",
    "            if subset_file_name.exists():\n",
    "                # already computed previously\n",
    "                continue\n",
    "\n",
    "            with concurrent.futures.ThreadPoolExecutor(max_workers=max_num_cpu_threads) as executor: # ThreadPoolExecutor\n",
    "                futures = [executor.submit(compute_features, pair) for pair in subset]\n",
    "\n",
    "            subset_features += [f.result() for f in futures if f.result()]\n",
    "            with open(subset_file_name, 'wb') as f:\n",
    "                pickle.dump(subset_features, f)\n",
    "                \n",
    "            pbar.update(1)\n",
    "            \n",
    "    # Once all features are computed; combine them in a single list of tuples to add to the graph\n",
    "    feature_files = [f for f in graph_output_fp.glob('features*.pkl')]\n",
    "    for ff in feature_files:\n",
    "        feature_tuples += pickle.load(open(ff, 'rb'))\n",
    "\n",
    "# this still creates 250 files!!!... :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5f5ced48",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuples_to_add = [t[0] for t_list in feature_tuples for t in t_list]\n",
    "irec_graph = add_tuples(irec_graph, tuples_to_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d99ff2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO; spans_with_potential_antonym x spans_with_potential_antonym pairs\n",
    "# consider adding wikidata classes as spans with is_a relation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf754ed1",
   "metadata": {},
   "source": [
    "#### Save final graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "83372d92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Graph identifier=N6b829faf098f4276bebec242edb83a35 (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irec_graph.serialize(destination=graph_output_fp.joinpath(\"featured_graph.ttl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5b3b39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89337b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ua.print_node_by_text(irec_graph, 'atrium', SPANS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b74867",
   "metadata": {},
   "outputs": [],
   "source": [
    "ua.print_node_by_text(irec_graph, 'a continuous space', SPANS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab2d685",
   "metadata": {},
   "outputs": [],
   "source": [
    "ua.print_node_by_id(irec_graph, 'Q189265', WIKI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0a4c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ua.print_node_by_text(irec_graph, 'atrium', CONCEPTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108dc151",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bac701d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ua.print_node_by_text(irec_graph, 'wet room', CONCEPTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fc1d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "ua.print_node_by_text(irec_graph, 'sanitary accommodation', CONCEPTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52be14d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ua.print_node_by_text(irec_graph, 'wet room', CONCEPTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac59eadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ua.print_node_by_text(irec_graph, 'tundish', SPANS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206da495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo; / done now?\n",
    "# - some concepts missing\n",
    "# - avoid empty span;\n",
    "# - need to avoid duplicate wiki concepts, simply add "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fb9e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ua.print_node_by_text(irec_graph, 'ferrite', SPANS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4969f530",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98afca6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
