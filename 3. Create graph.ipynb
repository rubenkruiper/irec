{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00aa64c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import pickle\n",
    "import glob, os\n",
    "import subprocess\n",
    "import json, random\n",
    "import requests, urllib\n",
    "import concurrent.futures\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from tqdm import tqdm\n",
    "from typing import List, Any, List, Dict, Tuple\n",
    "from pathlib import Path\n",
    "from textblob import TextBlob\n",
    "from threading import current_thread\n",
    "from itertools import islice, product\n",
    "from collections import Counter\n",
    "from nltk.corpus import wordnet as wn\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "from sklearn.neighbors import kneighbors_graph, KNeighborsClassifier\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "from rdflib import URIRef, BNode, Literal, Namespace, Graph\n",
    "from rdflib.namespace import XSD, RDF, RDFS, SKOS, NamespaceManager\n",
    "\n",
    "from utils.spar_utils import TermExtractor\n",
    "from utils.cluster_utils import levenshtein\n",
    "from utils.embedding_utils import Embedder\n",
    "from utils.cleaning_utils import split_list, custom_cleaning_rules, remove_unicode_chars, remove_determiners"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11663171",
   "metadata": {},
   "source": [
    "We would like to express the following features/relations:\n",
    "* Dictionary definition terms, which are always concepts\n",
    "  * We'll use the source as namespace, and corresponding concept identifier if it exists\n",
    "  * SKOS is used to establish a mapping (e.g., skos:exactMatch) and add the definition (skos:definition)\n",
    "* Special properties that we want to capture between words, which may help identify concepts:\n",
    "  * Word is part of MWE\n",
    "  * Morphologically similar words; stemming & Levenshtein distance\n",
    "  * Semantically similar words; distributed similarity (NNs)\n",
    "  * Acronyms\n",
    "  * Related, this is a generic relation, e.g., a `ampere` is related to `electric current`\n",
    "  * Domain-specificity; foreground or background term following our filtering procedure\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7038359e",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_output_fp = Path.cwd().joinpath(\"data\", \"graph_output\")\n",
    "graph_output_fp.mkdir(parents=True, exist_ok=True) # create directory if it doesn't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8d2fdc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979b0616",
   "metadata": {},
   "source": [
    "### Prepare namespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2b60e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = Namespace(\"https://example.org/top_concept_for_visulisation/#\")\n",
    "WIKI = Namespace(\"https://www.wikidata.org/wiki/#\")\n",
    "# Note: that UNICLASS is not a namespace (yet) only has identifiers \n",
    "UNICLASS = Namespace(\"https://www.example.org/uniclass/#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4081e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROV = Namespace(\"http://www.w3.org/ns/prov#\")\n",
    "DCT = Namespace(\"http://purl.org/dc/terms/#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "788665e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://www.w3.org/ns/prov'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROV.placeholder.defrag().__reduce__()[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "098e92aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example/placeholder URLs for the IReC project \n",
    "IREC_ontology_URL = \"https://schema.irec.org/#\"\n",
    "IREC_spans_URL = \"https://spans.irec.org/#\"\n",
    "IREC_concepts_URL = \"https://concepts.irec.org/#\"\n",
    "\n",
    "# create our custom namespace for the schema to store spans\n",
    "IREC = Namespace(IREC_ontology_URL)\n",
    "\n",
    "# create a custom namespace to store spans and concepts\n",
    "SPANS = Namespace(IREC_spans_URL)\n",
    "CONCEPTS = Namespace(IREC_concepts_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcef6e56",
   "metadata": {},
   "source": [
    "### graph creation utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2e8ce74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UID_assigner:\n",
    "    def __init__(self):\n",
    "        self.UIDs = {}\n",
    "        self.UID = 0\n",
    "        self.scheme_uids = {}\n",
    "        \n",
    "    def get_scheme_UID(self, namespace: Namespace):\n",
    "        \"\"\"\n",
    "        Determines which type of UID to assign, based on the namespace.\n",
    "        \"\"\"\n",
    "        return [x for x in self.UIDs[namespace._.defrag().__reduce__()[1][0]].values() if x == \"schemeUID\"][0]\n",
    "        \n",
    "    def assign_UID(self, text, namespace: Namespace):\n",
    "        \"\"\"\n",
    "        Determines which type of UID to assign, based on the namespace.\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            raise Exception(\"Not text label provided to assign a UID.\")\n",
    "        if namespace == SPANS:\n",
    "            return self.span_UID(text)\n",
    "        elif namespace == CONCEPTS:\n",
    "            return self.concept_UID(text)\n",
    "        else:\n",
    "            print(\"UID assignment not set up for this namespace, maybe use UID_assigner.keep_track_of_existing_UID()\")\n",
    "            \n",
    "    \n",
    "    def span_UID(self, text):\n",
    "        \"\"\"\n",
    "        NOTE: each text span is a unique identifier in and of itself. We'll simply convert the text span to \n",
    "        a URL friendly representation.\n",
    "        \"\"\"\n",
    "        text = text.strip().encode(\"ascii\", \"ignore\").decode()\n",
    "        n_space = SPANS.placeholder.defrag().__reduce__()[1][0]\n",
    "        if n_space not in self.UIDs:\n",
    "            self.UIDs[n_space] = {}\n",
    "        \n",
    "        urltext = urllib.parse.quote(text)\n",
    "        if text not in self.UIDs[n_space]:\n",
    "            self.UIDs[n_space][text] = urltext\n",
    "            \n",
    "        return self.UIDs[n_space][text]\n",
    "        \n",
    "    def concept_UID(self, text):\n",
    "        \"\"\"\n",
    "        For now I'll create my own dumb interger-based UIDs for nodes as a simple shortcut, split per namespace\n",
    "        \"\"\"\n",
    "        n_space = CONCEPTS.placeholder.defrag().__reduce__()[1][0]\n",
    "        text = text.strip().encode(\"ascii\", \"ignore\").decode()\n",
    "        if n_space not in self.UIDs:\n",
    "            self.UIDs[n_space] = {}\n",
    "        \n",
    "        if text not in self.UIDs[n_space]:\n",
    "            self.UID += 1\n",
    "            self.UIDs[n_space][text] = str(self.UID)\n",
    "            \n",
    "        return self.UIDs[n_space][text]\n",
    "        \n",
    "    def keep_track_of_existing_UID(self, text:str, existing_uid: str, namespace:Namespace):\n",
    "        \"\"\"\n",
    "        Simply keep track of UIDs that exist in the provided namespace.\n",
    "        \"\"\"\n",
    "        text = text.strip()\n",
    "        n_space = namespace.placeholder.defrag().__reduce__()[1][0]\n",
    "        if n_space not in self.UIDs:\n",
    "            self.UIDs[n_space] = {}\n",
    "            \n",
    "        if text not in self.UIDs[n_space]:\n",
    "            # already seen by this UID assigner\n",
    "            self.UIDs[n_space][text] = existing_uid\n",
    "            \n",
    "        return existing_uid\n",
    "    \n",
    "    def retrieve_uid_by_text(self, node_text, namespace: Namespace = SPANS):\n",
    "        n_space = namespace.placeholder.defrag().__reduce__()[1][0]\n",
    "        node_text = node_text.strip()\n",
    "        if node_text in self.UIDs[n_space]:\n",
    "            return self.UIDs[n_space][node_text]\n",
    "        else:\n",
    "            return None \n",
    "        \n",
    "    def count_nodes_in_namespace(self, namespace: Namespace = SPANS):\n",
    "        n_space = namespace.placeholder.defrag().__reduce__()[1][0]\n",
    "        print(f\"Number of nodes in '{n_space}': {len(self.UIDs[n_space])}\")\n",
    "        return len(self.UIDs[n_space])\n",
    "        \n",
    "    def print_node_by_id(self, graph, node_id, namespace: Namespace = SPANS):\n",
    "        for s, p, o in graph.triples((namespace[str(node_id)],  None, None)):\n",
    "            print(f\"{s.split('#')[-1]} ; {p.split('#')[-1]} ; {o.split('#')[-1]}\")\n",
    "        \n",
    "    def print_node_by_text(self, graph, node_text, namespace: Namespace = SPANS):\n",
    "        n_space = namespace.placeholder.defrag().__reduce__()[1][0]\n",
    "        node_text = node_text.strip()\n",
    "        node_id = self.UIDs[n_space][node_text]\n",
    "        # find all triples with subject\n",
    "        for s, p, o in graph.triples((namespace[node_id],  None, None)):\n",
    "            print(f\"{s.split('#')[-1]} ; {p.split('#')[-1]} ; {o.split('#')[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "772b6692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These wrappers only exist to help me consistently add nodes to the graph\n",
    "\n",
    "def dct_title(node_uid: str, title: str, namespace: Namespace) -> List[Tuple]:\n",
    "    return [(namespace[node_uid], DCT.title,  Literal(title.strip(), lang='en'))]\n",
    "\n",
    "def provenance(node_uid: str, source: URIRef, namespace: Namespace) -> List[Tuple]:\n",
    "    \"\"\" temp source attribution \"\"\"\n",
    "    return [(namespace[node_uid], PROV.hadPrimarySource, source)]\n",
    "\n",
    "def prov_agent(node_uid: str, agent: URIRef, namespace: Namespace) -> List[Tuple]:\n",
    "    \"\"\" temp agent attribution (for Spans generated by SPaR.txt) \"\"\"\n",
    "    return [(namespace[node_uid], PROV.wasAttributedTo, agent)]\n",
    "\n",
    "def rdf_type(subject_node_uid, object_node_uid,\n",
    "                subject_namespace: Namespace=SPANS, object_namespace: Namespace=SPANS) -> List[Tuple]:\n",
    "    \"\"\" temp agent attribution (for Spans generated by SPaR.txt) \"\"\"\n",
    "    return [(subject_namespace[subject_node_uid], RDF.type, object_namespace[object_node_uid])]\n",
    "\n",
    "# SKOS \n",
    "def skos_scheme(node_uid, namespace: Namespace=CONCEPTS) -> List[Tuple]:\n",
    "    \"\"\" Node that identifies the concep scheme with a URI, expecting/using as scheme root \"\"\"\n",
    "    return [(namespace[node_uid], RDF.type, SKOS.ConceptScheme)]\n",
    "\n",
    "def skos_top_concept(node_uid, top_concept_uid, \n",
    "                    namespace: Namespace=CONCEPTS, top_concept_namespace: Namespace=CONCEPTS) -> List[Tuple]:\n",
    "    \"\"\" Currently, we mainly use the top-concept for visualisation. \"\"\"\n",
    "    return [(namespace[node_uid], SKOS.hasTopConcept, top_concept_namespace[top_concept_uid]),\n",
    "            (top_concept_namespace[top_concept_uid], SKOS.topConceptOf, namespace[node_uid])]\n",
    "\n",
    "def skos_in_scheme(node_uid, scheme_uid, namespace: Namespace=CONCEPTS, scheme_namespace: Namespace=CONCEPTS) -> List[Tuple]:\n",
    "    \"\"\" Keep track of the scheme/source of a node. \"\"\"\n",
    "    return [(namespace[node_uid], SKOS.inScheme, scheme_namespace[scheme_uid])]\n",
    "\n",
    "def skos_node(node_uid, text, namespace: Namespace=CONCEPTS) -> List[Tuple]:\n",
    "    \"\"\" Add a concept with prefLabel to the graph in the CONCEPTS namespace, of type SKOS.Concept \"\"\"\n",
    "    return [(namespace[node_uid], RDF.type, SKOS.Concept), \n",
    "            (namespace[node_uid], SKOS.prefLabel, Literal(text.strip(), lang='en'))]\n",
    "\n",
    "def skos_prefLabel(node_uid, text, namespace: Namespace=CONCEPTS) -> List[Tuple]:\n",
    "    \"\"\" Add the text label for a node \"\"\"\n",
    "    return [(namespace[node_uid], SKOS.prefLabel, Literal(text.strip(), lang='en'))]\n",
    "\n",
    "def skos_altLabel(node_uid, alt_label_uid, namespace: Namespace=CONCEPTS)-> List[Tuple]:\n",
    "    \"\"\" Add an alternative text label for a concept node \"\"\"\n",
    "    return [(namespace[node_uid], SKOS.altLabel, namespace[alt_label_uid]), \n",
    "            (namespace[alt_label_uid], SKOS.altLabel, namespace[node_uid])]\n",
    "\n",
    "def skos_exact_match(subject_node_uid, object_node_uid,\n",
    "                subject_namespace: Namespace=SPANS, object_namespace: Namespace=CONCEPTS) -> List[Tuple]:\n",
    "    \"\"\" Denotes an exact match between two nodes, would expect the nodes to be in different vocabularies \"\"\"\n",
    "    return [(subject_namespace[subject_node_uid], SKOS.exactMatch, object_namespace[object_node_uid])]\n",
    "\n",
    "def skos_related(subject_node_uid, object_node_uid,\n",
    "                subject_namespace: Namespace=SPANS, object_namespace: Namespace=CONCEPTS) -> List[Tuple]:\n",
    "    \"\"\" Denotes a relation between two nodes, would expect the nodes to be in different vocabularies \"\"\"\n",
    "    return [(subject_namespace[subject_node_uid], SKOS.related, object_namespace[object_node_uid])]\n",
    "    \n",
    "def skos_broader(narrower_node_uid, broader_node_uid, \n",
    "                 narrower_namespace: Namespace=CONCEPTS, broader_namespace: Namespace=CONCEPTS) -> List[Tuple]:\n",
    "    \"\"\" Assuming narrower/broader is always reflexive, would expect the nodes to be in different vocabularies \"\"\"\n",
    "    return [(broader_namespace[narrower_node_uid], SKOS.narrower, narrower_namespace[broader_node_uid]),\n",
    "            (narrower_namespace[broader_node_uid], SKOS.broader, broader_namespace[narrower_node_uid])]\n",
    "    \n",
    "def skos_definition(node_uid, definition_text, namespace: Namespace=CONCEPTS) -> List[Tuple]:\n",
    "    \"\"\" Add a definition, if provided in the Merged Approved Documents \"\"\"\n",
    "    return [(namespace[node_uid], SKOS.definition, Literal(definition_text.strip(), lang='en'))]\n",
    "\n",
    "def skos_note(node_uid, note_text, namespace: Namespace=CONCEPTS) -> List[Tuple]:\n",
    "    \"\"\" Some notes exist in the approved docs at least, containing useful information \"\"\"\n",
    "    return [(namespace[node_uid], SKOS.note, Literal(note_text.strip(), lang='en'))]\n",
    "\n",
    "\n",
    "\n",
    "# IREC functions and REFERENCE\n",
    "IREC.CharacterSpan # A span is a sequence of characters that occurs verbatim in a text, either contiguous or discontiguos as extracted by SPaR.txt (Kruiper et al., 2021).   \n",
    "IREC.constitutes  # Indicates that a span constitutes another span, e.g., the Multi-Word Expression (MWE) Span `hot water storage system` the Span `storage`.\n",
    "IREC.isMorphologicallySimilarTo # Indicates that a Span is morphologically similar to another Span, e.g., they may have the same stem or a small Levenshtein distance.\n",
    "IREC.isSemanticallySimilarTo # Indicates that a Span is semantically similar to another Span, following a cosine similarity between their  embeddings.\n",
    "IREC.related # General way to indicate some relation between two spans, e.g., `ampere` is related to `electric current`\n",
    "IREC.definitionRelation # One span occurs in the definition of the other span.\n",
    "IREC.hasAcronym # A Span can have an acronym, e.g., `British Standards Institute` has the acronym `BSI`.\n",
    "IREC.isAcronymOf # A Span can have an acronym, e.g., `BSI` is the acronym for `British Standards Institute`.\n",
    "IREC.hasAntonym # Property that relates a Span to another Span, each being each other's antonyms.\n",
    "IREC.wikiDefinition # One of potentially multiple WikiData definitions for the irec:CharacterSpan node.\n",
    "IREC.wikiClassLabel # One of potentially multiple WikiData class labels for the irec:CharacterSpan node.\n",
    "\n",
    "def irec_span(node_uid, text, namespace: Namespace=SPANS) -> List[Tuple]:\n",
    "    \"\"\" Add a span node in the SPANS namespace, of type IREC.CharacterSpan and the span text set as its RDF.label \"\"\"\n",
    "    # is preflabel a property? I would assume so\n",
    "    return [(namespace[node_uid], RDF.type, IREC.CharacterSpan), \n",
    "            (namespace[node_uid], RDFS.label,  Literal(text.strip(), lang='en'))]\n",
    "\n",
    "def irec_constitutes(subject_node_uid, object_node_uid,\n",
    "                     subject_namespace: Namespace=SPANS, object_namespace: Namespace=SPANS) -> List[Tuple]:\n",
    "    \"\"\" Indicates that somewhere in the label of the first SPAN node, you can find the second span's label \"\"\"\n",
    "    return [(subject_namespace[subject_node_uid], IREC.constitutes, object_namespace[object_node_uid])]\n",
    "\n",
    "def irec_morp_sim(subject_node_uid, object_node_uid,\n",
    "                  subject_namespace: Namespace=SPANS, object_namespace: Namespace=SPANS) -> List[Tuple]:\n",
    "    \"\"\" Indicates that the labels of two SPAN nodes are morphologically similar \"\"\"\n",
    "    return [(subject_namespace[subject_node_uid], IREC.isMorphologicallySimilarTo, object_namespace[object_node_uid])]\n",
    "\n",
    "def irec_sem_sim(subject_node_uid, object_node_uid,\n",
    "                 subject_namespace: Namespace=SPANS, object_namespace: Namespace=SPANS) -> List[Tuple]:\n",
    "    \"\"\" Indicates that the labels of two SPAN nodes are semantically similar, following the distributed semantics hypothesis \"\"\"\n",
    "    return [(subject_namespace[subject_node_uid], IREC.isSemanticallySimilarTo, object_namespace[object_node_uid])]\n",
    "\n",
    "def irec_related(subject_node_uid, object_node_uid,\n",
    "                 subject_namespace: Namespace=SPANS, object_namespace: Namespace=SPANS) -> List[Tuple]:\n",
    "    \"\"\" Indicates that a Span is related in SOME way to another Span. \"\"\"\n",
    "    return [(subject_namespace[subject_node_uid], IREC.related, object_namespace[object_node_uid])]\n",
    "\n",
    "def irec_definition_related(subject_node_uid, object_node_uid,\n",
    "                 subject_namespace: Namespace=SPANS, object_namespace: Namespace=SPANS) -> List[Tuple]:\n",
    "    \"\"\" Indicates that a Span is related because it occurs in the definition of another Span. \"\"\"\n",
    "    return [(subject_namespace[subject_node_uid], IREC.definitionRelation, object_namespace[object_node_uid])]\n",
    "\n",
    "def irec_has_acronym(subject_node_uid, object_node_uid,\n",
    "                     subject_namespace: Namespace=SPANS, object_namespace: Namespace=SPANS) -> List[Tuple]:\n",
    "    \"\"\" Indicates that the label of the subject node has an acronym, ergo the label of the object node  \"\"\"\n",
    "    return [(subject_namespace[subject_node_uid], IREC.hasAcronym, object_namespace[object_node_uid]),\n",
    "            (object_namespace[object_node_uid], IREC.isAcronymOf, subject_namespace[subject_node_uid])]\n",
    "\n",
    "def irec_antonym(subject_node_uid, object_node_uid,\n",
    "                 subject_namespace: Namespace=SPANS, object_namespace: Namespace=SPANS) -> List[Tuple]:\n",
    "    \"\"\" Indicates that the label of the subject node is an antonym of the label of the object node  \"\"\"\n",
    "    return [(subject_namespace[subject_node_uid], IREC.hasAntonym, object_namespace[object_node_uid])]\n",
    "\n",
    "def irec_wikidef(node_uid, definition_text, namespace: Namespace=SPANS) -> List[Tuple]:\n",
    "    \"\"\" Add a candidate definition, as retrieved from WikiData for the Span \"\"\"\n",
    "    return [(namespace[node_uid], IREC.wikiDefinition, Literal(definition_text.strip(), lang='en'))]\n",
    "\n",
    "def irec_domain(node_uid, domain_text=\"Out of domain\", namespace: Namespace=SPANS) -> List[Tuple]:\n",
    "    \"\"\" Add a candidate class label, as retrieved from WikiData for the Span \"\"\"\n",
    "    return [(namespace[node_uid], IREC.domain, Literal(domain_text.strip(), lang='en'))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08060b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_tuples(graph, tuples):\n",
    "    \"\"\"\n",
    "    We'll never add the same tuple twice to a graph\n",
    "    \"\"\"\n",
    "    for t in tuples:\n",
    "        assert len(t) == 3\n",
    "    [graph.add(t) for t in tuples if t not in graph]\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fdc557",
   "metadata": {},
   "source": [
    "### Prepare graph\n",
    "* Currently creating a single graph to hold all information. \n",
    "* Relevant information gathered from external resources is added; primarily class labels and definitions from WikiData."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54a1e1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "irec_graph = Graph()\n",
    "\n",
    "irec_graph.bind(\"root\", ROOT)\n",
    "irec_graph.bind(\"wikipedia\", WIKI)\n",
    "irec_graph.bind(\"uniclass\", UNICLASS)\n",
    "irec_graph.bind(\"dct\", DCT)\n",
    "irec_graph.bind(\"prov\", PROV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56acbeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bind our vocabulary of classes/relations\n",
    "graph_data_fp = Path.cwd().joinpath(\"data\", \"graph_data\")\n",
    "irec_graph.parse(graph_data_fp.joinpath(\"IREC.rdf\"))\n",
    "irec_graph.bind(\"spans\", SPANS)\n",
    "irec_graph.bind(\"concepts\", CONCEPTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ccd4a8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# primary sources and agents\n",
    "irec_IRI = URIRef(\"https://github.com/rubenkruiper/irec\")\n",
    "\n",
    "merged_approved_documents_IRI = URIRef(\"https://www.gov.uk/government/collections/approved-documents\")\n",
    "wikidata_IRI = URIRef(\"https://www.wikidata.org/\")\n",
    "uniclass_IRI = URIRef(\"https://en.wikipedia.org/wiki/Uniclass\")\n",
    "spart_txt_IRI = URIRef(\"http://dx.doi.org/10.18653/v1/2021.nllp-1.14\")\n",
    "\n",
    "irec_graph = add_tuples(irec_graph, \n",
    "                        [\n",
    "                            (irec_IRI, PROV.type, PROV.PrimarySource),\n",
    "                            (merged_approved_documents_IRI, PROV.type, PROV.PrimarySource),\n",
    "                            (wikidata_IRI, PROV.type, PROV.PrimarySource),\n",
    "                            (uniclass_IRI, PROV.type, PROV.PrimarySource),\n",
    "                            (spart_txt_IRI, PROV.type, PROV.SoftwareAgent)       \n",
    "                        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67b6fd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_scheme_uid(graph: Graph, primary_source: URIRef, scheme_name: str, scheme_uid_label:str, namespace: Namespace) -> Graph:\n",
    "    # We'll set the UID ourselves\n",
    "    scheme_uid = ua.keep_track_of_existing_UID(scheme_name, scheme_uid_label, namespace)\n",
    "    # add title\n",
    "    graph = add_tuples(graph, dct_title(scheme_uid, scheme_name, namespace)) \n",
    "    # add source note  \n",
    "    graph = add_tuples(graph, provenance(scheme_uid, primary_source, namespace)) \n",
    "    # is of type skos:ConceptScheme\n",
    "    graph = add_tuples(graph, skos_scheme(scheme_uid, SPANS))\n",
    "    # self-reference being in scheme\n",
    "    graph = add_tuples(graph, skos_in_scheme(scheme_uid, scheme_uid, namespace, namespace))\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cbb5024e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ua = UID_assigner()\n",
    "\n",
    "# global UIDs for the schemes we'll be using\n",
    "irec_graph = add_scheme_uid(irec_graph, irec_IRI, \"IREC spans\", \"schemeUID\", SPANS)\n",
    "irec_graph = add_scheme_uid(irec_graph, irec_IRI, \"IREC concepts\", \"schemeUID\", CONCEPTS)\n",
    "\n",
    "# irec_graph = add_scheme_uid(irec_graph, \"IREC WikiData concepts\", \"schemeUID\", WIKI) # NON EXISTENT NODE\n",
    "# irec_graph = add_scheme_uid(irec_graph, \"IREC Uniclass concepts\", \"schemeUID\", UNICLASS)  # NON EXISTENT NODE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fceef5",
   "metadata": {},
   "source": [
    "### Add base antonyms\n",
    "* We may want to get a sense of which spans are antonyms\n",
    "* For this we'll use NLTK's version of WordNet, which mainly captures antonyms for adjectives and adverbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7fe6de0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cold']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet_antonyms = {}\n",
    "for i in wn.all_synsets():\n",
    "    if i.pos() in ['a', 's']:    # If synset is adj or satelite-adj.\n",
    "        for j in i.lemmas():     # Iterating through lemmas for each synset.\n",
    "            if j.antonyms():     # If adj has antonym.\n",
    "                wordnet_antonyms[str(j.name()).strip()] = [x.name() for x in j.antonyms()]\n",
    "\n",
    "# Example of a useful antonym for us\n",
    "wordnet_antonyms['hot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de3d0ae4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hot']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet_antonyms['cold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a9692dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('acidic', ['alkaline', 'amphoteric']),\n",
       " ('alkaline', ['amphoteric', 'acidic']),\n",
       " ('amphoteric', ['acidic', 'alkaline']),\n",
       " ('air-to-surface', ['air-to-air', 'surface-to-air']),\n",
       " ('air-to-air', ['surface-to-air', 'air-to-surface']),\n",
       " ('surface-to-air', ['air-to-surface', 'air-to-air']),\n",
       " ('anadromous', ['catadromous', 'diadromous']),\n",
       " ('catadromous', ['diadromous', 'anadromous']),\n",
       " ('diadromous', ['anadromous', 'catadromous']),\n",
       " ('aquatic', ['terrestrial', 'amphibious']),\n",
       " ('terrestrial', ['amphibious', 'aquatic']),\n",
       " ('amphibious', ['aquatic', 'terrestrial']),\n",
       " ('prenatal', ['perinatal', 'postnatal']),\n",
       " ('perinatal', ['postnatal', 'prenatal']),\n",
       " ('postnatal', ['prenatal', 'perinatal']),\n",
       " ('sonic', ['subsonic', 'supersonic']),\n",
       " ('subsonic', ['supersonic', 'sonic']),\n",
       " ('supersonic', ['sonic', 'subsonic']),\n",
       " ('binucleate', ['trinucleate', 'mononuclear']),\n",
       " ('mononuclear', ['binucleate', 'trinucleate']),\n",
       " ('trinucleate', ['mononuclear', 'binucleate']),\n",
       " ('lower-class', ['middle-class', 'upper-class']),\n",
       " ('middle-class', ['upper-class', 'lower-class']),\n",
       " ('upper-class', ['lower-class', 'middle-class']),\n",
       " ('carnivorous', ['herbivorous', 'omnivorous', 'insectivorous']),\n",
       " ('herbivorous', ['omnivorous', 'insectivorous', 'carnivorous']),\n",
       " ('omnivorous', ['insectivorous', 'carnivorous', 'herbivorous']),\n",
       " ('insectivorous', ['carnivorous', 'herbivorous', 'omnivorous']),\n",
       " ('acatalectic', ['catalectic', 'hypercatalectic']),\n",
       " ('catalectic', ['hypercatalectic', 'acatalectic']),\n",
       " ('hypercatalectic', ['acatalectic', 'catalectic']),\n",
       " ('isotonic', ['hypertonic', 'hypotonic']),\n",
       " ('unconventional', ['conventional', 'conventional']),\n",
       " ('cubic', ['linear', 'planar']),\n",
       " ('planar', ['cubic', 'linear']),\n",
       " ('annual', ['biennial', 'perennial']),\n",
       " ('biennial', ['perennial', 'annual']),\n",
       " ('perennial', ['annual', 'biennial']),\n",
       " ('sharp', ['flat', 'natural']),\n",
       " ('early', ['middle', 'late']),\n",
       " ('middle', ['late', 'early']),\n",
       " ('late', ['early', 'middle']),\n",
       " ('ectomorphic', ['endomorphic', 'mesomorphic']),\n",
       " ('endomorphic', ['mesomorphic', 'ectomorphic']),\n",
       " ('mesomorphic', ['ectomorphic', 'endomorphic']),\n",
       " ('endogamous', ['exogamous', 'autogamous']),\n",
       " ('exogamous', ['autogamous', 'endogamous']),\n",
       " ('autogamous', ['endogamous', 'exogamous']),\n",
       " ('flat', ['natural', 'sharp']),\n",
       " ('nonspecific', ['specific', 'specific']),\n",
       " ('endemic', ['epidemic', 'ecdemic']),\n",
       " ('haploid', ['diploid', 'polyploid']),\n",
       " ('diploid', ['polyploid', 'haploid']),\n",
       " ('polyploid', ['haploid', 'diploid']),\n",
       " ('heterosexual', ['homosexual', 'bisexual']),\n",
       " ('homosexual', ['bisexual', 'heterosexual']),\n",
       " ('bisexual', ['heterosexual', 'homosexual']),\n",
       " ('homologous', ['analogous', 'heterologous']),\n",
       " ('heterologous', ['analogous', 'homologous']),\n",
       " ('autologous', ['homologous', 'heterologous']),\n",
       " ('analogous', ['homologous', 'heterologous']),\n",
       " ('horizontal', ['vertical', 'inclined']),\n",
       " ('vertical', ['inclined', 'horizontal']),\n",
       " ('vernal', ['summery', 'autumnal', 'wintry']),\n",
       " ('summery', ['autumnal', 'wintry', 'vernal']),\n",
       " ('autumnal', ['wintry', 'vernal', 'summery']),\n",
       " ('wintry', ['vernal', 'summery', 'autumnal']),\n",
       " ('introversive', ['extroversive', 'ambiversive']),\n",
       " ('extroversive', ['ambiversive', 'introversive']),\n",
       " ('ambiversive', ['introversive', 'extroversive']),\n",
       " ('leptorrhine', ['catarrhine', 'platyrrhine']),\n",
       " ('catarrhine', ['leptorrhine', 'platyrrhine']),\n",
       " ('platyrrhine', ['catarrhine', 'leptorrhine']),\n",
       " ('epidemic', ['endemic', 'ecdemic']),\n",
       " ('ecdemic', ['endemic', 'epidemic']),\n",
       " ('mini', ['midi', 'maxi']),\n",
       " ('midi', ['maxi', 'mini']),\n",
       " ('maxi', ['mini', 'midi']),\n",
       " ('male', ['female', 'androgynous']),\n",
       " ('female', ['androgynous', 'male']),\n",
       " ('androgynous', ['male', 'female']),\n",
       " ('masculine', ['feminine', 'neuter']),\n",
       " ('feminine', ['neuter', 'masculine']),\n",
       " ('neuter', ['masculine', 'feminine']),\n",
       " ('univalent', ['bivalent', 'multivalent']),\n",
       " ('bivalent', ['multivalent', 'univalent']),\n",
       " ('multivalent', ['univalent', 'bivalent']),\n",
       " ('natural', ['sharp', 'flat']),\n",
       " ('hypertensive', ['hypotensive', 'normotensive']),\n",
       " ('hypotensive', ['normotensive', 'hypertensive']),\n",
       " ('normotensive', ['hypertensive', 'hypotensive']),\n",
       " ('one-piece', ['two-piece', 'three-piece']),\n",
       " ('two-piece', ['three-piece', 'one-piece']),\n",
       " ('three-piece', ['one-piece', 'two-piece']),\n",
       " ('parallel', ['perpendicular', 'oblique']),\n",
       " ('oblique', ['parallel', 'perpendicular']),\n",
       " ('perpendicular', ['oblique', 'parallel']),\n",
       " ('past', ['present', 'future']),\n",
       " ('future', ['past', 'present']),\n",
       " ('neutral', ['positive', 'negative']),\n",
       " ('quantitative', ['syllabic', 'accentual']),\n",
       " ('right-handed', ['left-handed', 'ambidextrous']),\n",
       " ('left-handed', ['ambidextrous', 'right-handed']),\n",
       " ('ambidextrous', ['right-handed', 'left-handed']),\n",
       " ('center', ['right', 'left']),\n",
       " ('liquid', ['gaseous', 'solid']),\n",
       " ('gaseous', ['solid', 'liquid']),\n",
       " ('some', ['no', 'all']),\n",
       " ('no', ['all', 'some']),\n",
       " ('all', ['some', 'no']),\n",
       " ('syllabic', ['accentual', 'quantitative']),\n",
       " ('accentual', ['quantitative', 'syllabic']),\n",
       " ('superscript', ['subscript', 'adscript']),\n",
       " ('subscript', ['adscript', 'superscript']),\n",
       " ('adscript', ['superscript', 'subscript']),\n",
       " ('top', ['bottom', 'side']),\n",
       " ('bottom', ['side', 'top']),\n",
       " ('side', ['top', 'bottom']),\n",
       " ('surface', ['subsurface', 'overhead']),\n",
       " ('subsurface', ['overhead', 'surface']),\n",
       " ('overhead', ['surface', 'subsurface']),\n",
       " ('viviparous', ['oviparous', 'ovoviviparous']),\n",
       " ('oviparous', ['ovoviviparous', 'viviparous']),\n",
       " ('ovoviviparous', ['viviparous', 'oviparous']),\n",
       " ('xeric', ['hydric', 'mesic']),\n",
       " ('hydric', ['mesic', 'xeric']),\n",
       " ('mesic', ['xeric', 'hydric'])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# there are cases of multiple antonyms:\n",
    "[(k, wordnet_antonyms[k]) for k, v in wordnet_antonyms.items() if len(v) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "afd035fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_uid = URIRef(\"https://www.wikidata.org/wiki/Q533822\")\n",
    "for span in wordnet_antonyms.keys():\n",
    "    span_uid = ua.assign_UID(span, SPANS)\n",
    "    \n",
    "    irec_graph = add_tuples(irec_graph, irec_span(span_uid, span))\n",
    "    irec_graph = add_tuples(irec_graph, skos_in_scheme(span_uid, 'schemeUID', SPANS, SPANS))\n",
    "    irec_graph = add_tuples(irec_graph, provenance(span_uid, wordnet_uid, SPANS))\n",
    "\n",
    "    antonyms = wordnet_antonyms[span]\n",
    "    for antonym in antonyms:\n",
    "        antonym_uid = ua.assign_UID(antonym, SPANS)\n",
    "\n",
    "        irec_graph = add_tuples(irec_graph, irec_span(antonym_uid, antonym))\n",
    "        irec_graph = add_tuples(irec_graph, skos_in_scheme(antonym_uid, 'schemeUID', SPANS, SPANS))\n",
    "        irec_graph = add_tuples(irec_graph, provenance(antonym_uid, wordnet_uid, SPANS))\n",
    "\n",
    "        # add the antonym relation\n",
    "        irec_graph = add_tuples(irec_graph, irec_antonym(span_uid, antonym_uid))\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41f63e5",
   "metadata": {},
   "source": [
    "### Add domain terms extracted from the Approved documents as Spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8178ad1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_terms = pickle.load(open(graph_data_fp.joinpath('domain_terms.pkl'), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a7ac095d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For our graph, we'll remove determiners from the spans\n",
    "domain_terms = list(set([remove_determiners(t) for t in custom_cleaning_rules(domain_terms)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "07f4b0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique nr of domain terms: 7322\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['RATED INPUT UP TO',\n",
       " 'vertical projection',\n",
       " 'funfairs',\n",
       " 'purge ventilation rate',\n",
       " 'internal masonry walls',\n",
       " 'External surface materials',\n",
       " 'compressive strength',\n",
       " 'metres',\n",
       " 'service providers',\n",
       " 'sympathetic treatment']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"unique nr of domain terms:\", len(domain_terms))\n",
    "random.sample(domain_terms, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c506907d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simply adding the extracted spans\n",
    "for span in domain_terms:\n",
    "    span_uid = ua.assign_UID(span, SPANS)\n",
    "    \n",
    "    irec_graph = add_tuples(irec_graph, irec_span(span_uid, span))\n",
    "    irec_graph = add_tuples(irec_graph, skos_in_scheme(span_uid, 'schemeUID', SPANS, SPANS))\n",
    "    irec_graph = add_tuples(irec_graph, provenance(span_uid, merged_approved_documents_IRI, SPANS))\n",
    "    # add agent for spans generated by SPaR.txt\n",
    "    irec_graph = add_tuples(irec_graph, prov_agent(span_uid, spart_txt_IRI, SPANS))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8c4c91",
   "metadata": {},
   "source": [
    "### Add Acronyms that were grabbed from the text\n",
    "\n",
    "These can help:\n",
    "* remove terms where the boundary detection is off\n",
    "* avoid suggesting similar acronyms, e.g., suggest that EPC and EPS are similar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "434b8738",
   "metadata": {},
   "outputs": [],
   "source": [
    "acronyms = {'PAS': ['ecification', 'Specification'],  'GSIUR': ['Regulations 1998'],  'HSE': ['Regulations 2000',   'water systems',   'Safety Executive',   'Health and Safety Executive'],  'PE': ['Polyethylene', 'polyethylene'],  'DN': ['pipe'],  'DCLG': ['land', 'Local Government', 'England', 'ment'],  'PP': ['Polypropylene'],  'BCB': ['Control Body',   'the building control body',   'Building control body',   'building control body',   'Building Control Body'],  'SRHRV': ['ventilator',   'single room heat recovery ventilator',   'a single room heat recovery ventilator'],  'MVHR': ['blocks', 'heat recovery'],  'WC': ['sets'],  'TFA': ['the total floor area'],  'LRV': ['Light reflectance value'],  'BER': ['Building CO2 Emission Rate', 'CO2 Emission Rate'],  'TER': ['CO2 Emission Rate',   'the Target CO2 Emission Rate',   'Target CO2 Emission Rate'],  'DER': ['CO2 Emission Rate', 'the Dwelling CO2 Emission Rate'],  'EPC': ['energy performance certificate'],  'TFEE': ['Target Fabric Energy Efficiency',   'Fixed building services',   'Energy Efficiency'],  'DHF': ['the Door and Hardware Federation', 'Door and Hardware Federation'],  'REI': ['fire resistance', 'bility'],  'PHE': ['horizontal evacuation'],  'W': ['the final exit', 'final exit'],  'DWELLINGS': ['RESIDENTIAL'],  'OTHER': ['RESIDENTIAL'],  'TSO': ['Office', 'The Stationery Office'],  'FPA': ['the Fire Protection Association', 'Association'],  'A': ['absorption area'],  'AT': ['absorption area'],  'DECC': ['Climate Change'],  'NCM': ['the National Calculation Methodology'],  'ADCAS': ['Allied Services'],  'DFEE': ['Energy Efficiency'],  'LPA': ['the local planning authority', 'planning authority'],  'UKAS': ['the United Kingdom Accreditation Service'],  'BSI': ['the British Standards Institution'],  'EA': ['Accreditation'],  'BGS': ['British Geological Survey'],  'HBN': ['Notes'],  'GGF': ['Glazing Federation'],  'E': ['terms of integrity'],  'TRADA': ['the Timber Research and Development Association', 'Association'],  'ACOP': ['Code of Practice'],  'ATTMA': ['Association'],  'RVA': ['Association', 'the Residential Ventilation Association'],  'TEHVA': ['Association'],  'DSA': ['Association'],  'CIRIA': ['Association'],  'MCRMA': ['Association'],  'DSMA': ['Association'],  'OFTEC': ['Association'],  'WHO': ['Organisation'],  'GAI': ['Architectural Ironmongers'],  'MEV': ['mechanical extract', 'extract ventilation'],  'VST': ['Vicat softening temperature'],  'SCI': ['Guild Steel Construction Institute'],  'FBE': ['the Built Environment', 'ment'],  'DSER': ['Rating'],  'WER': ['Rating'],  'CIWM': ['ment', 'Wastes Management'],  'EOTA': ['ment'],  'GQRA': ['ment'],  'BRE': ['ment', 'the Building Research Establishment'],  'PPS': ['ment'],  'PSV': ['Passive stack ventilation'],  'EST': ['the Energy Saving Trust'],  'CIBSE': ['Ventilation hygiene toolkit', 'Building Services Engineers'],  'AGS': ['Geoenvironmental Specialists'],  'SPAB': ['Ancient Buildings'],  'UF': ['urea formaldehyde'],  'ODPM': ['the Deputy Prime Minister']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aaf33be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for acronym, spans in acronyms.items():\n",
    "    \n",
    "    acronym_uid = ua.assign_UID(acronym, SPANS)\n",
    "   \n",
    "    irec_graph = add_tuples(irec_graph, irec_span(acronym_uid, acronym))\n",
    "    irec_graph = add_tuples(irec_graph, skos_in_scheme(acronym_uid, 'schemeUID', SPANS, SPANS))\n",
    "    irec_graph = add_tuples(irec_graph, provenance(acronym_uid, merged_approved_documents_IRI, SPANS))\n",
    "    \n",
    "    for span in spans:\n",
    "        span_uid = ua.assign_UID(span, SPANS) \n",
    "        irec_graph = add_tuples(irec_graph, irec_span(span_uid, span))\n",
    "        irec_graph = add_tuples(irec_graph, skos_in_scheme(span_uid, 'schemeUID', SPANS, SPANS))\n",
    "        irec_graph = add_tuples(irec_graph, provenance(span_uid, merged_approved_documents_IRI, SPANS))\n",
    "\n",
    "        # These spans should have been \n",
    "        \n",
    "        # todo; \n",
    "        #  could do some filtering here of the clearly erroneous span-acronym combinations\n",
    "        #  or leave this until later, using the graph...\n",
    "    \n",
    "        irec_graph = add_tuples(irec_graph, irec_has_acronym(acronym_uid, span_uid))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953c8e57",
   "metadata": {},
   "source": [
    "### Add CONCEPTS: defined terms from the Approved Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1af2368f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from csv file\n",
    "definitions = pd.read_excel(graph_data_fp.joinpath(\"Approved Documents and derived terms.xlsx\"), sheet_name=\"Definitions\", keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "77530dc1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Term</th>\n",
       "      <th>Definition</th>\n",
       "      <th>Alternative labels</th>\n",
       "      <th>Note</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Absorption</td>\n",
       "      <td>Conversion of sound energy to heat, often by t...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Absorption coefficient</td>\n",
       "      <td>A quantity characterising the effectiveness of...</td>\n",
       "      <td></td>\n",
       "      <td>See BS EN 20354:1993.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Absorptive material</td>\n",
       "      <td>Material that absorbs sound energy.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Term                                         Definition  \\\n",
       "0              Absorption  Conversion of sound energy to heat, often by t...   \n",
       "1  Absorption coefficient  A quantity characterising the effectiveness of...   \n",
       "2     Absorptive material                Material that absorbs sound energy.   \n",
       "\n",
       "  Alternative labels                   Note  \n",
       "0                                            \n",
       "1                     See BS EN 20354:1993.  \n",
       "2                                            "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "definitions[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9d066368",
   "metadata": {},
   "outputs": [],
   "source": [
    "concepts_definitions_dict = {} # keep track of definitions for parsing later\n",
    "\n",
    "# create graph from definitions first\n",
    "for i, row in definitions.iloc[1:].iterrows():\n",
    "    # These terms all start with a capital; lowercase them\n",
    "    term = row['Term'].strip() if row['Term'].isupper() else row['Term'].lower().strip()\n",
    "    alternative_labels = row['Alternative labels']\n",
    "    definition = row['Definition']\n",
    "    note = row['Note']\n",
    "\n",
    "    # add the term as a CONCEPT and as a SPAN\n",
    "    concept_uid = ua.assign_UID(term, CONCEPTS)\n",
    "    irec_graph = add_tuples(irec_graph, skos_node(concept_uid, term))\n",
    "    irec_graph = add_tuples(irec_graph, skos_in_scheme(concept_uid, 'schemeUID', CONCEPTS, CONCEPTS))\n",
    "    irec_graph = add_tuples(irec_graph, provenance(concept_uid, merged_approved_documents_IRI, CONCEPTS))\n",
    "    \n",
    "    span_uid = ua.assign_UID(term, SPANS)\n",
    "    irec_graph = add_tuples(irec_graph, irec_span(span_uid, term))\n",
    "    irec_graph = add_tuples(irec_graph, skos_in_scheme(span_uid, 'schemeUID', SPANS, SPANS))\n",
    "    irec_graph = add_tuples(irec_graph, provenance(span_uid, merged_approved_documents_IRI, SPANS))\n",
    "    \n",
    "    # link the concept and the span # as a skos:exactMatch? or smt else?\n",
    "    irec_graph = add_tuples(irec_graph, skos_exact_match(concept_uid, span_uid, CONCEPTS, SPANS))\n",
    "    \n",
    "    # always expecting a definition\n",
    "    irec_graph = add_tuples(irec_graph, skos_definition(concept_uid, definition))\n",
    "    \n",
    "    if note: \n",
    "        irec_graph = add_tuples(irec_graph, skos_note(concept_uid, note))\n",
    "    \n",
    "    if alternative_labels:\n",
    "        # These terms all start with a capital; lowercase if not an acronym\n",
    "        alt_labels = [x.strip() if x.isupper() else x.lower().strip() for x in alternative_labels.split(\", \")]  \n",
    "        \n",
    "        for alt_label in alt_labels:\n",
    "            if not alt_label:\n",
    "                continue\n",
    "            # add the altlabel to the concept node\n",
    "            alt_label_concept_uid = ua.assign_UID(alt_label, CONCEPTS)\n",
    "            irec_graph = add_tuples(irec_graph, skos_node(alt_label_concept_uid, alt_label))\n",
    "            irec_graph = add_tuples(irec_graph, skos_in_scheme(alt_label_concept_uid, 'schemeUID', CONCEPTS, CONCEPTS))\n",
    "            irec_graph = add_tuples(irec_graph, provenance(alt_label_concept_uid, merged_approved_documents_IRI, CONCEPTS))\n",
    "\n",
    "            irec_graph = add_tuples(irec_graph, skos_altLabel(concept_uid, alt_label_concept_uid))\n",
    "            \n",
    "            # also add as a span\n",
    "            alt_label_span_uid = ua.assign_UID(alt_label, SPANS)\n",
    "            irec_graph = add_tuples(irec_graph, irec_span(alt_label_span_uid, alt_label))\n",
    "            irec_graph = add_tuples(irec_graph, skos_in_scheme(alt_label_span_uid, 'schemeUID', SPANS, SPANS))\n",
    "            irec_graph = add_tuples(irec_graph, provenance(alt_label_span_uid, merged_approved_documents_IRI, SPANS))\n",
    "            \n",
    "            # link the altlabel concept and the altlabel span\n",
    "            irec_graph = add_tuples(irec_graph, skos_exact_match(alt_label_concept_uid, alt_label_span_uid, CONCEPTS, SPANS))\n",
    "    \n",
    "    if concept_uid not in concepts_definitions_dict: \n",
    "        concepts_definitions_dict[concept_uid] = [{'prefLabel': term, 'definition': definition, 'note': note}]  \n",
    "    else:\n",
    "        concepts_definitions_dict[concept_uid].append({'prefLabel': term, 'definition': definition, 'note': note})  \n",
    "                                                      \n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86a2f6c",
   "metadata": {},
   "source": [
    "### Add SPANS: glossary/index terms from the Approved Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "92cbdde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_terms = pd.read_excel(graph_data_fp.joinpath(\"Approved Documents and derived terms.xlsx\"), sheet_name=\"Index terms\", keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "42ddb6c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Term</th>\n",
       "      <th>AltLabel(s)</th>\n",
       "      <th>Related terms</th>\n",
       "      <th>Broader term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abbreviated eaves</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>eaves</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Access floors</td>\n",
       "      <td>access floor</td>\n",
       "      <td>Platform floors</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Access for fire service</td>\n",
       "      <td>fire access</td>\n",
       "      <td></td>\n",
       "      <td>Fire service facilities</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Term   AltLabel(s)    Related terms  \\\n",
       "0        abbreviated eaves                                  \n",
       "1            Access floors  access floor  Platform floors   \n",
       "2  Access for fire service   fire access                    \n",
       "\n",
       "              Broader term  \n",
       "0                    eaves  \n",
       "1                           \n",
       "2  Fire service facilities  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_terms[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843da834",
   "metadata": {},
   "source": [
    "* add triples from index terms / glossaries; we will treat these terms as SPANS\n",
    "* some of these terms were added manually on top of the index terms found in the Mergeds Approved documents, so we'll avoid adding the provenance relation to these\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4f1358bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total index terms found in spreadsheet:  2363\n"
     ]
    }
   ],
   "source": [
    "total_num_index_terms = 0\n",
    "for i, row in index_terms.iloc[1:].iterrows():\n",
    "    # Many of these terms start with a capital; lowercase them\n",
    "    term = row['Term'].strip() if row['Term'].isupper() else row['Term'].lower().strip()\n",
    "    alternative_labels = row['AltLabel(s)']\n",
    "    related_terms = row['Related terms']\n",
    "    broader_term = row['Broader term']\n",
    "    \n",
    "    # add the term as a SPAN only\n",
    "    span_uid = ua.assign_UID(term, SPANS)\n",
    "    irec_graph = add_tuples(irec_graph, irec_span(span_uid, term))\n",
    "    irec_graph = add_tuples(irec_graph, skos_in_scheme(span_uid, 'schemeUID', SPANS, SPANS))\n",
    "#     irec_graph = add_tuples(irec_graph, provenance(span_uid, merged_approved_documents_IRI, SPANS))\n",
    "    total_num_index_terms += 1\n",
    "        \n",
    "    if alternative_labels:\n",
    "        # Many of these terms start with a capital; lowercase them\n",
    "        alt_labels = [x.strip() if x.isupper() else x.lower().strip() for x in alternative_labels.split(\", \")]  \n",
    "        \n",
    "        for alt_label in alt_labels:\n",
    "            if not alt_label:\n",
    "                continue\n",
    "            # add alt-label as a span only (as well)\n",
    "            alt_label_uid = ua.assign_UID(alt_label, SPANS)\n",
    "            irec_graph = add_tuples(irec_graph, irec_span(alt_label_uid, alt_label))\n",
    "            irec_graph = add_tuples(irec_graph, skos_in_scheme(alt_label_uid, 'schemeUID', SPANS, SPANS))\n",
    "#             irec_graph = add_tuples(irec_graph, provenance(alt_label_uid, merged_approved_documents_IRI, SPANS))\n",
    "            total_num_index_terms += 1\n",
    "            \n",
    "            if alt_label.isupper():\n",
    "                # there are acronyms among the alternative labels\n",
    "                irec_graph = add_tuples(irec_graph, irec_has_acronym(span_uid, alt_label_uid))\n",
    "            else:\n",
    "                ### Should I use skos altlabels between spans? maybe create IREC alternative label?\n",
    "                ### Should I use skos altlabels between spans? maybe create IREC alternative label?\n",
    "                ### Should I use skos altlabels between spans? maybe create IREC alternative label?\n",
    "                irec_graph = add_tuples(irec_graph, skos_altLabel(span_uid, alt_label_uid))\n",
    "                \n",
    "\n",
    "    if related_terms:\n",
    "        # Many of these terms start with a capital; lowercase them\n",
    "        rel_terms = [x.strip() if x.isupper() else x.lower().strip() for x in related_terms.split(\", \")]\n",
    "        for rel_term in rel_terms:\n",
    "            if not rel_term:\n",
    "                continue\n",
    "            # add related terms as a span (as well)\n",
    "            related_uid = ua.assign_UID(rel_term, SPANS)\n",
    "            irec_graph = add_tuples(irec_graph, irec_span(related_uid, rel_term))\n",
    "            irec_graph = add_tuples(irec_graph, skos_in_scheme(related_uid, 'schemeUID', SPANS, SPANS))\n",
    "#             irec_graph = add_tuples(irec_graph, provenance(related_uid, merged_approved_documents_IRI, SPANS))\n",
    "            total_num_index_terms += 1\n",
    "            \n",
    "            if rel_term.isupper():\n",
    "                # there are acronyms among the related labels as well\n",
    "                irec_graph = add_tuples(irec_graph, irec_has_acronym(span_uid, related_uid))\n",
    "            else:\n",
    "                irec_graph = add_tuples(irec_graph, irec_related(span_uid, related_uid)) \n",
    "    \n",
    "    if broader_term:\n",
    "        # We do not expect that the broader term is necessarily a concept.\n",
    "        # Currently, it is simply a feature for future reference.\n",
    "        # We expect 1 broader term at most, assuming the final conceptualisation would\n",
    "        # be structured like a tree (Directed Acyclic Graph with 1 parent at most).\n",
    "        b_term = broader_term.strip().lower() if not broader_term.isupper() else broader_term.strip()\n",
    "        # also broader term as a span\n",
    "        b_term_uid = ua.assign_UID(b_term, SPANS)\n",
    "        irec_graph = add_tuples(irec_graph, irec_span(b_term_uid, b_term)) \n",
    "        irec_graph = add_tuples(irec_graph, skos_in_scheme(b_term_uid, 'schemeUID', SPANS, SPANS))\n",
    "#         irec_graph = add_tuples(irec_graph, provenance(b_term_uid, merged_approved_documents_IRI, SPANS))\n",
    "        total_num_index_terms += 1\n",
    "        \n",
    "        ### Should I use skos broader between spans? maybe create an IREC broader?\n",
    "        irec_graph = add_tuples(irec_graph, skos_broader(span_uid, b_term_uid, SPANS, SPANS)) \n",
    "\n",
    "print(\"Total index terms found in spreadsheet: \", total_num_index_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "40952822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Graph identifier=N16319baf899a4b57ae3b416418100356 (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irec_graph.serialize(destination=graph_output_fp.joinpath(\"approved_doc_terms_only.ttl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fbf590",
   "metadata": {},
   "source": [
    "### Print some insight in the graph so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "14daa5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes in 'https://spans.irec.org/': 11871\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11871"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ua.count_nodes_in_namespace(SPANS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "204cfd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes in 'https://concepts.irec.org/': 350\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "350"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ua.count_nodes_in_namespace(CONCEPTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bdfeb278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272 ; type ; Concept\n",
      "272 ; prefLabel ; sanitary accommodation\n",
      "272 ; inScheme ; schemeUID\n",
      "272 ; hadPrimarySource ; https://www.gov.uk/government/collections/approved-documents\n",
      "272 ; exactMatch ; sanitary%20accommodation\n",
      "272 ; definition ; A space containing one or more water closets or urinals, whether or not it also contains other sanitary appliances. Sanitary accommodation containing one or  more cubicles counts as a single space if there is free circulation of air throughout the space.\n"
     ]
    }
   ],
   "source": [
    "ua.print_node_by_id(irec_graph, 272, CONCEPTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e6ad1b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sanitary%20accommodation ; type ; CharacterSpan\n",
      "sanitary%20accommodation ; label ; sanitary accommodation\n",
      "sanitary%20accommodation ; inScheme ; schemeUID\n",
      "sanitary%20accommodation ; hadPrimarySource ; https://www.gov.uk/government/collections/approved-documents\n",
      "sanitary%20accommodation ; wasAttributedTo ; http://dx.doi.org/10.18653/v1/2021.nllp-1.14\n",
      "sanitary%20accommodation ; related ; sanitary%20appliance\n"
     ]
    }
   ],
   "source": [
    "ua.print_node_by_text(irec_graph, 'sanitary accommodation', SPANS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4172c596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wet%20room ; type ; CharacterSpan\n",
      "wet%20room ; label ; wet room\n",
      "wet%20room ; inScheme ; schemeUID\n",
      "wet%20room ; hadPrimarySource ; https://www.gov.uk/government/collections/approved-documents\n",
      "wet%20room ; wasAttributedTo ; http://dx.doi.org/10.18653/v1/2021.nllp-1.14\n"
     ]
    }
   ],
   "source": [
    "ua.print_node_by_id(irec_graph, urllib.parse.quote('wet room'), SPANS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1ddcb895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sanitary%20appliance ; type ; CharacterSpan\n",
      "sanitary%20appliance ; label ; sanitary appliance\n",
      "sanitary%20appliance ; inScheme ; schemeUID\n",
      "sanitary%20appliance ; hadPrimarySource ; https://www.gov.uk/government/collections/approved-documents\n",
      "sanitary%20appliance ; wasAttributedTo ; http://dx.doi.org/10.18653/v1/2021.nllp-1.14\n",
      "sanitary%20appliance ; broader ; bath\n",
      "sanitary%20appliance ; broader ; shower\n",
      "sanitary%20appliance ; broader ; sink\n",
      "sanitary%20appliance ; broader ; toilet\n",
      "sanitary%20appliance ; related ; sanitary%20accommodation\n",
      "sanitary%20appliance ; related ; toilet\n",
      "sanitary%20appliance ; related ; urinal\n",
      "sanitary%20appliance ; related ; sink\n",
      "sanitary%20appliance ; related ; shower\n",
      "sanitary%20appliance ; related ; bathtub\n",
      "sanitary%20appliance ; related ; bidet\n",
      "sanitary%20appliance ; related ; wastewater\n",
      "sanitary%20appliance ; related ; washbasin\n"
     ]
    }
   ],
   "source": [
    "ua.print_node_by_text(irec_graph, 'sanitary appliance', SPANS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1989fb60",
   "metadata": {},
   "source": [
    "As you can see in the examples above, the concept `wet room` and the span `sanitary accomdodation` are related:\n",
    "* The concept `wet room` is provided with a note in the merged approved documents.\n",
    "* The text inside this node describes how, for part F of the approved documents, `sanitary accomodation` is regarded as a `wet room`. \n",
    "\n",
    "Based on the above, we'd like to link the span `sanitary accomodation` to the concept `wet room`. While we could parse the note in more detail, and identify that a `skos:altLabel` relation should be added, we'll use a more generic approach:\n",
    "* Any span that is found inside a definition or note of a concept will be linked through `irec:related`\n",
    "* Based on the definitions above, potential spans related to the `wet room` concept then become: `sanitary accomdoation`, `airborn moisture`, `kitchen`, `utility room`, `bathroom`, `WC`, `tanking`, `drainage`, `gulley`, `shower`.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "We believe that the types of relations described above can be valuable and would like to provide more definitions for more terms, to help interrelate more spans and concepts. To this end, we first try to find WikiData definitions for all concepts and spans. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407358e6",
   "metadata": {},
   "source": [
    "### Grab wikipedia definitions for Concept nodes, and store locally for re-use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05db9542",
   "metadata": {},
   "source": [
    "* First, we try to grab all wiki definitions for all spans and concepts that are in the graph (so far)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f2e41964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the SPARQL endpoint for wikidata\n",
    "sparql_wrapper = SPARQLWrapper(\"https://query.wikidata.org/sparql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "12e9e280",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wiki_matches(graph_sparql_endpoint: SPARQLWrapper,\n",
    "                     jargon_term_and_uids: List):\n",
    "\n",
    "    all_wiki_definitions = {}\n",
    "    # we want to grab the term (subject), any definition (subjectDescription) and the class (subjectClass)\n",
    "    sparql_q = \"\"\"\n",
    "               SELECT DISTINCT ?subject ?subjectDescription ?classUID ?className WHERE {\n",
    "                  ?subject rdfs:label \"QUERY\"@en.\n",
    "                  ?subject wdt:P31|wdt:P279 ?classUID.\n",
    "                  SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\".}\n",
    "                  ?classUID  rdfs:label ?className  FILTER(LANG(?className) = \"en\").\n",
    "                }\n",
    "               \"\"\"\n",
    "    \n",
    "    for term, uid in tqdm(jargon_term_and_uids):\n",
    "        # make the call to \n",
    "        temp_q = sparql_q.replace(\"QUERY\", term)\n",
    "        graph_sparql_endpoint.setQuery(temp_q)\n",
    "        graph_sparql_endpoint.setReturnFormat(JSON)\n",
    "        try:\n",
    "            json_output = graph_sparql_endpoint.query().convert()\n",
    "        except:\n",
    "            # If no result, wait 2s; One client is allowed 30 error queries per minute\n",
    "            print(f\"Error for query, may want to check what's wrong with the term: {term}\")\n",
    "            time.sleep(2)\n",
    "            continue\n",
    "            \n",
    "        # sometimes multiple Wiki UIDs for a single term, we grab them all here\n",
    "        bindings = [v for v in json_output['results']['bindings']]\n",
    "            \n",
    "\n",
    "        for v in bindings:\n",
    "            class_uid = v['classUID']['value'] if 'classUID' in v else \"\"\n",
    "            class_label = v['className']['value'] if 'className' in v else \"\"\n",
    "            \n",
    "            if 'subjectDescription' in v:\n",
    "                if uid not in all_wiki_definitions:\n",
    "                    all_wiki_definitions[uid] = [{'prefLabel': term,\n",
    "                                                  'class_uid': class_uid,\n",
    "                                                  'class_label': class_label,\n",
    "                                                  'WikiUID': v['subject']['value'],\n",
    "                                                  'WikiDefinition': v['subjectDescription']['value']}]\n",
    "                else:\n",
    "                    all_wiki_definitions[uid].append({'prefLabel': term,\n",
    "                                                      'class_uid': class_uid,\n",
    "                                                      'class_label': class_label,\n",
    "                                                      'WikiUID': v['subject']['value'],\n",
    "                                                      'WikiDefinition': v['subjectDescription']['value']})\n",
    "            elif 'subject' in v:\n",
    "                # no description found, simply adding wiki UID if that exists\n",
    "                if uid not in all_wiki_definitions:\n",
    "                    all_wiki_definitions[uid] = [{'prefLabel': term,\n",
    "                                                  'class_uid': class_uid,\n",
    "                                                  'class_label': class_label,\n",
    "                                                  'WikiUID': v['subject']['value']}]\n",
    "                else:\n",
    "                    all_wiki_definitions[uid].append({'prefLabel': term,\n",
    "                                                      'class_uid': class_uid,\n",
    "                                                      'class_label': class_label,\n",
    "                                                      'WikiUID': v['subject']['value']})\n",
    "    return all_wiki_definitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6b57bf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "concepts_and_uids = [(k, v) for k, v in ua.UIDs[CONCEPTS.placeholder.defrag().__reduce__()[1][0]].items()]\n",
    "spans_and_uids = [(k, v) for k, v in ua.UIDs[SPANS.placeholder.defrag().__reduce__()[1][0]].items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3f9b3c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First run for the Concepts\n",
    "concept_wiki_dict_fp = graph_output_fp.joinpath(\"concept_wiki_dict.json\")\n",
    "if not concept_wiki_dict_fp.exists():\n",
    "    concept_wiki_dict = get_wiki_matches(sparql_wrapper, concepts_and_uids)\n",
    "    with open(concept_wiki_dict_fp, 'w') as f:\n",
    "        json.dump(concept_wiki_dict, f, indent=2)\n",
    "else:\n",
    "    with open(concept_wiki_dict_fp, 'r') as f:\n",
    "        concept_wiki_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1b368865",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now run for the spans (those in the graph at this moment)\n",
    "span_wiki_dict_fp = graph_output_fp.joinpath(\"span_wiki_dict.json\")\n",
    "if not span_wiki_dict_fp.exists():\n",
    "    span_wiki_dict = get_wiki_matches(sparql_wrapper, spans_and_uids)\n",
    "    with open(span_wiki_dict_fp, 'w') as f:\n",
    "        json.dump(span_wiki_dict, f, indent=2)\n",
    "else:\n",
    "    with open(span_wiki_dict_fp, 'r') as f:\n",
    "        span_wiki_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "10f67b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes in 'https://concepts.irec.org/': 350\n",
      "Number of concepts with WikiData definitions: 70 (20.00%)\n",
      "Number of nodes in 'https://spans.irec.org/': 11871\n",
      "Number of spans with WikiData definitions: 833 (7.02%)\n"
     ]
    }
   ],
   "source": [
    "# iets gaat er mis 70/833\n",
    "print(\"Number of concepts with WikiData definitions: {} ({:.2f}%)\".format(len(concept_wiki_dict), len(concept_wiki_dict)/ua.count_nodes_in_namespace(CONCEPTS)*100))\n",
    "print(\"Number of spans with WikiData definitions: {} ({:.2f}%)\".format(len(span_wiki_dict), len(span_wiki_dict)/ua.count_nodes_in_namespace(SPANS)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699c6acb",
   "metadata": {},
   "source": [
    "### Only keep WikiData definitions that belong to classes that we've annotated\n",
    "* We have previously annotated the relevance of all WikiData classes returned for the defined terms and index terms in the Approved Documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c5aa0969",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_wikidata_classes_df = pd.read_csv(graph_data_fp.joinpath(\"wiki_classes_annotated.csv\"), index_col=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c66f2893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WikiData class</th>\n",
       "      <th>Annotation</th>\n",
       "      <th>Example spans</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WikiData UIDs</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>['Q107715']</th>\n",
       "      <td>physical quantity</td>\n",
       "      <td>y</td>\n",
       "      <td>['sound pressure level', 'density', 'area', 's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>['Q82799']</th>\n",
       "      <td>name</td>\n",
       "      <td>n</td>\n",
       "      <td>['access point']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>['Q180160']</th>\n",
       "      <td>metadata</td>\n",
       "      <td>n</td>\n",
       "      <td>['access point']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  WikiData class Annotation  \\\n",
       "WikiData UIDs                                 \n",
       "['Q107715']    physical quantity          y   \n",
       "['Q82799']                  name          n   \n",
       "['Q180160']             metadata          n   \n",
       "\n",
       "                                                   Example spans  \n",
       "WikiData UIDs                                                     \n",
       "['Q107715']    ['sound pressure level', 'density', 'area', 's...  \n",
       "['Q82799']                                      ['access point']  \n",
       "['Q180160']                                     ['access point']  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotated_wikidata_classes_df[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1f587141",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikiclass_dict = {}\n",
    "for row in annotated_wikidata_classes_df.iterrows():\n",
    "    uid_list_string, class_annotations_examples = row\n",
    "    uid_list = uid_list_string[2:-2].split(',')\n",
    "    for uid in uid_list:\n",
    "        wikiclass_dict[uid] = {\n",
    "            'Class': class_annotations_examples['WikiData class'],\n",
    "            'Annotation': class_annotations_examples['Annotation'],\n",
    "            'Example spans': class_annotations_examples['Example spans']\n",
    "        }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dde9e8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_wikidata_classes(wiki_class_dict, term_dict):\n",
    "    new_term_dict = {}\n",
    "    removed_definitions = []\n",
    "    for uid, definition_dict_list in term_dict.items():\n",
    "        for definition_dict in definition_dict_list:\n",
    "            class_uid = definition_dict['class_uid'].rsplit(\"/\", 1)[1]\n",
    "            if class_uid in wikiclass_dict:\n",
    "                class_name = wikiclass_dict[class_uid][\"Class\"]\n",
    "                if wikiclass_dict[class_uid][\"Annotation\"] == 'y':\n",
    "                    if uid not in new_term_dict:\n",
    "                        new_term_dict[uid] = [definition_dict]\n",
    "                    else:\n",
    "                        new_term_dict[uid].append(definition_dict)\n",
    "                else:\n",
    "                    removed_definitions.append(definition_dict)\n",
    "                        \n",
    "    return new_term_dict, removed_definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e49d20d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_wiki_dict, removed_definitions = filter_wikidata_classes(wikiclass_dict, concept_wiki_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "78aefda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "span_wiki_dict, removed_definitions = filter_wikidata_classes(wikiclass_dict, span_wiki_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fae49f",
   "metadata": {},
   "source": [
    "### Parse all definitions (including WikiData) to identify additional spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bfdd2f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_spar_labels(input_dict: Dict[str, str], term_extractor: TermExtractor):\n",
    "    \"\"\"\n",
    "    Identify which spans occur in the definitions\n",
    "    \"\"\"\n",
    "    number_of_definitions = 0\n",
    "    for uid, definition_dict_list in tqdm(input_dict.items()):\n",
    "        for idx, definition_dict in enumerate(definition_dict_list):\n",
    "#             if 'Spans in definitions and notes' in definition_dict:\n",
    "#                 # spans already computed for this definition_dict, continuing to check next\n",
    "#                 continue\n",
    "            \n",
    "            spartxt_objects = []\n",
    "            for k, v in definition_dict.items():\n",
    "                if k in ['WikiDefinition', 'definition', 'note'] and v != '':\n",
    "                    to_be_parsed = definition_dict[k]\n",
    "                    number_of_definitions += 1\n",
    "                    sentences = term_extractor.split_into_sentences(to_be_parsed)\n",
    "                    # cleaning spans as well;\n",
    "                    sentences = [remove_unicode_chars(s).encode(\"ascii\", \"ignore\").decode() for s in sentences]\n",
    "                    spartxt_objects += custom_cleaning_rules(term_extractor.process_sentences(sentences))\n",
    "                    \n",
    "            input_dict[uid][idx]['Spans in definitions and notes'] = spartxt_objects\n",
    "    print(f\"Processed {number_of_definitions} definitions\")\n",
    "    return input_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdb4013",
   "metadata": {},
   "source": [
    "* Parse the definitions of concepts manually taken from the Approved Documents (not all concepts are defined, e.g., altLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "46bf46cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading previously computed concepts_definitions_dict with SPaR.txt objects from file\n"
     ]
    }
   ],
   "source": [
    "concepts_definitions_dict_fp = graph_output_fp.joinpath(\"concepts_definitions_dict.json\")\n",
    "termextractor = None\n",
    "if not concepts_definitions_dict_fp.exists():\n",
    "    if not termextractor:\n",
    "        # instantiate a TermExtractor\n",
    "        termextractor = TermExtractor(max_num_cpu_threads=4)\n",
    "    \n",
    "    print(\"Computing SPaR.txt objects for concepts_definitions_dict\")\n",
    "    concepts_definitions_dict = add_spar_labels(concepts_definitions_dict, termextractor)\n",
    "    with open(concepts_definitions_dict_fp, 'w') as f:\n",
    "        json.dump(concepts_definitions_dict, f, indent=2)\n",
    "else:\n",
    "    print(\"Loading previously computed concepts_definitions_dict with SPaR.txt objects from file\")\n",
    "    with open(concepts_definitions_dict_fp, 'r') as f:\n",
    "        concepts_definitions_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bdd48f",
   "metadata": {},
   "source": [
    "* Parse the definitions of concepts that were found in WikiData (the file already exists, need to check if spar_labels exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6746b48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "has_spar_labels = 'Spans in definitions and notes' in concept_wiki_dict[list(concept_wiki_dict.keys())[0]][0]\n",
    "if not has_spar_labels:\n",
    "    if not termextractor:\n",
    "        # instantiate a TermExtractor obj \n",
    "        termextractor = TermExtractor(max_num_cpu_threads=4)\n",
    "    concept_wiki_dict = add_spar_labels(concept_wiki_dict, termextractor)\n",
    "    # Save the updated concept_wiki_dict, will be loaded in previous cells anyway\n",
    "    with open(concept_wiki_dict_fp, 'w') as f:\n",
    "        json.dump(concept_wiki_dict, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ae7de0d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "has_spar_labels = 'Spans in definitions and notes' in span_wiki_dict[list(span_wiki_dict.keys())[0]][0]\n",
    "if not has_spar_labels:\n",
    "    if not termextractor:\n",
    "        # instantiate a TermExtractor obj\n",
    "        termextractor = TermExtractor(max_num_cpu_threads=4)\n",
    "    span_wiki_dict = add_spar_labels(span_wiki_dict, termextractor)\n",
    "    # Save the updated span_wiki_dict, which will be loaded in previous cells anyway\n",
    "    with open(span_wiki_dict_fp, 'w') as f:\n",
    "        json.dump(span_wiki_dict, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4dec3a",
   "metadata": {},
   "source": [
    "* Some examples of/insight in definitions from different sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0dfa4268",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'prefLabel': 'absorption coefficient',\n",
       "  'definition': 'A quantity characterising the effectiveness of a sound absorbing surface. The proportion of sound energy absorbed is given as a number between zero (for a fully reflective surface) and one (for a fully absorptive surface). Note that sound absorption coefficients determined from laboratory measurements may have values slightly larger than one.',\n",
       "  'note': 'See BS EN 20354:1993.',\n",
       "  'Spans in definitions and notes': ['sound energy',\n",
       "   'zero',\n",
       "   'a fully reflective surface',\n",
       "   'values',\n",
       "   'a number',\n",
       "   'The proportion',\n",
       "   'laboratory',\n",
       "   'abs surface',\n",
       "   'sound absorption coefficients',\n",
       "   'the effectiveness surface',\n",
       "   'A quantity character',\n",
       "   'BS EN 20354 : 1993']}]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of definitions from approved documents\n",
    "concepts_definitions_dict['1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e746463c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'prefLabel': 'absorption coefficient',\n",
       "  'class_uid': 'http://www.wikidata.org/entity/Q107715',\n",
       "  'class_label': 'physical quantity',\n",
       "  'WikiUID': 'http://www.wikidata.org/entity/Q97368968',\n",
       "  'WikiDefinition': 'measure for the exponential reduction of a quantity along a path due to absorption',\n",
       "  'Spans in definitions and notes': ['a quantity',\n",
       "   'a path due',\n",
       "   'the exponential reduction']}]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of definitions for the same concept, from WikiData\n",
    "concept_wiki_dict['1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2971f4f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['absorbent', 'absorption%20coefficient']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'prefLabel': 'absorbent',\n",
       "  'class_uid': 'http://www.wikidata.org/entity/Q3505845',\n",
       "  'class_label': 'state',\n",
       "  'WikiUID': 'http://www.wikidata.org/entity/Q110147344',\n",
       "  'WikiDefinition': 'having the ability or tendency to absorb; able to soak up liquid easily; absorptive.',\n",
       "  'Spans in definitions and notes': ['the ability',\n",
       "   'absorptive',\n",
       "   'tendency',\n",
       "   'liquid']}]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of definitions for a related span, from WikiData\n",
    "print([k for k in span_wiki_dict.keys() if 'absor' in k])\n",
    "span_wiki_dict['absorbent']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d77b070",
   "metadata": {},
   "source": [
    "### Add spans from the definitions to the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb8c45a",
   "metadata": {},
   "source": [
    "* First, we'd like to get some insight in the number of new spans identified through parsing the definitions. We will count:\n",
    "  * the total number of defined terms\n",
    "  * total number of definitions (a defined term may have multiple definitions)\n",
    "  * the number of spans found in all of these definitions (total, and new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ee7f6d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_spans(some_dict: Dict[str, str], primary_source: URIRef):\n",
    "    primary_source_str = primary_source.__reduce__()[1][0]\n",
    "    spartxt_objects_in_dict = {primary_source_str: {}}\n",
    "    \n",
    "    definition_cntr = Counter()\n",
    "    total_num_definitions = 0\n",
    "    for definition_dict_list in some_dict.values():\n",
    "        defined_term = definition_dict_list[0]['prefLabel']\n",
    "        \n",
    "        # count definitions (unique)\n",
    "        defs = list(set([dv for d in definition_dict_list for dk, dv in d.items() if (dk in ['WikiDefinition', 'definition', 'note'] and dv != '')]))\n",
    "        total_num_definitions += len(defs)\n",
    "        definition_cntr[defined_term] += len(defs)\n",
    "        \n",
    "        # collect spans (not unique, but cleaned)\n",
    "        span_lists = [dv for d in definition_dict_list for dk, dv in d.items() if (dk == 'Spans in definitions and notes' and dv != [])]\n",
    "        spans = [remove_determiners(x) for x in custom_cleaning_rules([s for sl in span_lists for s in sl])] \n",
    "        \n",
    "        if defined_term not in spartxt_objects_in_dict[primary_source_str]:\n",
    "            spartxt_objects_in_dict[primary_source_str][defined_term] = spans\n",
    "        else:\n",
    "            spartxt_objects_in_dict[primary_source_str][defined_term] += spans\n",
    "                \n",
    "    return spartxt_objects_in_dict, definition_cntr, total_num_definitions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "66bbed8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 334 + 547 + 1329 ~ expecting 2210 definitions total\n",
    "# most often defined term house ~ 61 definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0ff7e865",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_spartxt_objects = {}\n",
    "total_num_definitions = 0\n",
    "per_term_definition_counts = Counter()\n",
    "for some_dict, source in zip([concepts_definitions_dict, concept_wiki_dict, span_wiki_dict],\n",
    "                             [merged_approved_documents_IRI, wikidata_IRI, wikidata_IRI]):\n",
    "    spans_found_in_definitions, definition_counter, nr_definitions = count_spans(some_dict, source)\n",
    "    all_spartxt_objects.update(spans_found_in_definitions)\n",
    "    per_term_definition_counts.update(definition_counter)\n",
    "    total_num_definitions += nr_definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5a462180",
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_or_span_with_definition = []\n",
    "spans_found_in_definitions = []\n",
    "for source, defined_term_dict in all_spartxt_objects.items():\n",
    "    concept_or_span_with_definition += [k for k in defined_term_dict.keys()]\n",
    "    spans_found_in_definitions += [s for v in defined_term_dict.values() for s in v]\n",
    "        \n",
    "unique_new_spans = [x for x in list(set(spans_found_in_definitions)) if x not in domain_terms]\n",
    "terms_with_multiple_defs = len([k for k, v in per_term_definition_counts.items() if v > 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4f39750a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of defined terms:  1128\n",
      "Number of defined terms with more than 1 definition: 242 (21.45) \n",
      "Number of definitions/notes found: 1500\n",
      "Top 10 defined terms with most definitions:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('house', 61),\n",
       " ('Hotel', 14),\n",
       " ('frequency', 9),\n",
       " ('gallery', 9),\n",
       " ('Chimneys', 8),\n",
       " ('building', 7),\n",
       " ('landing', 7),\n",
       " ('pier', 7),\n",
       " ('span', 7),\n",
       " ('compartment', 6)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of defined terms: \", len(concept_or_span_with_definition))\n",
    "print(\"Number of defined terms with more than 1 definition: {} ({:.2f}) \".format(terms_with_multiple_defs, terms_with_multiple_defs/len(concept_or_span_with_definition) * 100))\n",
    "print(\"Number of definitions/notes found:\", total_num_definitions)\n",
    "print(\"Top 10 defined terms with most definitions:\")\n",
    "per_term_definition_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d97d0915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total count of terms found in definitions/notes: 5096\n",
      "Number of new spans (unique):  2655\n",
      "Random sample of unseen spans, found in the definitions:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['furniture',\n",
       " 'lightbulbs',\n",
       " 'common notions',\n",
       " 'inverse state',\n",
       " 'safety devices',\n",
       " 'airlines',\n",
       " 'oils',\n",
       " 'food preparation facilities',\n",
       " 'signaling',\n",
       " 'structure that']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Total count of terms found in definitions/notes:\", len(spans_found_in_definitions))\n",
    "print(\"Number of new spans (unique): \", len(unique_new_spans))\n",
    "print(\"Random sample of unseen spans, found in the definitions:\")\n",
    "random.sample(list(set(unique_new_spans)), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53825d2d",
   "metadata": {},
   "source": [
    "Quite a few defined terms have \n",
    "* Add any new spans to the graph, with prov:hasPrimarySource *WikiData* and prov:wasAttributedTo *SPaR.txt*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "558b5f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a related label between the defined concept/span, and the span found in a definition\n",
    "for i, (source, term_and_spans_dict) in enumerate(all_spartxt_objects.items()):\n",
    "    for term, related_spans in term_and_spans_dict.items():\n",
    "\n",
    "        # add the (concept or span) term as a span if it didn't exist yet as a span\n",
    "        term_uid = ua.assign_UID(term, SPANS)\n",
    "        irec_graph = add_tuples(irec_graph, irec_span(term_uid, term))\n",
    "        irec_graph = add_tuples(irec_graph, skos_in_scheme(term_uid, 'schemeUID', SPANS, SPANS))\n",
    "        irec_graph = add_tuples(irec_graph, provenance(term_uid, URIRef(source), SPANS))\n",
    "\n",
    "        rel_spans = [remove_determiners(r) for r in custom_cleaning_rules(related_spans)]\n",
    "        for rel_term in rel_spans :\n",
    "            # Add the spans that were extracted from the definitions, assign a new UID if needed\n",
    "            related_uid = ua.assign_UID(rel_term, SPANS)\n",
    "            irec_graph = add_tuples(irec_graph, irec_span(related_uid, rel_term))\n",
    "            irec_graph = add_tuples(irec_graph, skos_in_scheme(related_uid, 'schemeUID', SPANS, SPANS))\n",
    "            irec_graph = add_tuples(irec_graph, provenance(related_uid, URIRef(source), SPANS))\n",
    "            \n",
    "            # add agent for spans generated by SPaR.txt\n",
    "            irec_graph = add_tuples(irec_graph, prov_agent(related_uid, spart_txt_IRI, SPANS))\n",
    "\n",
    "            # Add relation between the defined span and the span from its definition\n",
    "            irec_graph = add_tuples(irec_graph, irec_definition_related(term_uid, related_uid)) \n",
    "\n",
    "#             # deprecated; Add relation between concept and span\n",
    "#             if concept_uid:\n",
    "#                 irec_graph = add_tuples(irec_graph, irec_definition_related(concept_uid, related_uid, CONCEPTS, SPANS)) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec817464",
   "metadata": {},
   "source": [
    "### Add WikiData definitions to graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c270684a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_wiki_definitions(irec_graph: Graph, wiki_dict: Dict[str, str], dict_namespace: Namespace):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    for i, (_, definition_dict_list) in enumerate(wiki_dict.items()):\n",
    "        for definition_dict in definition_dict_list:\n",
    "            wiki_term = definition_dict['prefLabel']\n",
    "            wiki_class_label = definition_dict['class_label'] \n",
    "            wiki_class_uid = definition_dict['class_uid'] \n",
    "            wiki_uid = definition_dict['WikiUID'].rsplit('/', 1)[1]\n",
    "\n",
    "            # keep track of uid in the Unique ID assigner obj as well\n",
    "            _ = ua.keep_track_of_existing_UID(wiki_term, wiki_uid, WIKI)\n",
    "            \n",
    "            # add the WikiData concept to the graph, in WIKI namespace\n",
    "            irec_graph = add_tuples(irec_graph, skos_node(wiki_uid, wiki_term, WIKI))\n",
    "            # irec_graph = add_tuples(irec_graph, skos_in_scheme(wiki_uid, 'schemeUID', WIKI, WIKI))\n",
    "            irec_graph = add_tuples(irec_graph, provenance(wiki_uid, wikidata_IRI, WIKI))\n",
    "            \n",
    "            # Add an exact match between the wiki node and our concept from the Merged Approved Documents\n",
    "            term_uid = ua.retrieve_uid_by_text(wiki_term, CONCEPTS)\n",
    "            if term_uid:\n",
    "                irec_graph = add_tuples(irec_graph, skos_exact_match(term_uid, wiki_uid, CONCEPTS, WIKI))\n",
    "            \n",
    "            #### We will link the WikiData concept to a span, rather than a concept, as well as its definitions\n",
    "            # and class labels\n",
    "            # 1) Add a span and a link to the wiki concept\n",
    "            span_uid = ua.retrieve_uid_by_text(wiki_term, SPANS)\n",
    "            if not span_uid:\n",
    "                raise Exception(f\"Cannot find the span: {wiki_term}!\")\n",
    "            \n",
    "            # 2) add wiki class label as a span in SPANS namespace, with provenance linking to original WikiData UID\n",
    "            wiki_class_span_uid = ua.assign_UID(wiki_class_label, SPANS)\n",
    "            irec_graph = add_tuples(irec_graph, irec_span(wiki_class_span_uid, wiki_class_label))\n",
    "            irec_graph = add_tuples(irec_graph, provenance(wiki_class_span_uid, WIKI[wiki_class_uid], SPANS))\n",
    "            # relate the class label span to the defined span, using RDF.type\n",
    "            irec_graph = add_tuples(irec_graph, rdf_type(span_uid, wiki_class_span_uid, SPANS, SPANS))\n",
    "\n",
    "            # 3)  Add the WIKI definition to the node if it exists, in SPANS namespace\n",
    "            if 'WikiDefinition' in definition_dict:            \n",
    "                definition = definition_dict['WikiDefinition']\n",
    "                irec_graph = add_tuples(irec_graph, irec_wikidef(span_uid, definition, SPANS))\n",
    "                \n",
    "            # 4) Add an exact match between the span and wikidata class as well\n",
    "            irec_graph = add_tuples(irec_graph, skos_exact_match(span_uid, wiki_uid, SPANS, WIKI))\n",
    "\n",
    "    return irec_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "43cf5dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "irec_graph = add_wiki_definitions(irec_graph, concept_wiki_dict, CONCEPTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "25128df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "irec_graph = add_wiki_definitions(irec_graph, span_wiki_dict, SPANS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f99770a",
   "metadata": {},
   "source": [
    "### We will also add the Uniclass terms that we found in the text to the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "82fe7391",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(graph_data_fp.joinpath(\"uniclass_terms_in_text.pkl\"), 'rb') as f:\n",
    "    uniclass_terms_in_text = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1076ed68",
   "metadata": {},
   "outputs": [],
   "source": [
    "for uniclass_uid, definition_dict in uniclass_terms_in_text.items():\n",
    "    # Add the Uniclass node to our graph\n",
    "    uniclass_term = definition_dict['pref_label']\n",
    "    # keep track of uid that is added to the graph\n",
    "    _ = ua.keep_track_of_existing_UID(uniclass_term, uniclass_uid, UNICLASS)\n",
    "    \n",
    "    # add the concept to the graph, in UNICLASS namespace\n",
    "    irec_graph = add_tuples(irec_graph, skos_node(uniclass_uid, uniclass_term, UNICLASS))\n",
    "    # irec_graph = add_tuples(irec_graph, skos_in_scheme(uniclass_uid, 'schemeUID', UNICLASS, UNICLASS))\n",
    "    irec_graph = add_tuples(irec_graph, provenance(uniclass_uid, uniclass_IRI, UNICLASS))\n",
    "    \n",
    "    # Determine or create the corresponding term_uid in SPANS and add a # skos:exactMatch?\n",
    "    if ua.retrieve_uid_by_text(uniclass_term): # First as is (no lowercasing, despite Uniclass casing)\n",
    "        # Add an exact match between the Uniclass node and the corresponding span\n",
    "        span_uid = ua.retrieve_uid_by_text(uniclass_term)\n",
    "        irec_graph = add_tuples(irec_graph, skos_exact_match(span_uid, uniclass_uid, SPANS, UNICLASS))\n",
    "    elif ua.retrieve_uid_by_text(uniclass_term.lower()):\n",
    "        # Add an exact match between the wiki node and the corresponding lowercased version in SPANS\n",
    "        span_uid = ua.retrieve_uid_by_text(uniclass_term.lower())\n",
    "        irec_graph = add_tuples(irec_graph, skos_exact_match(span_uid, uniclass_uid, SPANS, UNICLASS))\n",
    "    else:\n",
    "        # Although the uniclass term was found in the text, no exact matching span was extracted by SPaR.txt\n",
    "        # add the term (defined concept) as a span \n",
    "        term_uid = ua.assign_UID(uniclass_term, SPANS)\n",
    "        irec_graph = add_tuples(irec_graph, irec_span(term_uid, uniclass_term))\n",
    "        irec_graph = add_tuples(irec_graph, skos_in_scheme(term_uid, 'schemeUID', SPANS, SPANS))\n",
    "        irec_graph = add_tuples(irec_graph, provenance(term_uid, uniclass_IRI, SPANS))\n",
    "\n",
    "        irec_graph = add_tuples(irec_graph, skos_exact_match(term_uid, uniclass_uid, SPANS, UNICLASS))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ae56c35e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Graph identifier=N16319baf899a4b57ae3b416418100356 (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irec_graph.serialize(destination=graph_output_fp.joinpath(\"unfeatured_graph.ttl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e978d7",
   "metadata": {},
   "source": [
    "### Compute properties for and between spans [15K spans is 225 Million combinations... ]\n",
    "* link definitions to spans (if span occurs verbatim; linkwith irec:related)\n",
    "* link spans to spans (if a concept exists, they have the an equivalent span anyway)\n",
    "  * semantic similarity, x and y might be alternative labels or have the same superclass -> based on kNN\n",
    "  * constitutes; x occurs in y, thus y might be an extended phrase for x and perhaps a subclass, or x may be a material property, and so on\n",
    "  * morphological similarity, x may be an inflection of y or somehow related"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8aed2a",
   "metadata": {},
   "source": [
    "**Embedding the new spans to determine distributed similarity and classify domain-specificity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e46c83a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_model_name = \"bert-base-cased\"\n",
    "embedding_output_fp = Path.cwd().joinpath(\"data\", \"term_embedding\")\n",
    "IDF_path = embedding_output_fp.joinpath(\"IDF_weights.json\")\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "bert_model = BertModel.from_pretrained(bert_model_name, output_hidden_states=True)\n",
    "embedder = Embedder(tokenizer, bert_model, \n",
    "                      IDF_dict=json.load(open(IDF_path)), \n",
    "                      embedding_fp=embedding_output_fp,\n",
    "                      layers_to_use = [12],         # we'll use the output of the last layer\n",
    "                      layer_combination = \"avg\",    # how to combine layers if multiple are used\n",
    "                      idf_threshold = 1.5,          # minimum IDF value for a token to contribute\n",
    "                      idf_weight_factor = 1.0,      # modify how strong the influence of IDF weighting is\n",
    "                      not_found_idf_value = 0.5)    # IDF value for tokens that weren't seen during IDF computation (doesn't apply here)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4742e3c9",
   "metadata": {},
   "source": [
    "* First we'll need to compute the embeddings for the unique new spans\n",
    "  * Same process as before, EXCEPT that we now normalise the spans directly as well.\n",
    "  * This seems to break sometimes when I run it, and I don't know why yet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4f06a659",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### TODO: move this into a utility function\n",
    "# Compute the embeddings, this is split into subsets so we don't overload your memory (adjust these values if needed)\n",
    "max_num_cpu_threads = 4\n",
    "subset_size = 1000\n",
    "\n",
    "# Checks which of the embeddings for the clustering cluster_data already exist, so they can be re-used\n",
    "term_subsets = split_list(unique_new_spans, subset_size)\n",
    "embedding_files = [f for f in embedder.embedding_fp.glob('def_term_standardised_embeddings*.pkl')]\n",
    "span_and_embedding_pairs = []\n",
    "if len(embedding_files) == len(term_subsets):\n",
    "    for e in embedding_files:\n",
    "        span_and_embedding_pairs += pickle.load(open(e, 'rb'))\n",
    "else:\n",
    "    print(f\"Preparing embeddings for {len(unique_new_spans)} spans, in groups of: {subset_size}\")\n",
    "    subset_idx = 0            # iterator index outside of tqdm \n",
    "    for subset in tqdm(term_subsets):\n",
    "        subset_embeddings = []\n",
    "        subset_file_name = embedder.embedding_fp.joinpath(\"def_term_standardised_embeddings_part_{}.pkl\".format(subset_idx))\n",
    "        subset_idx += 1\n",
    "        if subset_file_name.exists():\n",
    "            # already computed previously\n",
    "            continue\n",
    "        \n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=max_num_cpu_threads) as executor:\n",
    "            futures = [executor.submit(embedder.embed_and_normalise_span, subset[idx]) for idx in range(len(subset))]\n",
    "\n",
    "        subset_embeddings += [f.result() for f in futures if f.result()]\n",
    "\n",
    "        with open(subset_file_name, 'wb') as f:\n",
    "            pickle.dump(subset_embeddings, f)\n",
    "\n",
    "    # Once all embeddings are created; combine them in span_and_embedding_pairs\n",
    "    embedding_files = [f for f in embedder.embedding_fp.glob('def_term_standardised_embeddings*.pkl')]\n",
    "    for e in embedding_files:\n",
    "        span_and_embedding_pairs += pickle.load(open(e, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2c64704c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the old span_and_embedding_pairs as well\n",
    "old_embedding_files = [f for f in embedder.embedding_fp.glob('embeddings*.pkl')]\n",
    "old_span_and_embedding_pairs = []\n",
    "for e in old_embedding_files:\n",
    "    old_span_and_embedding_pairs += pickle.load(open(e, 'rb'))\n",
    "\n",
    "domain_span_and_embedding_pairs = [(s, e) for (s, e) in old_span_and_embedding_pairs if s in domain_terms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "87890294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the old and new span and embeding pairs\n",
    "unique_spans = [s for (s, e) in (domain_span_and_embedding_pairs + span_and_embedding_pairs)]\n",
    "standardised_clustering_data = np.stack([np.mean(e, axis=0) if len(e.shape) > 1 else e for (s, e) in (domain_span_and_embedding_pairs + span_and_embedding_pairs)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b3cbbf",
   "metadata": {},
   "source": [
    "**Feature; domain-specifity property based on kNN classification**\n",
    "* We will re-use the knn graph and TFIDF based classification of the term extraction notebook\n",
    "* The domain options are either \"AEC domain\" or \"Out of domain\", which we'll add as a property to the span node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "160b6304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data to prepare the kNN based domain classifier\n",
    "knn_X = pickle.load(open(embedding_output_fp.joinpath(\"unique_embeddings.pkl\"), 'rb'))\n",
    "\n",
    "knn_spans = pickle.load(open(embedding_output_fp.joinpath(\"unique_spans.pkl\"), 'rb'))\n",
    "with open(embedding_output_fp.joinpath(\"span_domain_ood_dict.json\"), 'r') as f:\n",
    "    span_df_dict = json.load(f)  \n",
    "    \n",
    "knn_y = [span_df_dict[span]['domain'] for span in knn_spans]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1e85409e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the classifier\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=50, \n",
    "                                      weights='distance',\n",
    "                                      leaf_size=100, \n",
    "                                      metric='euclidean', \n",
    "                                      n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "dc05b930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(leaf_size=100, metric='euclidean', n_jobs=4,\n",
       "                     n_neighbors=50, weights='distance')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the classifier\n",
    "knn_classifier.fit(knn_X, knn_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "09fb9e4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'y'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict on a single span\n",
    "test_span = 'shared wall'\n",
    "knn_classifier.predict(embedder.embed_and_normalise(test_span).reshape(1, -1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "342b11cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = knn_classifier.predict(standardised_clustering_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d52bba6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Span does not exist in the graph: at\n",
      "Span does not exist in the graph: obs\n",
      "Span does not exist in the graph: at\n",
      "Span does not exist in the graph: ( emergency\n",
      "Span does not exist in the graph: Sun\n"
     ]
    }
   ],
   "source": [
    "# depending on how long this takes, we'll simply predict the label for each of the spans (even the ones we already know the answer for)\n",
    "tuples_to_add = []\n",
    "for span, prediction in zip(unique_spans, predictions):\n",
    "    try:\n",
    "        span_uid = ua.UIDs[SPANS.placeholder.defrag().__reduce__()[1][0]][span.strip()]\n",
    "    except KeyError:\n",
    "        print(f\"Span does not exist in the graph: {span}\")\n",
    "        \n",
    "    if prediction == \"y\":\n",
    "        tuples_to_add += irec_domain(span_uid, \"AEC domain\")\n",
    "    elif prediction == \"n\":\n",
    "        tuples_to_add += irec_domain(span_uid)\n",
    "    else:\n",
    "        print(\"this should not happen!\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "cc4d732b",
   "metadata": {},
   "outputs": [],
   "source": [
    "irec_graph = add_tuples(irec_graph, tuples_to_add)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de86670d",
   "metadata": {},
   "source": [
    "**Add semantic similarity relation to the 5 nearest neighbours**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809890b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the kNN graph for ALL spans now (old + new)\n",
    "n_neighbors = 5 # the number of neighbours we compute for each term\n",
    "knn_graph = kneighbors_graph(standardised_clustering_data, \n",
    "                             n_neighbors,    \n",
    "                             metric=\"cosine\", # <- note we're using cosine sim\n",
    "                             n_jobs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19302366",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_sim_dict = {}\n",
    "for span_idx, span in enumerate(unique_spans):\n",
    "    knn_sim_dict[span] = [unique_spans[neighbour_idx] for neighbour_idx in knn_graph[span_idx].indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b078319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add similarity to the irec_graph\n",
    "tuples_to_add = []\n",
    "missing_spans = []\n",
    "for span_one, neighbour_spans in knn_sim_dict.items():\n",
    "    try:\n",
    "        span_one_uid = ua.UIDs[SPANS.placeholder.defrag().__reduce__()[1][0]][span_one]\n",
    "    except:\n",
    "        missing_spans.append(span_one)\n",
    "        continue    \n",
    "    for span_two in neighbour_spans:\n",
    "        try:\n",
    "            span_two_uid = ua.UIDs[SPANS.placeholder.defrag().__reduce__()[1][0]][span_two]\n",
    "            tuples_to_add += irec_sem_sim(span_one_uid, span_two_uid)\n",
    "        except:\n",
    "            missing_spans.append(span_two)\n",
    "            \n",
    "irec_graph = add_tuples(irec_graph, tuples_to_add)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4276a04a",
   "metadata": {},
   "source": [
    "**Constitutes & morphological similarity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17cc54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterSpan:\n",
    "    def __init__(self, span:str, span_uid: str):\n",
    "        \n",
    "        if not span:\n",
    "            raise Exception(\"Input is an empty string!\")\n",
    "        \n",
    "        self.text = span\n",
    "        self.uid = span_uid\n",
    "        self.blob = TextBlob(span)\n",
    "        self.words = [w for w in self.blob.words]\n",
    "        self.stems = [w.stem() for w in self.words]\n",
    "        \n",
    "        self.morphologically_similar_uids = {}\n",
    "        self.semantically_similar_uids = {}\n",
    "        self.constitutes_uids = {}\n",
    "        self.contains_antonym_uids = {}       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b50ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def span_constitutes_span(span_one: CharacterSpan, span_two: CharacterSpan):\n",
    "    \"\"\" True if in a span, you can find all of the words comprising span_two (order doesn't matter) \"\"\"\n",
    "    word_overlap = list(set(span_one.words) & set(span_two.words))\n",
    "    if len(word_overlap) in [len(span_one.words), len(span_two.words)]:\n",
    "#             span_one.constitutes_uids.add(span_two.uid)\n",
    "        return True\n",
    "    return False\n",
    "    \n",
    "def morphologically_similar(span_one: CharacterSpan, span_two: CharacterSpan):\n",
    "    \"\"\" True if this span has either a small Levenshtein distance, or many overlapping words/stems with another span \"\"\"\n",
    "    wl_one = len(span_one.words)\n",
    "    wl_two = len(span_two.words)\n",
    "    longest = max(wl_one, wl_two)\n",
    "    \n",
    "    if (wl_one < wl_two - 1) or (wl_one > wl_two + 1):\n",
    "        # max 1 word difference in length\n",
    "        return False\n",
    "    \n",
    "    if (wl_one == 1) and (wl_two == 1):\n",
    "        if span_one.stems == span_two.stems:\n",
    "            # if the stem is the same, then we'll assume that the words are morphologically similar\n",
    "            return True\n",
    "        \n",
    "        if levenshtein(span_one.text, span_two.text):\n",
    "            # small edit distance (levenshtein), works for single words only\n",
    "            return True\n",
    "    else:\n",
    "        stem_overlap = list(set(span_one.stems) & set(span_two.stems))\n",
    "        if (len(stem_overlap) >= (2 * longest // 3)):\n",
    "            # 2 out of 3 words or stems (ish) are overlapping\n",
    "            unique_w_indices_one = [idx for idx, s in enumerate(span_one.stems) if s not in stem_overlap]\n",
    "            unique_w_indices_two = [idx for idx, s in enumerate(span_two.stems) if s not in stem_overlap]\n",
    "            for w_one_idx, w_two_idx in product(unique_w_indices_one, unique_w_indices_two):\n",
    "                if not levenshtein(span_one.words[w_one_idx], span_two.words[w_two_idx]):\n",
    "                    # if any of the non-overlapping words have a large edit distance, the the whole thing is not \n",
    "                    # morphologically similar\n",
    "                    return False\n",
    "            \n",
    "            # now, many of the words are pretty much the same, and the edit disance for remainder of words is small\n",
    "            return True\n",
    "        \n",
    "    return False\n",
    "\n",
    "def span_with_antonym(span_one: CharacterSpan, span_two: CharacterSpan, wordnet_antonyms: Dict[str, str]=wordnet_antonyms):\n",
    "    antonyms_one = [w for w in span_one.words if w in wordnet_antonyms.keys()]\n",
    "    for a in antonyms_one:\n",
    "        antonyms_to_find = wordnet_antonyms[a]\n",
    "        if any([x for x in span_two.words if x in antonyms_to_find]):\n",
    "            # antonym present, but any overlap in the rest of the spans?\n",
    "            if morphologically_similar(span_one, span_two):\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a61f002",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1 = \"acoustic\"\n",
    "test_2 = \"thermal\"\n",
    "cs1 = CharacterSpan(test_1, '1')\n",
    "cs2= CharacterSpan(test_2, '2')\n",
    "print(\"span_1 constitutes span_2: \", span_constitutes_span(cs1, cs2))\n",
    "print(\"morphologically similar: \", morphologically_similar(cs1, cs2))\n",
    "print(\"antonym present: \", span_with_antonym(cs1, cs2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e207d87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1 = \"damp proof course\"\n",
    "test_2 = \"damp proof membrane\"\n",
    "cs1 = CharacterSpan(test_1, '1')\n",
    "cs2= CharacterSpan(test_2, '2')\n",
    "print(\"span_1 constitutes span_2: \", span_constitutes_span(cs1, cs2))\n",
    "print(\"morphologically similar: \", morphologically_similar(cs1, cs2))\n",
    "print(\"antonym present: \", span_with_antonym(cs1, cs2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac1d190",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1 = \"hot water storage\"\n",
    "test_2 = \"cold water storage\"\n",
    "cs1 = CharacterSpan(test_1, '1')\n",
    "cs2= CharacterSpan(test_2, '2')\n",
    "print(\"span_1 constitutes span_2: \", span_constitutes_span(cs1, cs2))\n",
    "print(\"morphologically similar: \", morphologically_similar(cs1, cs2))\n",
    "print(\"antonym present: \", span_with_antonym(cs1, cs2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95204ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1 = \"hot water storage system\"\n",
    "test_2 = \"cold press\"\n",
    "cs1 = CharacterSpan(test_1, '1')\n",
    "cs2= CharacterSpan(test_2, '2')\n",
    "print(\"span_1 constitutes span_2: \", span_constitutes_span(cs1, cs2))\n",
    "print(\"morphologically similar: \", morphologically_similar(cs1, cs2))\n",
    "print(\"antonym present: \", span_with_antonym(cs1, cs2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491b93c5",
   "metadata": {},
   "source": [
    "* compute features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b875cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features(argument_list: List[CharacterSpan]):\n",
    "    span_one, span_two = argument_list\n",
    "    feature_tuples = []\n",
    "    \n",
    "    if span_one == span_two:\n",
    "        return feature_tuples\n",
    "    \n",
    "    if span_constitutes_span(span_one, span_two):\n",
    "        feature_tuples.append(irec_constitutes(span_one.uid, span_two.uid))\n",
    "        \n",
    "    if morphologically_similar(span_one, span_two):\n",
    "        feature_tuples.append(irec_morp_sim(span_one.uid, span_two.uid))\n",
    "        \n",
    "    return feature_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29daf2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to CharacterSpan objects\n",
    "spans_namespace_uid = SPANS.placeholder.defrag().__reduce__()[1][0]\n",
    "spans = [k for k in ua.UIDs[spans_namespace_uid].keys() if ua.UIDs[spans_namespace_uid][k] != 'schemeUID']\n",
    "spans_c = [CharacterSpan(span, ua.UIDs[spans_namespace_uid][span]) for span in spans]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76941b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we check for antonyms\n",
    "# only check the spans that actually contain a potential antonym verbatim, could do more flexible matching\n",
    "spans_with_antonyms = [s for s in spans_c if (s.text not in wordnet_antonyms and any([w for w in s.words if w in wordnet_antonyms.keys()]))]\n",
    "print(len(spans_with_antonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28647576",
   "metadata": {},
   "outputs": [],
   "source": [
    "antonym_examples = []\n",
    "antonym_tuples = []\n",
    "for span_one, span_two in tqdm(product(spans_with_antonyms, spans_with_antonyms)):\n",
    "    if span_with_antonym(span_one, span_two):\n",
    "        antonym_examples.append([span_one.text, span_two.text])\n",
    "        antonym_tuples += irec_antonym(span_one.uid, span_two.uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f2b85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "irec_graph = add_tuples(irec_graph, antonym_tuples)\n",
    "random.sample(antonym_examples, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de94f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "'explicit' in wordnet_antonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8de5684",
   "metadata": {},
   "source": [
    "* Second, we check for the other relations, morphologiical similarity and irec:constitutes\n",
    "     * **TODO**: consider reducing the the amount of features to compute, e.g., only certain distance (maxhops) or already having a relation of some type (related / simRel / ...)\n",
    "     * we could make an educated decision based on checking feature correlations maybe once we have computed them all?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbcba7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Will need to compute {len(spans)} x {len(spans)} = {len(spans) * len(spans)} combinations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b80c289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_list too memory intensive for the amount of combinations, need to yield         \n",
    "def batcher(iterable, batch_size):\n",
    "    iterator = iter(iterable)\n",
    "    while batch := list(islice(iterator, batch_size)):\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa299ab1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split the processing into multiple parts again and save intermediate states for re-use\n",
    "max_num_cpu_threads = 1024\n",
    "subset_size = 5000000 # 1M\n",
    "\n",
    "feature_files = [f for f in graph_output_fp.glob('features*.pkl')]\n",
    "feature_tuples = []\n",
    "if len(feature_files) == (len(spans_c)*len(spans_c)/subset_size):\n",
    "    for ff in feature_files:\n",
    "        feature_tuples += pickle.load(open(ff, 'rb'))\n",
    "else:\n",
    "    print(f\"Preparing feature triples for {len(spans) * len(spans)} span pairs, in groups of: {subset_size}\")\n",
    "    subset_idx = 0 \n",
    "    with tqdm(total=(len(spans_c) * len(spans_c))//subset_size) as pbar:\n",
    "        for subset in tqdm(batcher(product(spans_c, spans_c), subset_size)):\n",
    "            subset_features = []\n",
    "            subset_file_name = graph_output_fp.joinpath(\"features_part_{}.pkl\".format(subset_idx))\n",
    "            subset_idx += 1\n",
    "            if subset_file_name.exists():\n",
    "                print(f\"Already computed '{subset_file_name.stem}' previously, skipping\") \n",
    "                pbar.update(1)\n",
    "                continue\n",
    "\n",
    "            with concurrent.futures.ThreadPoolExecutor(max_workers=max_num_cpu_threads) as executor: # ThreadPoolExecutor\n",
    "                futures = [executor.submit(compute_features, pair) for pair in subset]\n",
    "\n",
    "            subset_features += [f.result() for f in futures if f.result()]\n",
    "            with open(subset_file_name, 'wb') as f:\n",
    "                pickle.dump(subset_features, f)\n",
    "                \n",
    "            pbar.update(1)\n",
    "            \n",
    "    # Once all features are computed; combine them in a single list of tuples to add to the graph\n",
    "    feature_files = [f for f in graph_output_fp.glob('features*.pkl')]\n",
    "    for ff in feature_files:\n",
    "        feature_tuples += pickle.load(open(ff, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5ced48",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuples_to_add = [t[0] for t_list in feature_tuples for t in t_list]\n",
    "irec_graph = add_tuples(irec_graph, tuples_to_add)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf754ed1",
   "metadata": {},
   "source": [
    "#### Save final graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "83372d92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Graph identifier=N9c8697010b4a428ab2ce19b3c48d2cac (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irec_graph.serialize(destination=graph_output_fp.joinpath(\"featured_graph.ttl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99ff2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_HERE()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c9239f",
   "metadata": {},
   "source": [
    "### Testing morphological similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89337b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "featured_graph = Graph().parse(graph_output_fp.joinpath(\"featured_graph.ttl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4d7250",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_morph_sim_for_node(uad, graph, node_text, namespace: Namespace = SPANS):\n",
    "    n_space = namespace.placeholder.defrag().__reduce__()[1][0]\n",
    "    node_text = node_text.strip()\n",
    "    node_id = uad.UIDs[n_space][node_text]\n",
    "    candidates = []\n",
    "    # find all triples with subject node_text and predicate isMorphologicallySimilarTo\n",
    "    for s, p, o in graph.triples((namespace[node_id],  IREC.isMorphologicallySimilarTo, None)):\n",
    "        subj = urllib.parse.unquote(s.split('#')[-1])\n",
    "        obj = urllib.parse.unquote(o.split('#')[-1])\n",
    "        candidates.append(obj)\n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bac701d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# node_url_text = \"impact%20sound%20insulation\"\n",
    "node_url_text = \"ferrite\"\n",
    "node_text = urllib.parse.unquote(node_url_text)\n",
    "candidates = get_morph_sim_for_node(ua, featured_graph, node_text, SPANS)\n",
    "for s1, s2 in product ([node_text], candidates):\n",
    "    cs1 = CharacterSpan(s1, '1')\n",
    "    cs2= CharacterSpan(s2, '2')\n",
    "    print(f\"{morphologically_similar(cs1, cs2)} \\t {s1} -- {s2}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac59eadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ua.print_node_by_text(irec_graph, 'absorption coefficient', CONCEPTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98afca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_term_definition_counts.most_common(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f573c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ua.print_node_by_text(irec_graph, 'partition', SPANS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e63fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ua.print_node_by_text(irec_graph, 'separation', SPANS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d592dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.parse.unquote(\"airborne%20sound%20insulation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84496b00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "81fc1d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing NNs for new spans\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d0f14090",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_nn = NearestNeighbors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5fca77c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NearestNeighbors()"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_nn.fit(knn_X, knn_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "64a180ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_span = 'shared wall'\n",
    "closest_distances, indices = test_nn.kneighbors(embedder.embed_and_normalise(test_span).reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "52be14d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shared wall\n",
      "separating wall\n",
      "a shared case\n",
      "part wall\n",
      "an cavity wall\n",
      "a cavity wall\n"
     ]
    }
   ],
   "source": [
    "print(test_span)\n",
    "for idx in indices[0]:\n",
    "    print(knn_spans[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d87b2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469f8bb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
